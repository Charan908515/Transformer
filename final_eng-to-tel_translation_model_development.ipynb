{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44c2906-8652-4069-8900-b045223b6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0187c99-7317-47bc-b1f0-d6f8c05efd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN=\"<start>\"\n",
    "END_TOKEN=\"<end>\"\n",
    "PADDING_TOKEN=0\n",
    "telugu_vocabulary = [\n",
    "     START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', \n",
    "    '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`',\n",
    "    \"అ\", \"ఆ\", \"ఇ\", \"ఈ\", \"ఉ\", \"ఊ\", \"ఋ\", \"ఌ\", \"ఎ\", \"ఏ\", \"ఐ\", \"ఒ\", \"ఓ\", \"ఔ\", \"క\", \"ఖ\", \"గ\", \"ఘ\", \"ఙ\", \n",
    "    \"చ\", \"ఛ\", \"జ\", \"ఝ\", \"ఞ\", \"ట\", \"ఠ\", \"డ\", \"ఢ\", \"ణ\", \"త\", \"థ\", \"ద\", \"ధ\", \"న\", \"ప\", \"ఫ\", \"బ\", \"భ\", \"మ\", \n",
    "    \"య\", \"ర\", \"ఱ\", \"ల\", \"ళ\", \"ఴ\", \"వ\", \"శ\", \"ష\", \"స\", \"హ\", \"ౠ\", \"◌ా\", \"◌ి\", \"◌ీ\", \"ు\", \"ూ\", \"ృ\", \"ౄ\", \n",
    "    \"◌ె\", \"◌ే\", \"◌ై\", \"◌ొ\", \"◌ో\", \"◌ౌ\", \"◌ౢ\", \"◌ౣ\", \".\", \"।\", \"॥\", \"॰\", \"◌్\", \"◌ౕ\", \"◌ౖ\", \"◌ఀ\", \"ఁ\",  'ా','ి',\n",
    "'ీ','ె','ే','ై','ొ','ో', 'ౌ','్','ౖ','౦','౧','౨','౩','౭','౯','ಂ','ಆ','ಕ','ಗ','ಚ','ಜ','ನ','ಬ','ಮ','ರ','ಳ','ಾ','ಿ','ು','ೂ','ೆ','್','ງ','ດ','ທ','ປ','ມ','ລ','ວ','ອ','ະ','າ','ື','ເ','–','—','‘','’','“','”','‡','•','․','…', '′','″',\n",
    "   \n",
    "'آ',\n",
    " 'ئ',\n",
    " 'ا',\n",
    " 'ب',\n",
    " 'ة',\n",
    " 'ت',\n",
    " 'د',\n",
    " 'ر',\n",
    " 'ز',\n",
    " 'ش',\n",
    " 'ض',\n",
    " 'ع',\n",
    " 'ف',\n",
    " 'ق',\n",
    " 'ل',\n",
    " 'و',\n",
    " 'پ',\n",
    " 'ہ',\n",
    " 'ی',\n",
    " 'ं',\"ం\", \"ః\", \"ॐ\", '{', '|', '}', '~' , PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "english_vocabulary = [\n",
    "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', \n",
    "    '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c',\n",
    "    'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', \n",
    "    'z', '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfe0330-bc3a-40dc-8837-24029e51ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=set(telugu_vocabulary)\n",
    "telugu_vocabulary=[k for k in t]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32051ec7-164e-463f-ba3d-e305b5b18974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 'ఉ',\n",
       " '6',\n",
       " 'భ',\n",
       " ')',\n",
       " 'ఢ',\n",
       " '?',\n",
       " 'ఝ',\n",
       " 'త',\n",
       " 'ಳ',\n",
       " 'ວ',\n",
       " 'آ',\n",
       " '=',\n",
       " 'ಜ',\n",
       " 'ೂ',\n",
       " '్',\n",
       " '”',\n",
       " '౭',\n",
       " '!',\n",
       " 'ధ',\n",
       " 'ఊ',\n",
       " 'ఛ',\n",
       " '।',\n",
       " 'ರ',\n",
       " 'ఈ',\n",
       " 'ಿ',\n",
       " 'శ',\n",
       " '◌ఀ',\n",
       " '_',\n",
       " 'డ',\n",
       " '◌ై',\n",
       " 'ທ',\n",
       " '‡',\n",
       " 'ನ',\n",
       " 'ఞ',\n",
       " '․',\n",
       " 'ద',\n",
       " '′',\n",
       " '/',\n",
       " 'ఁ',\n",
       " 'ఆ',\n",
       " 'ఱ',\n",
       " 'ື',\n",
       " 'ض',\n",
       " '“',\n",
       " 'د',\n",
       " '@',\n",
       " 'ఇ',\n",
       " 'ఖ',\n",
       " '◌ీ',\n",
       " '3',\n",
       " 'ಂ',\n",
       " 'ປ',\n",
       " 'ເ',\n",
       " 'ఠ',\n",
       " 'ل',\n",
       " 'ງ',\n",
       " '*',\n",
       " '౯',\n",
       " '4',\n",
       " '&',\n",
       " 'చ',\n",
       " 'ా',\n",
       " 'ం',\n",
       " '◌ో',\n",
       " '◌ౌ',\n",
       " '>',\n",
       " 'ౠ',\n",
       " 'ు',\n",
       " 'ే',\n",
       " 'ూ',\n",
       " 'ة',\n",
       " 'າ',\n",
       " 'ప',\n",
       " '\"',\n",
       " 'ఘ',\n",
       " 'ట',\n",
       " ',',\n",
       " '◌ౣ',\n",
       " 'ಾ',\n",
       " 'ಮ',\n",
       " 'ق',\n",
       " '-',\n",
       " 'ಚ',\n",
       " '2',\n",
       " 'ర',\n",
       " 'ౄ',\n",
       " '{',\n",
       " '~',\n",
       " 'ణ',\n",
       " ']',\n",
       " 'ع',\n",
       " 'ౖ',\n",
       " 'و',\n",
       " '౩',\n",
       " '◌ి',\n",
       " '◌్',\n",
       " 'ೆ',\n",
       " '౧',\n",
       " 'ئ',\n",
       " '◌ౢ',\n",
       " 'ं',\n",
       " 'న',\n",
       " '◌ా',\n",
       " 'ె',\n",
       " 'ز',\n",
       " 'అ',\n",
       " '}',\n",
       " '|',\n",
       " '…',\n",
       " 'ృ',\n",
       " '◌ొ',\n",
       " '+',\n",
       " 'ీ',\n",
       " 'ఎ',\n",
       " 'ఔ',\n",
       " 'బ',\n",
       " 'ి',\n",
       " 'ಆ',\n",
       " '[',\n",
       " 'ష',\n",
       " 'ش',\n",
       " '(',\n",
       " '9',\n",
       " '<',\n",
       " '^',\n",
       " 'స',\n",
       " '•',\n",
       " '౨',\n",
       " '5',\n",
       " 'ॐ',\n",
       " 'య',\n",
       " 'ು',\n",
       " 'ొ',\n",
       " '0',\n",
       " 'ఒ',\n",
       " '%',\n",
       " '◌ౕ',\n",
       " 'ت',\n",
       " 'ఓ',\n",
       " 'ہ',\n",
       " 'پ',\n",
       " '\\\\',\n",
       " 'గ',\n",
       " 'ر',\n",
       " '್',\n",
       " ':',\n",
       " '॰',\n",
       " 'ఙ',\n",
       " 'ಗ',\n",
       " '8',\n",
       " 'ళ',\n",
       " 'ب',\n",
       " '◌ౖ',\n",
       " 'థ',\n",
       " 'ఏ',\n",
       " 'ಬ',\n",
       " '–',\n",
       " 'ఴ',\n",
       " 'ః',\n",
       " 'ະ',\n",
       " '1',\n",
       " 'జ',\n",
       " 'ో',\n",
       " \"'\",\n",
       " '7',\n",
       " 'మ',\n",
       " '౦',\n",
       " '’',\n",
       " '.',\n",
       " 'ఐ',\n",
       " 'ອ',\n",
       " '‘',\n",
       " 'ఋ',\n",
       " 'వ',\n",
       " 'ລ',\n",
       " '$',\n",
       " '<start>',\n",
       " '◌ే',\n",
       " 'ມ',\n",
       " 'ై',\n",
       " ' ',\n",
       " 'ف',\n",
       " 'క',\n",
       " 'ی',\n",
       " '◌ె',\n",
       " 'ດ',\n",
       " 'ౌ',\n",
       " '`',\n",
       " '॥',\n",
       " 'ల',\n",
       " 'హ',\n",
       " '—',\n",
       " 'ఫ',\n",
       " '″',\n",
       " '<end>',\n",
       " 'ಕ',\n",
       " '#',\n",
       " 'ఌ',\n",
       " 'ا']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telugu_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4aac668-cc6e-49dc-8bbf-4d7780fe3e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rise again.\\n',\n",
       " 'How do we glorify Jehovahs undeserved kindness?\\n',\n",
       " 'India also continues to push back economically.\\n',\n",
       " 'I remember my childhood days.\\n',\n",
       " 'All transactions are made online.\\n',\n",
       " 'I love night shoots in my city.\\n',\n",
       " 'Three members of a family were killed in the incident.\\n',\n",
       " 'The film is directed by V V Vinayak.\\n',\n",
       " 'The Hyderabad weather office forecast more rains.\\n',\n",
       " 'We are family!\\n',\n",
       " 'In this context, the Rs.\\n',\n",
       " 'What is the phrase?\\n',\n",
       " 'In Monaco-Ville, street signs are printed in both French and Mongasque.\\n',\n",
       " 'The same has been confirmed in a report carried by BBC.\\n',\n",
       " \"I said, 'who are you?'\\n\",\n",
       " 'In this state they can easily survive one year or longer.\\n',\n",
       " 'The complete details of his death are yet to be known.\\n',\n",
       " 'If You Go...\\n',\n",
       " '\"All the very best.\"\"\"\\n',\n",
       " 'What is this, child?\\n',\n",
       " 'Speaking during another Session on Reducing poverty through social and economic transformation, Shri Gangwar the Government of India, under the leadership of Prime Minister Shri Narendra Modi, is committed to eradicate poverty and promote prosperity in a changing world\\n',\n",
       " 'Ministry of Science Technology Antiviral nano-coatings to be upscaled for making triple layer medical masks N-95 respirator receives support to combat COVD 19 Professor Ashutosh Sharma, Secretary, DST--- Use of highly effective antimicrobial nanoparticles on PPEs, masks etc is a useful application that will provide an extra layer of protection for the high risk settings As part of Nano Mission programme, the Department of Science and Technology (DST) has approved support for upscaling an antiviral nano-coatings developed by Professor Ashwini Kumar Agrawal of Indian Institute of Technology, Delhi for use as appropriate material for producing anti-COVID-19 Triple Layer Medical masks and N-95 respirator in large quantities.\\n',\n",
       " 'We have seen many such cases.\\n',\n",
       " 'The Redmi 4 is available in three versions.\\n',\n",
       " 'Parupalli Kashyap advcanced to Korea Open semifinals.\\n',\n",
       " 'However, there has been no official statement on this yet.\\n',\n",
       " 'Annual International Yoga Day is a modern celebration of the ancient Indian discipline.\\n',\n",
       " 'The skirting at the side and the rear bumper is decked in faux carbon fibre\\n',\n",
       " 'Finally, a little blue engine arrives.\\n',\n",
       " 'The rooms are spacious and well furnished.\\n',\n",
       " 'It will be a milestone in his career.\\n',\n",
       " 'The teaser of the song was released.\\n',\n",
       " 'The problem lies at the top.\\n',\n",
       " 'Prime Minister Narendra Modi never made any such promise.\\n',\n",
       " 'Skip: Nothing.\\n',\n",
       " 'Unusual mix\\n',\n",
       " 'Is it jihaad?\\n',\n",
       " 'Accordingly, the due date for payment of self-assessment tax for taxpayers whose self-assessment tax liability is up to Rs.1 lakh has been extended to 31st January, 2021 for the taxpayers mentioned in para 3(A) and para 3(B) and to 31st December, 2020 for the taxpayers mentioned in para 3\\n',\n",
       " '\"Who say, \"\"Let him make speed, let him hasten his work, that we may see it. and let the counsel of the Holy One of Israel draw near and come, that we may know it!\"\"\"\\n',\n",
       " 'Rs 14 crore\\n',\n",
       " 'The university has accepted foreign students since 1980.\\n',\n",
       " 'There are many reasons.\\n',\n",
       " 'How does Walnut work?\\n',\n",
       " 'Railway stations and bus stands are littered with rubbish.\\n',\n",
       " 'outside school,office,market,couts many words will useregullarly\\n',\n",
       " 'There is also a small mosque on the western side of the plinth\\n',\n",
       " 'Begin slowly and progress gradually.\\n',\n",
       " 'While the work of the Constitution was going on, the work of the constitution was still in progress, it was just the beginning and I want to repeat what Bhupender Kumar Dutt had said at that time in the same Constituent Assembly\\n',\n",
       " 'Ganguly discharged from hospital\\n',\n",
       " 'The police was forced go back.\\n',\n",
       " 'How much sleep are you getting every day?\\n',\n",
       " '205 crores.\\n',\n",
       " 'Transmission options include the 1.2-litre petrol to come mated to a five-speed manual and the 1.5-litre diesel to come mated to a six-speed manual with an optional torque converter\\n',\n",
       " 'Diesel in Delhi at Rs.\\n',\n",
       " 'The National Council for , 1993\\n',\n",
       " ': Rahul Gandhi asks Rajnath\\n',\n",
       " 'However, schools and offices remained shut.\\n',\n",
       " \"Jean said, 'I think it is better this way for Uncle Georgie.\\n\",\n",
       " 'The police were also on their toes to avoid any untoward incidents.\\n',\n",
       " 'The police has registered a case on her complaint.\\n',\n",
       " 'Music is composed by Mickey J Meyer and background score is composed by Thaman.\\n',\n",
       " 'Hyderabad: Telangana Congress Party leader Revanth Reddy accused state chief minister K Chandrasekhar Rao of being corrupt and selfish.\\n',\n",
       " 'The dead in the explosions included an eight-year-old boy, the local daily said citing a law enforcement source\\n',\n",
       " 'Dont go anywhere.\\n',\n",
       " 'He studied only up to Class 7.\\n',\n",
       " 'The fans are awaiting this movie eagerly.\\n',\n",
       " 'He said the responsibility of protecting the environment lies with everyone.\\n',\n",
       " 'Chief Election Commissioner Sunil Arora.\\n',\n",
       " 'Important dates :\\n',\n",
       " 'He is unable to speak properly.\\n',\n",
       " 'In Mumbai, petrol cost Rs.\\n',\n",
       " 'Crops are also damaged with flood.Farmers are facing very bad situation.\\n',\n",
       " 'Well done .\\n',\n",
       " 'Another accused arrested in bank robbery case\\n',\n",
       " 'I dont understand one bit.\\n',\n",
       " 'The Haryana Police has issued a look out notice for her.\\n',\n",
       " 'Is the government prepared?\\n',\n",
       " 'Yesterday, I have discussed in detail about the experiences of Unlock-One in 21 states and Union Territories of the country.\\n',\n",
       " 'It was made known through the media.\\n',\n",
       " '1 tbsp.\\n',\n",
       " 'Mumbai: Mukesh Ambani-led Reliance Industries Limited (RIL) has offered the shareholders of its subsidiary and unlisted unit Reliance Retail to swap their units with RIL shares.\\n',\n",
       " '\"Aamir said it had been an \"\"honour and a pleasure\"\" for him to be the \"\"Brand Ambassador for the Incredible India campaign for the past 10 years.\"\"\"\\n',\n",
       " 'The police registered a case against the accused and are investigating the incident.\\n',\n",
       " 'God save us all!\\n',\n",
       " 'Except that they hadnt.\\n',\n",
       " 'No document is required to update a photo on the Aadhaar card.\\n',\n",
       " 'Not only that!\\n',\n",
       " 'The teaser of the film has already garnered a huge response.\\n',\n",
       " 'This is one of his own favourite performances.\\n',\n",
       " 'It still isnt.\\n',\n",
       " 'The BJP in Haryana is in a tricky position.\\n',\n",
       " 'And healthy too.\\n',\n",
       " 'Police recovered a mobile phone.\\n',\n",
       " 'The storage can be expanded upto 256 GB.\\n',\n",
       " 'Prime Minister Narendra Modi, Sri Lankan President Gotabaya Rajapaksa, Maldivian President Ibrahim Mohamed Solih, Nepalese Prime Minister KP Sharma Oli, Bhutanese premier Lotay Tshering and Bangladeshi Prime Minister Sheikh Hasina participated in the conference.\\n',\n",
       " 'The coronavirus cases are increasing exponentially each day.\\n',\n",
       " 'So far, we have completed 60 per cent of the film.\\n',\n",
       " 'A distance of nearly 200 kilometres.\\n',\n",
       " 'Libparted Warning\\n',\n",
       " 'Kamal Haasan will play the protagonist in Indian 2.\\n',\n",
       " 'Microscopic view of various kinds of pollen\\n',\n",
       " 'Long live\\n',\n",
       " 'Rajasthan Sports University (Hindi: ) is a newly established first public sports university situated in the Jhunjhunu Town of the Indian state Rajasthan to promote the sports education in state Rajasthan.\\n',\n",
       " 'More common in children.\\n',\n",
       " 'Honda Confirms 8th Fatality Due To Faulty Airbag\\n',\n",
       " 'Stage I Test consists of Intelligence Tests, Picture Perception and Group Discussion Tests.\\n',\n",
       " 'It is not just in the film industry.\\n',\n",
       " 'Recently, Karisma did a cameo in the film Zero starring Shah Rukh Khan and Anushka Sharma as the leads.\\n',\n",
       " 'Information and Broadcasting minister Prakash Javadekar (PTI)\\n',\n",
       " 'He hasnt seen the film.\\n',\n",
       " 'What happened previously:\\n',\n",
       " 'Describing yet another difference between the one who is careful about words and the one who is not, the king of Israel says: A shrewd man is covering knowledge, but the heart of the stupid ones is one that calls out foolishness. Proverbs 12: 23.\\n',\n",
       " 'This is not a special on education.\\n',\n",
       " 'The letter was encouraging, so Valrie said: Well, why not?\\n',\n",
       " 'It is not at uncommon for a Telugu film to overtake Hindi films.\\n',\n",
       " 'We have none.\\n',\n",
       " 'Internal Memory: 128 GB, 256 GB, 512 GB\\n',\n",
       " 'She played the role of Rama Lakshmi in the movie.\\n',\n",
       " 'Only one survived.\\n',\n",
       " 'I did invite them.\\n',\n",
       " 'ND Tiwari subsequently married Rohit Tiwaris mother, Ujjwala Sharma.\\n',\n",
       " 'A total of 737 deaths have occurred till date.\\n',\n",
       " 'The government relies on advertisement and marketing to create illusions in peoples minds.\\n',\n",
       " 'The security personnel are investigating the matter.\\n',\n",
       " 'Defence Minister Rajnath Singh\\n',\n",
       " '5, for two-wheelers Rs.\\n',\n",
       " 'His dream was to become a doctor.\\n',\n",
       " 'Our government has given more than Rs.800,000 crores to the farmers by purchasing wheat and paddy in the five years.\\n',\n",
       " 'The recently released trailer of the film has received an exceptional response from the audience.\\n',\n",
       " 'Only 16 patients are still under treatment.\\n',\n",
       " 'Functional Testing\\n',\n",
       " 'And when those beasts give glory and honour and thanks to him that sat on the throne, who liveth for ever and ever,\\n',\n",
       " 'Opposition members strongly object to it.\\n',\n",
       " 'Former Andhra Pradesh Minister Bhuma Akhila Priya who was arrested in Bowenpally kidnap case got bail.\\n',\n",
       " 'Thus, its important to manage your weight by exercising and eating a healthful diet.\\n',\n",
       " 'He is out of work.\\n',\n",
       " \"Mitigate people's problems\\n\",\n",
       " 'When I told Lidiya about the invitation extended to us, she asked: But what about our home, our garden, and our belongings?\\n',\n",
       " 'Some have been against his comments, others have supported him.\\n',\n",
       " 'He alleged that Mr KCR has been cheating the people with a false propaganda.\\n',\n",
       " 'Chief Minister YS Jagan Mohan Reddy has issued the instructions to complete the pending irrigation projects as per the decided timelines.\\n',\n",
       " 'It will have 100 Questions.\\n',\n",
       " 'But something is happening.\\n',\n",
       " 'Why at all?\\n',\n",
       " 'Wheres the study?\\n',\n",
       " 'The cops detained the accused for interrogation and launched a probe into the matter.\\n',\n",
       " 'Fact is that there is no truth in this allegation.\\n',\n",
       " 'Here are the details for you-\\n',\n",
       " \"That's what I thought.\\n\",\n",
       " 'I found this scribbled note among my wifes papers shortly after her death in May 1994.\\n',\n",
       " 'Besan is called Kadale hittoo in Kannada.\\n',\n",
       " 'Cow dung is used as fertilizer.\\n',\n",
       " '419 crore.\\n',\n",
       " 'They invest in it.\\n',\n",
       " 'Are you scared of themIJ\\n',\n",
       " 'Immovable anger\\n',\n",
       " 'The reason behind the clash is suspected to be a family dispute.\\n',\n",
       " 'So pick carefully.\\n',\n",
       " 'The financial assistance has been given under Modified Marketing Development Assistance (MMDA) scheme of KVIC which is directly linked with the production activities\\n',\n",
       " 'And brothers and sisters, if even a single Indian stays away from development, that too is not acceptable to India.\\n',\n",
       " 'Film director Ram Gopal Varma recently announced his upcoming film, Thriller on social media.\\n',\n",
       " 'Demonetisation had a horrendous impact on the economy.\\n',\n",
       " 'Krishnam Vande Jagadgurum is a film directed by Krish and jointly produced by Saibabu Jagarlamudi and Rajeev Reddy of First Frame Entertainment.\\n',\n",
       " '\"\"\", said Arjun.\"\\n',\n",
       " 'But with time, many things have changed.\\n',\n",
       " 'They need to be watched carefully.\\n',\n",
       " 'The impact of her death was felt not just in India but across the world.\\n',\n",
       " 'To have Jehovah as our one and only God, we should give him our exclusive devotion.\\n',\n",
       " 'He doesnt think much about it.\\n',\n",
       " 'The news was a shock.\\n',\n",
       " '\"The command used to handle \"\"trash\"\" URLs, if enabled.\"\\n',\n",
       " 'But what did we want?\\n',\n",
       " 'About 70 per cent of Indians spend all their income on healthcare and buying drugs.\\n',\n",
       " 'Then said the prophet Jeremiah unto Hananiah the prophet, Hear now, Hananiah. The LORD hath not sent thee. but thou makest this people to trust in a lie.\\n',\n",
       " 'Shew the things that are to come hereafter, that we may know that ye are gods: yea, do good, or do evil, that we may be dismayed, and behold it together.\\n',\n",
       " 'The film is being made in Tamil and Telugu.\\n',\n",
       " 'Total Rs.\\n',\n",
       " '40 thousand.\\n',\n",
       " 'The subjects included are Physics, Chemistry, and Mathematics.\\n',\n",
       " 'Qualification: MCA/BE (Computer Science / IT) / B.Tech (Computer Science / IT).\\n',\n",
       " \"Congress demanded Uttar Pradesh Chief Minister Yogi Adityanath and Health Minister Siddharth Nath Singh's resignations, blaming the state government for its sheer failure in providing proper medical facilities to the patients in time.\\n\",\n",
       " 'Shops are closed.\\n',\n",
       " 'All films are different.\\n',\n",
       " 'It also provides Life Insurance worth Rs 4 lakh.\\n',\n",
       " 'He fell asleep.\\n',\n",
       " 'Give us jobs\\n',\n",
       " 'It was 7.3 percent.\\n',\n",
       " 'However, the BJP is yet to react to the same.\\n',\n",
       " 'Add seasoning of salt, pepper and parsley.\\n',\n",
       " 'she had posted on her Twitter account.\\n',\n",
       " 'They play here.\\n',\n",
       " 'Yahweh your God will circumcise your heart, and the heart of your seed, to love Yahweh your God with all your heart, and with all your soul, that you may live.\\n',\n",
       " 'Security guards killed all three of the assailants.\\n',\n",
       " '16 deaths have taken place so far.\\n',\n",
       " 'But it has been a ball.\\n',\n",
       " 'It can determine the amount of carbon dioxide released by the aquatic systems, so from there again you can indirectly measure the net primary productivity.\\n',\n",
       " 'In a poster, Ram Charan is seen riding on a bike while Jr NTR is seen riding on the horse.\\n',\n",
       " 'Rooms & facilities\\n',\n",
       " 'Home surroundings should be kept clean.\\n',\n",
       " \"The first part of the film will trace the journey of NTR's career in films.\\n\",\n",
       " \"Ahmedabad: A mysterious incident recently surfaced in Gujarat's capital.\\n\",\n",
       " 'The temple is located near Allahabad fort near the banks of the river Ganga and when the river is flooded, the temple gets submerged in the water\\n',\n",
       " 'India are out of the tournament.\\n',\n",
       " 'Expectations are riding high over this film.\\n',\n",
       " 'The video was uploaded by his daughter on Twitter.\\n',\n",
       " '\"But it doesn\\'t happen\"\".\"\\n',\n",
       " 'and do not be casual.\\n',\n",
       " 'He demanded a CBI probe into the irregularities.\\n',\n",
       " 'Something had to be done.\\n',\n",
       " 'Bollywood star Ajay Devgn will play an important role in the film.\\n',\n",
       " 'It is the second tallest building in Beijing.\\n',\n",
       " 'Heavy rains forecast for Telangana\\n',\n",
       " 'The line carries 2.5 million tons of LPG annually.\\n',\n",
       " 'Chief Minister KCR\\n',\n",
       " 'In the latest review taken by Smt Nirmala Sitharaman the following progress has been reported so far:\\n',\n",
       " 'Very happy.\\n',\n",
       " 'In the first phase, healthcare workers & frontline workers will be vaccinated.\\n',\n",
       " 'Ginger paste - 1/2 tsp\\n',\n",
       " 'Music and dance programme\\n',\n",
       " 'He is undergoing treatment at a private hospital.\\n',\n",
       " 'Lost faith\\n',\n",
       " 'Shah Rukh plays a dwarf in the movie.\\n',\n",
       " 'In Rajasthan, Congress leads by a strong margin whereas in Madhya Pradesh and Chhattisgarh the fight is closer.\\n',\n",
       " 'See these photos.\\n',\n",
       " 'Total four types of transportation are available in our country.\\n',\n",
       " 'Questions are then asked.\\n',\n",
       " 'In this way they were earning their living.\\n',\n",
       " 'Lets look at each one in more detail.\\n',\n",
       " 'No official announcement has yet been made about this.\\n',\n",
       " 'He lodged a complaint to the Election Commission in this regard.\\n',\n",
       " '\"\"\"As long as I am railway minister, no train will operate between Pakistan and India.\"\"\"\\n',\n",
       " 'What is overweight?\\n',\n",
       " 'His body was wrapped in the flag of Tamil Eelam as he wished.\\n',\n",
       " 'They both stared at each other.\\n',\n",
       " ', vide number S.O.748 (E), dated the 3rd August, 2001, namely:-\\n',\n",
       " '2 inch full HD display\\n',\n",
       " 'The couple had two children son, actor Ranbir Kapoor, and daughter, Riddhima Kapoor Sahni.\\n',\n",
       " '2 lakh crore and Rs.\\n',\n",
       " 'You might also have conflicts with your partner.\\n',\n",
       " 'It is not an offbeat movie.\\n',\n",
       " 'We have no security.\\n',\n",
       " 'This is not real freedom.\\n',\n",
       " 'These dried cakes can be stored for many years.\\n',\n",
       " 'Arent you ashamed of talking like that?\\n',\n",
       " 'Vaishyas attain higher planes through charity and hospitality.\\n',\n",
       " 'Lethal kiss\\n',\n",
       " 'No doubts about it!\\n',\n",
       " 'It also led to massive traffic jam.\\n',\n",
       " 'He lost the focus.\\n',\n",
       " 'Be careful about health.\\n',\n",
       " 'Then, things went downhill.\\n',\n",
       " 'Police was summoned subsequently.\\n',\n",
       " 'The governments have to work on that.\\n',\n",
       " 'The news comes ...\\n',\n",
       " 'Hyderabad: Vote counting begins for the GHMC polls. Who will win?\\n',\n",
       " 'It helps to remove toxins in the body.\\n',\n",
       " 'The police action galvanised the students.\\n',\n",
       " 'The children become tired.\\n',\n",
       " 'Kohli was given the player of the match award for his match-winning century.\\n',\n",
       " 'It has three cameras on the rear to begin with.\\n',\n",
       " 'There was a traffic jam on the main highway.\\n',\n",
       " 'the Department for the safety\\n',\n",
       " 'The new Parliament building will be constructed close to the existing structure under the Central Vista redevelopment project.\\n',\n",
       " 'Maharashtra continued to top the chart with more than 23,000 new recoveries, while both Karnataka and Andhra Pradesh accounted for over 10,000 single-day recoveries, it said.\\n',\n",
       " 'Consider, for example, the words of the apostle Paul concerning conditions that would exist during these last days: In the last days critical times hard to deal with will be here.\\n',\n",
       " 'Thats why he was upset.\\n',\n",
       " 'The next day, the bubble burst.\\n',\n",
       " '30 lakh.\\n',\n",
       " 'India Win Womens Hockey Asia Cup Beating China\\n',\n",
       " 'He has been fighting all his life.\\n',\n",
       " 'With a Micro SD card, the memory is expandable to 512 GB.\\n',\n",
       " 'Most African American miners left by 1852 to nearby mining sites that were more successful.\\n',\n",
       " 'As a result, passengers are facing hardship.\\n',\n",
       " 'Sixtus replied that this was his responsibility and that Wessel should choose something for himself.\\n',\n",
       " 'Delhi Chief Minister Arvind Kejriwal had posted this picture on Twitter.\\n',\n",
       " 'There are two types of measles:\\n',\n",
       " 'Cinematography: Sameer Reddy\\n',\n",
       " 'Despite this, the BJP failed to form government.\\n',\n",
       " 'Day by day atrocities against women are increasing.\\n',\n",
       " 'Based on the victims complaint, police registered a case and took the accused into custody.\\n',\n",
       " 'Stylish star Allu Arjun and Pooja Hegde are busy with forthcoming flick Ala Vaikunthapurramuloo which is being helmed by Trivikram Srinivas.\\n',\n",
       " \"And he said to the king of Israel, Put thine hand upon the bow. And he put his hand upon it: and Elisha put his hands upon the king's hands.\\n\",\n",
       " 'Anything might happen\\n',\n",
       " 'MLC Dokka Manikya Vara Prasad, former MLA Lingamsetty Eswara Rao, VGTM UDA former chairman Vanukuri Srinivas Reddy were among those who participated in the programme.\\n',\n",
       " 'He led boycotts of places to achieve these goals.\\n',\n",
       " 'In the past too several such incidents were reported in the state.\\n',\n",
       " 'His theories\\n',\n",
       " 'And the light of a candle shall shine no more at all in thee. and the voice of the bridegroom and of the bride shall be heard no more at all in thee: for thy merchants were the great men of the earth. for by thy sorceries were all nations deceived.\\n',\n",
       " 'looking the other way\\n',\n",
       " 'The heroine...\\n',\n",
       " 'Tribute to Martyrs\\n',\n",
       " 'Nawanshahr town is said to have been built by an Afghan Military Chief, Nausher Khan.\\n',\n",
       " '\"Simon Msanjila, mines ministry permanent secretary, at an event at Simanjiro district in Tanzania\\'s northern Manyara region said that \"\"Today\\'s event is to recognise the two largest tanzanite gemstones in history since the beginning of mining activities in Mirerani.\"\"\"\\n',\n",
       " 'Hyderabad won 3-2\\n',\n",
       " 'He urged the government to take stern action against the accused.\\n',\n",
       " 'There is no cure for it.\\n',\n",
       " 'He lauded the hard work of the engineers and laborers and active support of the state government for completing this project on time, in spite of the challenging terrain\\n',\n",
       " 'Bollywood actress Kangana Ranaut, in her recent tweet, has attacked Hrithik Roshan.\\n',\n",
       " 'How to reach Kodagu?\\n',\n",
       " 'Can You Guess Them?\\n',\n",
       " 'They dont die.\\n',\n",
       " 'What do you need\\n',\n",
       " 'Several films have been shot here in the recent past.\\n',\n",
       " 'The details on the film will be announced soon.\\n',\n",
       " 'For me thats the challenge.\\n',\n",
       " 'Women generally live longer than the men as it is customary to marry men of higher age.\\n',\n",
       " 'One way is by being present for the singing of Kingdom songs.\\n',\n",
       " 'This victory is a tribute to the team.\\n',\n",
       " 'Sanand was also a member of National River Ganga Basin Authority during the UPA tenure.\\n',\n",
       " 'There are no answers to these questions.\\n',\n",
       " 'Everything is resolved.\\n',\n",
       " 'Theres no fear.\\n',\n",
       " 'The first look of the film has been unveiled.\\n',\n",
       " 'And Jacob rose up early in the morning, and took the stone that he had put for his pillows, and set it up for a pillar, and poured oil upon the top of it.\\n',\n",
       " 'Is it not lawful for me to do what I will with mine own? Is thine eye evil, because I am good?\\n',\n",
       " 'Maximizing space\\n',\n",
       " 'When we wish to know what caused a fact we look for another fact.\\n',\n",
       " 'For beginners, with inadequate experience in handling databases it is advisable do not create cumbersome codes.\\n',\n",
       " 'There are children also.\\n',\n",
       " 'Considering each zones in the country, Maharashtra took the top-slot for the West-Zone. Kerala for the South-Zone. Uttar Pradesh for North-Zone and West-Bengal for the East-Zone\\n',\n",
       " 'You are not free at all.\\n',\n",
       " 'I will make a spectacle of you, God said.\\n',\n",
       " 'Who asked you to go?\\n',\n",
       " 'This doesnt require a lot of effort.\\n',\n",
       " 'coronavirus scare\\n',\n",
       " 'Similar is the case with Karnataka.\\n',\n",
       " 'A relationship between man and woman.\\n',\n",
       " 'Crazy who ?\\n',\n",
       " \"What is 'Jersey' about?\\n\",\n",
       " 'Building information modeling (BIM)\\n',\n",
       " 'Things will soon change.\\n',\n",
       " 'Is this friendship?\\n',\n",
       " 'I am very blunt.\\n',\n",
       " 'He said a special investigation team (SIT) had been constituted to get to the bottom of the case.\\n',\n",
       " 'brain and mind.\\n',\n",
       " 'England cricketer...\\n',\n",
       " 'The ruling TRS party had fielded Surabhi Vani Devi, daughter of PV Narasimha Rao, the late prime minister of India as its candidate.\\n',\n",
       " 'He has changed his opinion.\\n',\n",
       " 'There isnt time.\\n',\n",
       " 'Its also easy to install.\\n',\n",
       " 'Some are of its own making.\\n',\n",
       " 'An empowered woman contributes equally to every decision, at every level, as much as anyone else.\\n',\n",
       " 'Why are you doing this injustice.\\n',\n",
       " 'What else will be different?\\n',\n",
       " '4 million.\\n',\n",
       " 'The living room\\n',\n",
       " 'Crops in thousands of acres have been affected.\\n',\n",
       " 'Its very expensive.\\n',\n",
       " 'This, the government says, would come into effect immediately.\\n',\n",
       " \"The armorials on the Founder's Window represent all of the interests present at the founding of the University of Bristol including the Wills and Fry families.\\n\",\n",
       " 'BMWs 7 Series.\\n',\n",
       " 'Nothing new there!\\n',\n",
       " 'Modern technology should be used.\\n',\n",
       " 'Constitutional institutions are being undermined.\\n',\n",
       " 'Happy Diwali to everyone.\\n',\n",
       " 'Cricketer-turned commentator Sanjay Manjrekar has criticised Shastri on the incident via the social media.\\n',\n",
       " 'This is another negative.\\n',\n",
       " 'Design tips\\n',\n",
       " '40 CRPF jawans were killed in the attack.\\n',\n",
       " 'Lexus has launched its flagship sedan LS 500h in India\\n',\n",
       " 'He prayed to God to rest her soul in peace.\\n',\n",
       " 'The scenic water park is skirted by groves of chikoo, coconut and mango trees\\n',\n",
       " 'The hiring of personnel for these posts is done by Centre for Personnel Talent Management (CEPTAM).\\n',\n",
       " 'But there is no clarity on this.\\n',\n",
       " 'What are its benefits?\\n',\n",
       " 'Police subsequently reached the sight to assess the accident.\\n',\n",
       " 'Top skills\\n',\n",
       " 'This is not very safe.\\n',\n",
       " 'he wrote.\\n',\n",
       " 'Again she said, Sure.\\n',\n",
       " 'Audio Track #%d\\n',\n",
       " '12 lakh, and Rs.\\n',\n",
       " 'Telangana Chief Minister K Chandrashekar Rao, Andhra Pradesh Chief Minister N Chandrababu Naidu and Tamil Nadu Chief Minister Edappadi K Palaniswami were among those in attendance.\\n',\n",
       " 'Hamm said.\\n',\n",
       " 'Everybody does what he likes.\\n',\n",
       " 'Science, too, validates this.\\n',\n",
       " '. Upon presentation of an official note by the local Ministry of Foreign Affairs along with the visa application of the teachers and their dependent spouse and children, a single entry Service/Official gratis visa of three months validity will be issued, within twenty working days.\\n',\n",
       " 'It happens at night.\\n',\n",
       " 'This Anil Paduri directorial is produced by Puri Jagannadh and Charmee Kaur under Puri Connects banner.\\n',\n",
       " '\"Good days are coming.\"\"\"\\n',\n",
       " 'Charges against officers.\\n',\n",
       " 'As a result, motorists would be inconvenienced.\\n',\n",
       " 'Similarly, there has been a spike in investment.\\n',\n",
       " 'Where is the joy in all this?\\n',\n",
       " 'I believe in doing things when its ready.\\n',\n",
       " 'Palanisamy is chief minister of Tamil Nadu while Panneerselvam is his deputy.\\n',\n",
       " 'Category: 1923 births\\n',\n",
       " 'State Election Commissioner Nimmagadda Ramesh Kumar.\\n',\n",
       " 'Who were these people?\\n',\n",
       " 'This is called quackery.\\n',\n",
       " 'Only 500 crores.\\n',\n",
       " 'De Villiers has made similar comments about Kohli.\\n',\n",
       " 'In this, users are given only 12 GB of data.\\n',\n",
       " 'Hard work will get you results.\\n',\n",
       " 'This app is only applicable for Android users.\\n',\n",
       " 'You got no passion.\\n',\n",
       " 'There was a big noise about it at the time.\\n',\n",
       " 'All the greats have to sacrifice something.\\n',\n",
       " 'He said he is in good health.\\n',\n",
       " 'He was admitted to a hospital in Delhi.\\n',\n",
       " 'How is all this possible?\\n',\n",
       " '\"\"\"Who is she?\"\\n',\n",
       " 'However, the second was a different story.\\n',\n",
       " 'Any changes?\\n',\n",
       " 'More commonly today, oven wilting is used for the initial dehydration.\\n',\n",
       " 'Why it happened\\n',\n",
       " 'Pakistan had made 165 in the first innings.\\n',\n",
       " 'Rs 3 crore loss\\n',\n",
       " \"It is of Incredible opportunities, and about India's Credible Policies.\\n\",\n",
       " 'If fish farming is taken up in all these, the State would have enormous wealth of fish and all the fishermen in the State can get employment.\\n',\n",
       " 'Blast suicide bomber kills 10 in Afghanistan\\n',\n",
       " 'Mumbai Indians beat Delhi Capitals by 9 wickets\\n',\n",
       " 'Two missiles were fired.\\n',\n",
       " 'Here again, the facts are evident.\\n',\n",
       " 'The tree can grow up to 40 metres.\\n',\n",
       " 'He immediately picked it up.\\n',\n",
       " 'That said...\\n',\n",
       " \"However Yahweh would not destroy Judah, for David his servant's sake, as he promised him to give to him a lamp for his children always.\\n\",\n",
       " 'The song will be released soon.\\n',\n",
       " 'Winters are warm.\\n',\n",
       " 'Good news for SBI Customers!\\n',\n",
       " 'A skyscraper is a large continuously habitable building having multiple floors.\\n',\n",
       " 'The youth is the future.\\n',\n",
       " 'The BJP is keen to retain power in Rajasthan, Madhya Pradesh and Chhattisgarh.\\n',\n",
       " 'Master is directed by Lokesh Kanagaraj.\\n',\n",
       " 'Many face financial worries.\\n',\n",
       " 'Rainwater in hospital\\n',\n",
       " 'And what does that committee do?\\n',\n",
       " 'There is a question.\\n',\n",
       " 'People have been raving about the film.\\n',\n",
       " '\"Introducing Conan as \"\"probably the world\\'s most famous dog\"\", Trump said he had given the dog a plaque and called the canine commando \"\"so brilliant, so smart\"\".\"\\n',\n",
       " 'Dehradun is the Capital of Uttarakhand.\\n',\n",
       " 'In view of Pauls counsel to consider things that are righteous and chaste, what should be avoided? The word righteous means being right in Gods eyes meeting his standards.\\n',\n",
       " 'It is also rich in minerals like Calcium, Phosphorus, Chlorine, Sodium, Potassium, Iron, Magnesium, and Iodine.\\n',\n",
       " 'Deaths are on the rise.\\n',\n",
       " 'The film is direcrted by PS Mithran.\\n',\n",
       " 'Without two, there is no one.\\n',\n",
       " 'He alleged that the government has failed on all fronts.\\n',\n",
       " 'It helps protect against the cold.\\n',\n",
       " 'Personal Study\\n',\n",
       " 'Theres a lot of work behind this.\\n',\n",
       " 'Ministry of Chemicals and Fertilizers Shri Gowda took stock of progress of Talcher Fertilizers Ltd Minister of Chemicals and Fertilisers Shri DV Sadananda Gowda took stock of progress of Talcher Fertilisers Limited (TFL).\\n',\n",
       " 'Chasing is one of them.\\n',\n",
       " 'Should you say something?\\n',\n",
       " '( b) What do Deuteronomy 23: 21, 23 and Psalm 15: 4 impress upon you about making a vow to God?\\n',\n",
       " 'The Prime Minister also spoke of the importance of value addition in farming.\\n',\n",
       " 'around Rs.\\n',\n",
       " 'TMC MPs protest against Centre in Parliament premises\\n',\n",
       " 'In fulfillment of Isaiah 21: 8, what watchman has God had in our time?\\n',\n",
       " 'The singers are singing.\\n',\n",
       " 'That is why he resigned.\\n',\n",
       " 'Defeat BJP in Gujarat Assembly Elections\\n',\n",
       " 'This was reasonably typical of professional rowing at the time.\\n',\n",
       " 'Its almost impossible.\\n',\n",
       " 'But theres hope.\\n',\n",
       " 'His daughter Sheikh Hasina is the current Prime Minister of Bangladesh.\\n',\n",
       " 'Blueberry is rich in antioxidants.\\n',\n",
       " 'Silver price also decreased.\\n',\n",
       " 'True, they make good use of Bible literature so that interested ones can gain more Bible knowledge at their leisure. But they always seek to show people words of Scripture.\\n',\n",
       " 'This happened at Delhi.\\n',\n",
       " 'The function was organised at Dharanidharpur village in Sundargarh district.\\n',\n",
       " 'Mela Patt is a famous festival of Bhaderwah.\\n',\n",
       " 'YSRCP gets new district presidents\\n',\n",
       " 'Stylish star Allu Arjun is enjoying the success of Ala Vaikunthapurramuloo helmed by maverick director, Trivikram.\\n',\n",
       " 'Good response\\n',\n",
       " 'Slide your fingers\\n',\n",
       " 'Dont be suspicious.\\n',\n",
       " 'Call the doctor\\n',\n",
       " 'Trisha will be playing a major character in the movie.\\n',\n",
       " 'The temple boasts of a grand paarna, or fast breaking- hall situated in front of it\\n',\n",
       " 'That is a very harsh word.\\n',\n",
       " 'Are you for or against it?\\n',\n",
       " '\"\"\"The subway is planned to be six metres wide and three metres high.\"\\n',\n",
       " 'The first schedule of the film is currently taking place in Pune.\\n',\n",
       " 'On the ninth day, the film scored Rs.\\n',\n",
       " 'Not that difficult at all.\\n',\n",
       " \"Don't be).\\n\",\n",
       " 'Bigg Boss Telugu season 3 will go on air tonight.\\n',\n",
       " 'But the hosts...\\n',\n",
       " 'By uttering the first lie ever spoken and by slandering Jehovah, Satan made himself the father of the lie.\\n',\n",
       " 'India won the game by 48 runs.\\n',\n",
       " 'They include doctors and paramedics.\\n',\n",
       " 'Its filled with water.\\n',\n",
       " 'Please dont ask again.\\n',\n",
       " 'Interestingly, he did not sign the letter.\\n',\n",
       " 'So I bought a belt according to the word of Yahweh, and put it on my waist.\\n',\n",
       " '1 Apr 2001.\\n',\n",
       " '53,500, the 64GB at Rs.\\n',\n",
       " 'Hyderabad has developed tremendously during the last decade.\\n',\n",
       " 'What we have essentially done is replaced these two distributed forces with two point forces, 1 mu times N acting at the same d location.\\n',\n",
       " 'How is strep throat treated?\\n',\n",
       " 'Police said they were looking for the attacker.\\n',\n",
       " 'Video: Toyota Fortuner And Mahindra XUV 500 Head-On Collision Leaves Three Dead\\n',\n",
       " \"Pawan Kalyan's recent movies Katamarayudu and Sardaar Gabbar Singh tanked at the box office.\\n\",\n",
       " 'It is the largest state in the country.\\n',\n",
       " 'However, nothing more about the film is known.\\n',\n",
       " 'For the dairy, Rs.\\n',\n",
       " 'The movie has generated good among the audiences.\\n',\n",
       " 'The ceremony will be organised at Ramleela Ground.\\n',\n",
       " '\"\"\"India desires a prosperous and progressive Pakistan at peace with its neighbours,\"\" he said.\"\\n',\n",
       " 'Congress leader Salman Khurshid, CPI-ML politburo member Kavita Krishnan, student activist Kawalpreet Kaur, scientist Gauhar Raza and advocate Prashant Bhushan find a mention in disclosure statements of accused persons in a chargesheet filed by the Delhi Police last week alleging a conspiracy in the violence in north-east district of the national capital.\\n',\n",
       " 'And they shall not come near unto me, to do the office of a priest unto me, nor to come near to any of my holy things, in the most holy place: but they shall bear their shame, and their abominations which they have committed.\\n',\n",
       " 'How do you select a slip fielder?\\n',\n",
       " 'He was shifted to a house detention at his Delhi residence.\\n',\n",
       " 'are available in the market.\\n',\n",
       " 'It is their personal opinion.\\n',\n",
       " 'Is this any criteria for selection?\\n',\n",
       " \"Maharashtra's Six Districts To Be Diesel-free, Says Union Minister Nitin Gadkari\\n\",\n",
       " '10 lakh.\\n',\n",
       " \"Indian skipper Virat Kohli also praised the team's effort.\\n\",\n",
       " 'There are apprehensions.\\n',\n",
       " 'Sources said India had handed over to the US the evidence of Pakistan using F-16 fighter planes in last weeks aerial combat with India.\\n',\n",
       " 'Can this be done at all?\\n',\n",
       " 'Brahmastra logo was launched using 150 drones.\\n',\n",
       " 'He said that in a recent interview.\\n',\n",
       " 'Both the players scored half-centuries.\\n',\n",
       " 'In 1995 Bavaria introduced direct democracy on the local level in a referendum.\\n',\n",
       " 'The latter work, in addition to offering much linguistic information, describes the life, farming methods, and beliefs of the Bihar peasantry.\\n',\n",
       " 'But I have no hate.\\n',\n",
       " 'With MLC Yadava Reddy being disqualified, elections for Telangana Legislative Council have become inevitable.\\n',\n",
       " 'If cancer is detected at an early stage it is easy to treat.\\n',\n",
       " 'No one has problem.\\n',\n",
       " 'Shri Subramaniam in his address said that the problem of drug abuse and illicit trafficking is at Society level andso we have to involve Communities along with health department officials with focus on our youths.\\n',\n",
       " 'Officials said they are investigating the matter.\\n',\n",
       " 'It is not possible to give employment to everyone.\\n',\n",
       " 'You cant fight, can you?\\n',\n",
       " 'You must take a bath.\\n',\n",
       " \"Duplicating file %'d of %'d\\n\",\n",
       " 'However, an official announcement in this regard is yet to be made.\\n',\n",
       " 'What task did the brothers begin to undertake in 1919, and why was it a challenge?\\n',\n",
       " 'In 2015, Krotov tried to unsuccessfully appeal his sentence.\\n',\n",
       " \"Here's what Pawan Kalyan posted..\\n\",\n",
       " 'Rahul Gandhi had last week cancelled his two-day election visit to Puducherry, Tamil Nadu and Kerala, saying he was down with high fever.\\n',\n",
       " 'He is the only son.\\n',\n",
       " 'Gandhari is a hill fort located near Bokkalagutta, in Mandamarri Mandal in Mancherial district in the south Indian state of Telangana.\\n',\n",
       " 'After considering this for a moment, the first brother responded, You are right.\\n',\n",
       " 'They were shifted to a local hospital.\\n',\n",
       " 'His relatives rushed him to hospital.\\n',\n",
       " 'was registered.\\n',\n",
       " 'Of those interviewed, 89 percent said that it takes good health. 79 percent mentioned a satisfying marriage or partnership. 62 percent pointed to the rewards of parenthood. and 51 percent thought a successful career was needed for happiness.\\n',\n",
       " 'The incident took place at the Gandhi Nagar police station.\\n',\n",
       " 'The accident took place on Yamuna Expressway in Greater Noida.\\n',\n",
       " 'Keep these things in mind\\n',\n",
       " 'Because it doesnt have any commercial possibilities.\\n',\n",
       " 'More details about the film will be revealed soon.\\n',\n",
       " 'We will fight for this.\\n',\n",
       " \"However, it's unclear where the incident took place.\\n\",\n",
       " 'Trump handed Narayanan, who was wearing a bright coral pink sari, her Certificate of Citizenship.\\n',\n",
       " 'I attend a government school.\\n',\n",
       " 'She later lodged a complaint with the police.\\n',\n",
       " 'Electric cars.\\n',\n",
       " 'The cops who recieved the information alerted the fire officials.\\n',\n",
       " 'It has to be said.\\n',\n",
       " 'After taking a decision for the development of Telangana only they agreed for the United Andhra Pradesh.\\n',\n",
       " 'Congress President Rahul Gandhi slammed the RSS and the BJP for the plight of Dalits.\\n',\n",
       " 'They have understood that the soul, rather than being immortal, can die and be destroyed.\\n',\n",
       " 'Within a short duration of its release, the video became viral on the social media.\\n',\n",
       " 'There is no religious bias.\\n',\n",
       " 'The quake measured 4.2 on the Richter scale.\\n',\n",
       " 'Priyanka Chopra and Nick Jonas are expected to get married later next month in Rajasthan.\\n',\n",
       " 'You should drink plenty of water.\\n',\n",
       " 'Two mobile phones were recovered from the alleged accused.\\n',\n",
       " 'Passengers must wear face covers/masks\\n',\n",
       " 'Total 200 motorized tricycle were distributed in the camp\\n',\n",
       " 'Former Chief Election Commissioner TN Seshan passes away\\n',\n",
       " 'In furtherance of its reforms agenda, the Government is ensuring the setting up of dedicated Special Manufacturing Zones for the production of Pharma, Medical Devices and APIs in 6 States\\n',\n",
       " 'Police department\\n',\n",
       " 'This was followed up by the Union minister of finance, Nirmala Sitharaman.\\n',\n",
       " 'Years later, before she divorced him, his wife succeeded in having him committed briefly to a mental institution because of his religious beliefs!\\n',\n",
       " 'Ishant Sharma has completed 300 Test wickets.\\n',\n",
       " 'It is not them\\n',\n",
       " 'The central government has allotted Rs.\\n',\n",
       " 'She really is a goddess.\\n',\n",
       " 'It was pretty funny to see that.\\n',\n",
       " 'The Prime Minister said that the Vice President had been a career diplomat, and he had benefited from the Vice Presidents insights on diplomatic issues on several occasions.\\n',\n",
       " 'Recently, the trailer of the film was released.\\n',\n",
       " 'Police reached the spot and booked a case against the car holder.\\n',\n",
       " 'All this was recorded on camera.\\n',\n",
       " 'Good films all.\\n',\n",
       " 'It aimed at providing specialised health services in the rural areas.\\n',\n",
       " 'Working as teacher.\\n',\n",
       " 'Ive just seen the trailer.\\n',\n",
       " 'The issue had created a political outrage across the country.\\n',\n",
       " '\"Go away from here.\"\"\"\\n',\n",
       " 'Home Latest News Maha CM Devendra Fadnavis quits\\n',\n",
       " 'Each area of subdivision is identified in the numbering plan plan with a routing code.\\n',\n",
       " 'But no decision seems to have been taken just yet.\\n',\n",
       " 'Know in detail.\\n',\n",
       " 'All you need is chickpea flour and water.\\n',\n",
       " 'However, are they going to get penalised for that?\\n',\n",
       " 'Firing also took place between the two sides.\\n',\n",
       " 'The Prof.P.C. Mahalanobis National Award winner, 2020 will be felicitated during the event.\\n',\n",
       " 'Change the channel.\\n',\n",
       " 'Teach your children Bible truth whenever possible (See paragraph 10)\\n',\n",
       " 'He advised Pakistan not to meddle in the internal affairs of India.\\n',\n",
       " \"But I don't know who he was.\\n\",\n",
       " 'Perturbed father commits suicide\\n',\n",
       " 'Here are some tips to grow the herbs.\\n',\n",
       " 'The fire department was informed about the incident.\\n',\n",
       " 'Four people were killed and one seriously injured after the truck fell on them.\\n',\n",
       " 'Chennai: Actor-turned-politician Kamal Haasans party Makkal Needhi Maiam (MNM) will not be contesting in the upcoming local body elections in Tamil Nadu.\\n',\n",
       " \"Let's check out the review of the movie.\\n\",\n",
       " 'The prices of onions are soaring day by day across the country.\\n',\n",
       " 'Its still going on.\\n',\n",
       " 'But he did not use it.\\n',\n",
       " 'Locals were initially scared.\\n',\n",
       " 'There are various other benefits.\\n',\n",
       " 'This gives them a cushion.\\n',\n",
       " 'Here is Ram Gopal Varmas letter.\\n',\n",
       " 'Hezekiah, who may have written Psalm 119, chose the way of faithfulness.\\n',\n",
       " 'Oscars 2019 are here.\\n',\n",
       " 'These men are struggling with an addiction!\\n',\n",
       " 'The teaser and trailer raised good expectations over this film.\\n',\n",
       " 'The active cases now comprise only 6.19% of the total positive cases of the country\\n',\n",
       " 'Set the selected profile as the default for new terminal sessions\\n',\n",
       " 'Their second child, a girl again, was born in the forests.\\n',\n",
       " 'Why dont you say something?\\n',\n",
       " 'Unknown to him, his wife and mother-in-law have also invited people to be the godfather.\\n',\n",
       " 'It runs on the Android Ice Cream Sandwich operating system.\\n',\n",
       " 'I also follow a healthy diet.\\n',\n",
       " 'The incident took place in Chilkalguda police station limits.\\n',\n",
       " \"One such incident happened in Hyderabad city's Chaderghat.\\n\",\n",
       " 'Safety and Efficacy\\n',\n",
       " 'Hence, it is beneficial for blood pressure patients.\\n',\n",
       " \"It isn't getting any easier these days.\\n\",\n",
       " 'Yoga Guru Swami Ramdev has strongly condemned the attack by Pakistanis on Nanakana Sahib Gurdwara in Pakistan.\\n',\n",
       " 'I dont know how to go about it.\\n',\n",
       " 'It is rich in Vitamin C, folic acid, potassium and pectin.\\n',\n",
       " \"Don't ask.\\n\",\n",
       " \"Argentina's per capita income was 70% higher than Italy's, 90% higher than Spain's, 180% higher than Japan's and 400% higher than Brazil's.\\n\",\n",
       " 'Rakesh agreed.\\n',\n",
       " 'Vaibhav Pichad is from Akole tehsil in Ahmednagar district and is the son of former NCP minister Madhukar Pichad.\\n',\n",
       " 'We have enough money.\\n',\n",
       " 'Police Station - Bisfi\\n',\n",
       " 'The business has been good.\\n',\n",
       " 'Each team will play 10 league matches.\\n',\n",
       " 'He said that details of the project will be disclosed shortly.\\n',\n",
       " 'Addressing listeners of all Community Radio stations, the Minister exhorted people to keep up their fight against Coronavirus.\\n',\n",
       " 'Cash transaction of above Rs 3 lakh not to be permitted.\\n',\n",
       " 'First name.\\n',\n",
       " 'But its unclear if that will be the case.\\n',\n",
       " 'Its not us.\\n',\n",
       " 'There are many such stories.\\n',\n",
       " 'A Vacation From Disease\\n',\n",
       " 'Fires dont kill people.\\n',\n",
       " 'Its a family drama.\\n',\n",
       " 'The dough should be soft and pliable.\\n',\n",
       " 'I could understand nothing of the story.\\n',\n",
       " 'Thoughts On The Maruti Suzuki Eeco BS6\\n',\n",
       " 'A video of the same was released.\\n',\n",
       " 'The students are divided into four houses upon their intake in the college: Agni, Akash, Prithvi and Vayu.\\n',\n",
       " 'The interview will be of 100 marks.\\n',\n",
       " 'Vishal-Shekhar has rendered the music for the film.\\n',\n",
       " 'Most important cities are located in the south, near this lake, including the capital Kampala and the nearby city of Entebbe.\\n',\n",
       " 'Its all gone.\\n',\n",
       " 'And it is awesome really.\\n',\n",
       " 'Routine surveillance in containment zones and screening at points of entry:\\n',\n",
       " 'From 1980 to 1990, he helped the BJP expand in the four Southern states of Kerala, Tamil Nadu, Karnataka and Andhra Pradesh.\\n',\n",
       " 'Ill tell you a little story.\\n',\n",
       " 'This is a special trait.\\n',\n",
       " 'Nitish Kumar sworn-in as Bihar CM for sixth time\\n',\n",
       " 'Your next film?\\n',\n",
       " 'TDP General Secretary Nara Lokesh\\n',\n",
       " 'But the film didnt start on time.\\n',\n",
       " 'They were demanding immediate arrest and action against the accused.\\n',\n",
       " 'However, as you age, this recedes.\\n',\n",
       " 'Recently released motion poster has created tremendous response.\\n',\n",
       " 'Whats to understand?\\n',\n",
       " 'It will have no impact on the UPA.\\n',\n",
       " 'Goyal will hold the Finance and Corporate Affairs portfolios on a temporary basis until Jaitley resumes duties.\\n',\n",
       " 'Get sufficient sleep.\\n',\n",
       " 'Kummanam had contested the Vattiyoorkavu Assembly seat in the 2016 elections\\n',\n",
       " 'At first I could not understand.\\n',\n",
       " 'Lets find out what they are:\\n',\n",
       " 'But Jesus taught us to pray: Let your kingdom come.\\n',\n",
       " 'Good news for railway passengers!\\n',\n",
       " 'It weighs 35 kg, he said.\\n',\n",
       " 'The sky is earth.\\n',\n",
       " 'For Mukesh Ambani, it has been a perfect storm, with his wealth down 28 per cent, Hurun Report India Managing Director Anas Rahman said.\\n',\n",
       " 'And they pretty much did that as well.\\n',\n",
       " 'Oh, come on sir!\\n',\n",
       " 'Rajya Sabha member Vijaya Sai Reddy, MPs Mekapati Raja Mohan Reddy, YV Subba Reddy, Vara Prasad, Mithun Reddy, etc participated in the protest.\\n',\n",
       " 'It offers\\n',\n",
       " 'Book Four appeared in 1742, and a complete revision of the whole poem in the following year.\\n',\n",
       " 'However, in order to touch the heart of our listeners, it is often better to use their mother tongue the language that speaks to their deepest aspirations, motives, and hopes. Luke 24: 32.\\n',\n",
       " 'No doubt, they were greatly relieved.\\n',\n",
       " 'We wish him a very happy birthday on this occasion.\\n',\n",
       " 'This is great environment\\n',\n",
       " 'The theatrical life changes also occurred.\\n',\n",
       " 'Men must understand this.\\n',\n",
       " 'Who should not get the vaccine?\\n',\n",
       " \"You don't shave your head.\\n\",\n",
       " '\"He had not even played the first warm-up match.\"\"\"\\n',\n",
       " 'Just stories.\\n',\n",
       " 'The film will be released in Hindi, Tamil, Telugu and Kannada languages.\\n',\n",
       " 'suicide by hanging\\n',\n",
       " 'Reconstruction of the temple started soon after.\\n',\n",
       " 'She has also researched the history of erotica in India.\\n',\n",
       " 'These include firms of fugitives like Mehul Choksi and Vijay Mallya.\\n',\n",
       " 'DBT has provided seed funding for the development of Gennovas novel self-amplifying mRNA-based vaccine candidate for COVID19\\n',\n",
       " 'Environmental concerns\\n',\n",
       " 'Dash Of Beauty\\n',\n",
       " 'Responsibility lies on us .\\n',\n",
       " 'Polling for the Lok Sabha elections was done in seven phases.\\n',\n",
       " \"The car will be based on Hyundai's ix25 that is was revealed in China.\\n\",\n",
       " 'Serial No.8\\n',\n",
       " 'How much education do they have?\\n',\n",
       " '1886 - Cholera outbreak.\\n',\n",
       " 'Good news for motorists!\\n',\n",
       " 'No Rama in BJP state\\n',\n",
       " 'They often resort to distortions, half - truths, and outright falsehoods.\\n',\n",
       " 'BJP president JP Nadda said that Rajiv Gandhi Foundation took money from China compromising the interest of India.\\n',\n",
       " 'Producer Bhushan Kumar shared more details about the film.\\n',\n",
       " 'The device will support a 33W fast charger.\\n',\n",
       " \"I did 'F2' with Anil Ravipudi.\\n\",\n",
       " 'A caring husband recognizes and respects personality differences between him and his wife.\\n',\n",
       " 'Hope for England\\n',\n",
       " 'Other than this, the tax relief to business, relief from contractual commitments to contractors in public procurement and compliance relief to real estate sector were also covered.\\n',\n",
       " 'The victim was shifted to a Niloufer Hospital for treatment.\\n',\n",
       " 'Assam: In Assam during the last 24 hours 169 cases of COVID-19 detected out of 25225 tests conducted.\\n',\n",
       " 'Indian Overseas Bank - Rs.\\n',\n",
       " 'Vivek Agnihotri announces film on Lal Bahadur Shastris controversial death\\n',\n",
       " 'The Bill was introduced in the Lok Sabha the same month.\\n',\n",
       " 'Why the hurry\\n',\n",
       " 'Meanwhile, India has, so far, recorded 73 cases of coronavirus.\\n',\n",
       " 'It has many ancient Vishnu and Shiva temples, and water tanks built during the Hoysala Dynasty.\\n',\n",
       " 'Here are the questions....\\n',\n",
       " 'Do a simple exercise.\\n',\n",
       " 'During interrogation, he told the police that he had masterminded the hijacking of over 100 cars.\\n',\n",
       " 'You can get many opportunities to advance in career.\\n',\n",
       " 'Police arrested the youth and his friend, who accompanied him to the dhaba.\\n',\n",
       " 'Scientists believe there are several reasons.\\n',\n",
       " 'Couple dies after falling off train\\n',\n",
       " '120 crores.\\n',\n",
       " 'The PCC president...\\n',\n",
       " 'It is powered by the Qualcomm Snapdragon 855+ processor and comes with 12GB of RAM and 256GB of storage.\\n',\n",
       " 'He also advised KCR to stop making provocative statements.\\n',\n",
       " \"One twenty Tirupati's might be destroyed.\\n\",\n",
       " 'Another 14 were wounded.\\n',\n",
       " 'Its a terrible fate.\\n',\n",
       " 'Its as good as new.\\n',\n",
       " 'Its price Rs.\\n',\n",
       " \"There is much excitement for Hrithik Roshan and Tiger Shroff's upcoming film.\\n\",\n",
       " 'A bad day at work.\\n',\n",
       " 'The family urged to the government for compensation.\\n',\n",
       " 'BJP And Gujarat Election Results\\n',\n",
       " 'It is a bit tacky.\\n',\n",
       " 'Polls were held for 199 seats.\\n',\n",
       " 'He has denied the accusations against him.\\n',\n",
       " 'Rock salt (limit salt intake to 5 grams (equivalent to a teaspoon) a day.\\n',\n",
       " 'Nagarjuna plays a special role in this film.\\n',\n",
       " 'Kumaraswamy is contesting from Ramanagara and Channapatnatwo adjoining constituenciesin Karnataka assembly elections\\n',\n",
       " 'Some more ideas:\\n',\n",
       " 'The woman suffered serious head injuries and died on the spot.\\n',\n",
       " 'Mytri Movie Makers are producing this film.\\n',\n",
       " 'Chiranjeevi has stricked as hero with the film of KHIDHI ,which is directed by A.Kodandarami Reddy.\\n',\n",
       " 'Do not ignore the warnings of your conscience\\n',\n",
       " 'Pictures surfaced on social media.\\n',\n",
       " 'The incident took place in Palasa town of Srikakulam district.\\n',\n",
       " 'In India, economic activity remains buoyant, but the growth forecast for 2016-17 was trimmed slightly, reflecting a more sluggish investment recovery, the IMF said.\\n',\n",
       " 'He probably didnt know his name.\\n',\n",
       " 'Director: VP Viji\\n',\n",
       " '23 people died.\\n',\n",
       " 'This creates a rift between them.\\n',\n",
       " 'There is no better way of celebrating the festival than in the company of Lord Krishna and his consort Radha\\n',\n",
       " 'However, he failed.\\n',\n",
       " 'Family life\\n',\n",
       " '\"I don\\'t possess THAT much courage.\"\"\"\\n',\n",
       " 'The film will also feature Ram Charan.\\n',\n",
       " 'Godly View of Moral Cleanness, 11 / 1\\n',\n",
       " 'Theres ample space here.\\n',\n",
       " 'Blueberry is rich in antioxidants.\\n',\n",
       " 'A horse happened to be nearby, so I offered it to him.\\n',\n",
       " 'Directed by Surender Reddy, this film featured Amitabh Bachchan, Tamannaah, Nayantara, Jagapathi Babu and Vijay Sethupathi.\\n',\n",
       " 'Police has also used tear gas shells to disperse the protesters.\\n',\n",
       " 'This higher-torque engine will be mated to a 6-speed manual transmission\\n',\n",
       " 'So if we apply that particular criteria to this function, what do we get?\\n',\n",
       " 'of Jet Airways employees\\n',\n",
       " 'That was just one of the rooms.\\n',\n",
       " 'There is no other go.\\n',\n",
       " 'The movie is being made on a huge budget of Rs 350 crore.\\n',\n",
       " 'Rs 1.40 lakh\\n',\n",
       " 'Now the first inhabitants that dwelt in their possessions in their cities were, the Israelites, the priests, Levites, and the Nethinims.\\n',\n",
       " 'The bill was cleared in Rajya Sabha by 125 votes in favour and 61 votes against it.\\n',\n",
       " 'leading from the front\\n',\n",
       " 'Likely, the full number of spiritual Israel, that little flock of Kingdom heirs, has been selected.\\n',\n",
       " '50 crore.\\n',\n",
       " '49,999 respectively.\\n',\n",
       " 'Bollywood actor Anushka Sharma\\n',\n",
       " 'Local health department\\n',\n",
       " '25 lakh respectively.\\n',\n",
       " 'No guts, no glory.\\n',\n",
       " 'This is fraud.\\n',\n",
       " 'The Congress hopes to gain substantially in the upcoming elections.\\n',\n",
       " 'These are rich in Vitamin B-complex which is a composite of eight vitamins B1, B2, B3, B5, B6, B7, B9, B12.\\n',\n",
       " 'Who knows which is which?\\n',\n",
       " 'It is already installed at two checkposts in the State of Sikkim - Rangpo Checkposts, East Sikkim, and Melli checkposts, South Sikkim.\\n',\n",
       " 'PM Modi scorches opposition in Lok Sabha\\n',\n",
       " \"ITS OFFICIAL... Vidya Balan in #NTR biopic... She enacts the part of NTRs wife Basavatarakam... NTR's son Balakrishna enacts the role of NTR...\\n\",\n",
       " 'That should be uniform.\\n',\n",
       " 'The shamans do not command them.\\n',\n",
       " 'The movie is jointly produced by Gopi Krishna Movies and UV Creations.\\n',\n",
       " 'Old Rs 500 notes.\\n',\n",
       " 'Budget slashed\\n',\n",
       " 'Whats coming up?\\n',\n",
       " 'The weather department has issued an advisory for fishermen not to venture into the sea due to bad weather.\\n',\n",
       " 'The details are as follows.\\n',\n",
       " '\"\"\", the SSP said.\"\\n',\n",
       " 'He obtained bachelors degree from the University of Mysore.\\n',\n",
       " 'India lost the match by three runs.\\n',\n",
       " 'President Ashraf Ghani said that the greatest vulnerability of Afghanistan is an open border with Iran.\\n',\n",
       " '50,000 crore for MSMEs through Fund of Funds (FoF).\\n',\n",
       " 'Spoof song\\n',\n",
       " 'Girls make friends.\\n',\n",
       " 'Videographing of marriages.\\n',\n",
       " 'Certificates were issued in remembrance of the forest personnel who laid their live while performing their duty in the year 2019-20\\n',\n",
       " 'In summer season, dehydration is common.\\n',\n",
       " 'Chiranjeevis son Ram Charan will produce the movie under the banner of Konidela Production Company.\\n',\n",
       " '1 cup pineapple juice\\n',\n",
       " 'The concept is good.\\n',\n",
       " 'Board of Directors Meeting:\\n',\n",
       " 'A chargesheet was filed in court.\\n',\n",
       " 'She played a double role in the film.\\n',\n",
       " 'At present, the most populous state of the country points to a tripartite contest between the SP, the BJP and the BSP.\\n',\n",
       " 'Come on !\\n',\n",
       " '\"\"\"Vive la France!\"\\n',\n",
       " 'Bigger text size\\n',\n",
       " 'Police reached at the spot and started investigation.\\n',\n",
       " 'KOLKATA: Kolkata Traffic Police has decided to make reflectors mandatory for cyclists in Kolkata.\\n',\n",
       " 'The state government has put in place all the arrangements for this purpose.\\n',\n",
       " 'He decided he had to do something.\\n',\n",
       " 'Pooja Hegde is the heroine of the film.\\n',\n",
       " 'Its Apple.\\n',\n",
       " 'But the Congress declined.\\n',\n",
       " 'The government has given the option for people who live close to the rivers to carry sand in bullock carts for free.\\n',\n",
       " 'He has Rs.\\n',\n",
       " 'Cinnamon 1 tablespoon\\n',\n",
       " 'The Holkar Stadium in Indore is all set to host Ranji Trophy finals 2017-18 season.\\n',\n",
       " 'Stipend: Rs.\\n',\n",
       " 'Police have detained some suspects and are interrogating them.\\n',\n",
       " 'A case has been registered against Prasad at Sitanagar police station under section 74/2019 and section 324 of Indian Penal Code (IPC).\\n',\n",
       " 'And there is something deeper and more sinister that is going on.\\n',\n",
       " \"Government's decision\\n\",\n",
       " 'Leading banks have cut their lending rates by 15-25 basis points.\\n',\n",
       " 'He awoke with tears in his eyes.\\n',\n",
       " 'Mechanical control\\n',\n",
       " 'How do you plan to repay the loan?\\n',\n",
       " 'of very low salary.\\n',\n",
       " 'Here it is.\\n',\n",
       " 'He slowly pushed the door.\\n',\n",
       " 'Vamana Avatar - Lord Vishnu in the form of a Dwarf\\n',\n",
       " 'Mobile phone app\\n',\n",
       " 'Because of my Bible - based beliefs, I clearly explained to these medical experts that no blood should be administered to me under any circumstances. Acts 15: 28, 29.\\n',\n",
       " 'BJP leaders?\\n',\n",
       " 'A look at the projects progress.\\n',\n",
       " 'Jharkhand Assembly Election Result 2019: Congress-JMM alliance takes early lead\\n',\n",
       " 'New theme name:\\n',\n",
       " 'The film also got dubbed in Telugu with the title Mr KK.\\n',\n",
       " 'This helps to make the Christian congregation attractive to others who wish to be free of conflict.\\n',\n",
       " 'This led to an extra-marital relationship between the two.\\n',\n",
       " 'Still, fear need not paralyze us when we are confronted with problems.\\n',\n",
       " 'Automated process\\n',\n",
       " 'The heart skipped a beat.\\n',\n",
       " 'Overseas Business\\n',\n",
       " 'Tamil music director Anirudh Ravichander is composing the music.\\n',\n",
       " 'Contestants Abhijeet, Harika, Sohel, Akhil, Ariyana, Avinash and Monal are in a race to compete in the grand finale.\\n',\n",
       " 'Presentation of details of 3rd Tranche by Union Finance & Corporate Affairs Minister Smt.Nirmala Sitharaman under Aatmanirbhar Bharat Abhiyaan to support Indian economy in fight against COVID-19\\n',\n",
       " 'There is still a lot of construction going on.\\n',\n",
       " 'Released he flees again with his fiance.\\n',\n",
       " '147 crore.\\n',\n",
       " 'Companies providing internships:\\n',\n",
       " '\"\"\"As Supreme Commander of the Armed Forces, I bow to the exemplary courage and supreme sacrifice of our soldiers to protect the sovereignty and integrity of the country,\"\" Kovind tweeted.\"\\n',\n",
       " 'After the retirement of Mahendra Singh Dhoni from Test cricket, Virat Kohli took over the responsibility of leading the Indian side.\\n',\n",
       " 'The pain will subside.\\n',\n",
       " 'People will be healthy.\\n',\n",
       " 'Heavy rainfall in Mumbai.\\n',\n",
       " 'Tata Motors is offering attractive discounts and special offers on select models in its lineup\\n',\n",
       " 'Kamal Haasans Hollywood project\\n',\n",
       " 'Fiat India has decided it will launch its new Punto Evo soon\\n',\n",
       " 'Moreover Rs.\\n',\n",
       " 'That must be recognised.\\n',\n",
       " 'Among his other books are:\\n',\n",
       " 'Facing hair fall problems?\\n',\n",
       " 'Im looking forward to this film.\\n',\n",
       " 'I know him since childhood.\\n',\n",
       " \"Country's largest lender State Bank of India (SBI) offers various services to its account holders.\\n\",\n",
       " 'BJP is the only secular political party.\\n',\n",
       " 'Who does it help?\\n',\n",
       " 'There is evidence of this.\\n',\n",
       " 'Which team doesnt deserve to be in the final?\\n',\n",
       " 'For every Rs.\\n',\n",
       " 'These must be rigorously analyzed.\\n',\n",
       " 'Railway TC beaten up\\n',\n",
       " 'Tata Motors has been targeting the mass market segment with its affordable products in the Indian market\\n',\n",
       " 'Im shaking.\\n',\n",
       " 'Checking of vehicles\\n',\n",
       " 'How does this make sense?\\n',\n",
       " 'Some people say I am supporting Modi.\\n',\n",
       " 'Can you just give me a chance?\\n',\n",
       " 'Tears flowed.\\n',\n",
       " 'India has received first tranche of details about financial accounts of its residents in Swiss banks.\\n',\n",
       " 'Why the...didnt I say something?\\n',\n",
       " 'Hyderabad: Telugu Desam Party (TDP) has stepped away from contesting in the Telangana Lok Sabha elections.\\n',\n",
       " 'The New Member..!\\n',\n",
       " '6 with Rs.\\n',\n",
       " 'Noklak sub-division contained the five admin circles of Noklak, Thonoknyu, Nokhu, Panso and Chingmei.\\n',\n",
       " 'Nirbhay gives Indian armed forces deep-strike capabilities and can be launched from land, ship and submarine\\n',\n",
       " 'I worked in a bank in Mumbai.\\n',\n",
       " \"Thief targets Jaitley's cremation. steals phones of Babul Supriyo, 10 others\\n\",\n",
       " '200 crore to the government.\\n',\n",
       " \"It's all on the opponent.\\n\",\n",
       " 'While Bihar chief minister Nitish Kumar skipped the meeting, senior Janata Dal (United), or JD(U), leader Sharad Yadav did attend.\\n',\n",
       " 'Arey baba re baba!\\n',\n",
       " 'A temple known as Mhatobar Shree Vinayaka Devaru, devoted to Lord Ganapati, is the prime attraction of this site\\n',\n",
       " 'Go to sleep!\\n',\n",
       " \"Former India captain Sourav Ganguly also attended former teammate Yuvraj's reception\\n\",\n",
       " 'The Union Cabinet, chaired by the Prime Minister, Shri Narendra Modi has given its approval to the proposal for equity infusion by Government of Rs 6000 crores in NIIF Debt Platform sponsored by National Investment and Infrastructure Fund (NIIF), comprising of Aseem Infrastructure Finance Limited (AIFL) and NIIF Infrastructure Finance Limited (NIIF-IFL), subject to the following conditions:\\n',\n",
       " 'The Prime Minister, Shri Narendra Modi, today chaired his fourteenth interaction through PRAGATI - the ICT-based, multi-modal platform for Pro-Active Governance and Timely Implementation.\\n',\n",
       " 'Your favourite movies...\\n',\n",
       " 'Faculty of Economics: Opened in 1986.\\n',\n",
       " 'In several districts, more than half the homes were destroyed or severely damaged.\\n',\n",
       " 'There was a difference.\\n',\n",
       " 'It is expected to cost around Rs 150 crore.\\n',\n",
       " 'Deputy Chief Minister K.Narayana Swamy\\n',\n",
       " 'Training together\\n',\n",
       " 'There is no change in this.\\n',\n",
       " 'Whos that girl?\\n',\n",
       " 'Lights are on.\\n',\n",
       " 'According to reports, the injured have been shifted to government hospital in Karimnagar.\\n',\n",
       " '( b) What has this experience taught you about Jehovah and his dealings with his servants? The knowledge that Gods servants acquire from their Bible study is not superficial.\\n',\n",
       " 'One must consult their doctors before taking such medications.\\n',\n",
       " 'Police rushed to the spot and removed them from the venue.\\n',\n",
       " 'Sweets were distributed afterwards.\\n',\n",
       " 'What does the Quran say?\\n',\n",
       " 'Study well.\\n',\n",
       " 'This dream could be made true.\\n',\n",
       " 'He has also captained Mumbai to the Ranji Trophy title.\\n',\n",
       " 'But they rarely do.\\n',\n",
       " 'A postmortem was conducted on the bodies and handed over to relatives.\\n',\n",
       " 'He was 49\\n',\n",
       " 'He said alternative designs are now available, and area of the houses has also been increased.\\n',\n",
       " 'Who are those leaders?\\n',\n",
       " 'Its really wonderful.\\n',\n",
       " 'Eat foods that boost immunity.\\n',\n",
       " 'Thank you for trusting me.\\n',\n",
       " 'TDP Cadre Attacks YSRCP Workers\\n',\n",
       " 'However, there was a lot of commotion during this time.\\n',\n",
       " 'In line with that, Jesus said: Stop storing up for yourselves treasures upon the earth, where moth and rust consume, and where thieves break in and steal.\\n',\n",
       " \"Clive's victory, and the award of the diwani of the rich region of Bengal, brought India into the public spotlight in Britain.\\n\",\n",
       " 'Pooja Hegde will be seen as the female lead and the film is being directed by Radha Krishna.\\n',\n",
       " 'The film was a hit in Tamil.\\n',\n",
       " 'So far Rs.\\n',\n",
       " 'Breast problems during pregnancy & lactation\\n',\n",
       " 'Royalty for farmers?\\n',\n",
       " 'Where should I bring up my children?\\n',\n",
       " 'Former Andhra minister Atchannaidu arrested in ESI scam\\n',\n",
       " 'Family sources said he died due to old age-related ailments.\\n',\n",
       " 'Lifestyles are changing.\\n',\n",
       " 'Main points\\n',\n",
       " '\"\"\"Why only TDP leaders are targeted by I-T officials?\"\\n',\n",
       " '12 THE BIBLES VIEWPOINT\\n',\n",
       " 'Put your hand on your heart and answer these:\\n',\n",
       " 'I am crying...\\n',\n",
       " 'The shooting has been completed and the post-production is on.\\n',\n",
       " 'Setback not a problem\\n',\n",
       " 'Prime Minister Narendra Modi and Chief Minister...\\n',\n",
       " 'He said that in Delhi, the SAD would contest the elections with its ally BJP.\\n',\n",
       " 'She was the first Indian athlete ever to win a medal in a World Championships.\\n',\n",
       " 'Do you know this?\\n',\n",
       " 'Claire (center)\\n',\n",
       " 'Despite being one of the richest men in the city, Pachaiyappa Mudaliar led a simple and austere lifestyle.\\n',\n",
       " 'The Modi government has committed to achieving this by 2025.\\n',\n",
       " 'With the financial aid of Rs.\\n',\n",
       " 'Hundreds of cases are being lodged.\\n',\n",
       " 'Police rushed to the spot immediately after receiving the information and the dead body was shifted to Osmania hospital for postmortem.\\n',\n",
       " 'He alleged the BJP in an anti-democratic way has been indulging in destabilising the democratically elected Governments in Karnataka and other States.\\n',\n",
       " 'She likes it a lot.\\n',\n",
       " 'He sank onto the bed.\\n',\n",
       " 'There is a temple in this town, which was constructed to honour Saint Vallabhacharya\\n',\n",
       " '\"\"\"You dont know?\"\\n',\n",
       " 'How are these estimates arrived at?\\n',\n",
       " 'The judgement in the case has been reserved by the court.\\n',\n",
       " 'Crisis in agriculture\\n',\n",
       " 'Police is investigating the case from many angles.\\n',\n",
       " 'This will result in financial losses.\\n',\n",
       " 'The BJP leaders are daydreaming.\\n',\n",
       " 'Of this, 22 billion had been re-hypothecated.\\n',\n",
       " 'Pawan Kalyan had...\\n',\n",
       " 'Drink water, before, during and after exercising.\\n',\n",
       " 'He always stayed away from politics.\\n',\n",
       " 'Time is less.\\n',\n",
       " 'Early initiation\\n',\n",
       " 'Used for standard notifications\\n',\n",
       " 'That is also good.\\n',\n",
       " 'A bench headed by Chief Justice S A Bobde took note of the plea filed by an NGO, Centre for Public Interest Litigation, alleging that the government has not done enough in pursuance of its own policy of converting public transport and government vehicles into EVs.\\n',\n",
       " 'Why is\\n',\n",
       " 'Rahul Sankirtyan is the director of the movie.\\n',\n",
       " 'Supreme Court has banned private companies from using Aadhaar.\\n',\n",
       " 'Rahul Gandhi has offered to resign as the president of the party, taking responsibility for its dismal performance in the Lok Sabha elections.\\n',\n",
       " 'And people have shown that if you put around 10 dollars or 10 rupees in providing a good quality water and good hygienic conditions, your expenditure in health sector can be reduced around 80 rupees can be reduced in the health sector.\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"C:/Users/nchar/Desktop/eng-to-tel_translation/train_en.txt\", 'r',encoding=\"utf-8\") as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(\"C:/Users/nchar/Desktop/eng-to-tel_translation/train_te.txt\", 'r',encoding=\"utf_8\") as file:\n",
    "    telugu_sentences = file.readlines()\n",
    "\n",
    "english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34441a2-746b-468f-9a85-c682ec96fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_telugu = {k:v for k,v in enumerate(telugu_vocabulary)}\n",
    "telugu_to_index = {v:k for k,v in enumerate(telugu_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86b0532-8126-4bcd-9ad3-3bf27ad5cc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telugu_to_index[PADDING_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af238ba-7172-42ab-a31f-14d078c6978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length telugu: 169.0\n",
      "97th percentile length English: 177.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length telugu: {np.percentile([len(x) for x in telugu_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f7208e-4c65-49ae-9529-5e55c10c093d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['మళ్లీ ఉదయిస్తాడు.',\n",
       " 'యెహోవా కృపను మనమెలా మహిమపరచవచ్చు?',\n",
       " 'ఆర్థికంగా కూడా భారత్\\u200c వే గంగా పయనిస్తున్నది.',\n",
       " '‘విద్యార్థులను చూస్తుంటే నాకు చిన్నప్పటి రోజులు గుర్తుకొస్తున్నాయి.',\n",
       " 'ఆర్థిక లావాదేవీలన్నీ ఆన్\\u200cలైన్\\u200cలోనే',\n",
       " 'పద్మినీ స్త్రీ రాత్రి వేళల్లో రతికి ఇష్టపడదు.',\n",
       " 'ఓకే కుటుంబానికి చెందిన ముగ్గురు మృతిచెందటంతో ఈ సంఘటన కలవరపర్చింది.',\n",
       " 'ఆ సినిమాకు వి. వి. వినాయక్\\u200c దర్శకత్వం వహించనున్నారు.',\n",
       " 'హైదరాబాద్\\u200cకు మరో భారీ వర్షం సూచనలున్నాయని వాతావరణ శాఖ హెచ్చరించింది.',\n",
       " 'మనం అంటున్న మమ్ముట్టి ఫ్యామిలీ !']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_SENTENCES = 10000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "telugu_sentences = telugu_sentences[:TOTAL_SENTENCES]\n",
    "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
    "telugu_sentences = [sentence.rstrip('\\n') for sentence in telugu_sentences]\n",
    "     \n",
    "\n",
    "telugu_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99034a1-81db-4016-bbdd-28cf664954d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific unwanted characters\n",
    "telugu_sentences = [sentence.replace('\\u200B', '').replace('\\u200C', '').replace('\\u200D', '') for sentence in telugu_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0291b7ef-1784-4fab-bd43-66afacf95034",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 200\n",
    "\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    l=set()\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            l.add(token)\n",
    "    return l\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "\n",
    "p=set()\n",
    "for index in range(len(telugu_sentences)):\n",
    "    telugu_sentence, english_sentence = telugu_sentences[index], english_sentences[index]\n",
    "    a=is_valid_tokens(telugu_sentence, telugu_vocabulary)\n",
    "    for i in a:\n",
    "        p.add(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba75070-d7b1-4bd3-bd2f-291fcb8bdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(len(telugu_sentences)):\n",
    "    if( is_valid_length(telugu_sentences[i],200)and is_valid_length(english_sentences[i],200)):\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858a4b26-54dd-4df8-9e6c-99490b2b8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_sentences = [telugu_sentences[i] for i in index]\n",
    "english_sentences = [english_sentences[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f410d83-6ee6-4066-a682-d8df4972a7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a74275-521b-447f-a823-bda97e6707b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e045070e-eb44-4e6c-81ca-880afcc5e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[]\n",
    "for sentence in telugu_sentences:\n",
    "    b=sentence\n",
    "    for a in p:\n",
    "        if a in b:\n",
    "            b=b.replace(a,\"\")\n",
    "    s.append(b)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccbd6d2-1286-4c38-b241-b3a3584d509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['మళ్లీ ఉదయిస్తాడు.',\n",
       " 'యెహోవా కృపను మనమెలా మహిమపరచవచ్చు?',\n",
       " 'ఆర్థికంగా కూడా భారత్ వే గంగా పయనిస్తున్నది.',\n",
       " '‘విద్యార్థులను చూస్తుంటే నాకు చిన్నప్పటి రోజులు గుర్తుకొస్తున్నాయి.',\n",
       " 'ఆర్థిక లావాదేవీలన్నీ ఆన్లైన్లోనే',\n",
       " 'పద్మినీ స్త్రీ రాత్రి వేళల్లో రతికి ఇష్టపడదు.',\n",
       " 'ఓకే కుటుంబానికి చెందిన ముగ్గురు మృతిచెందటంతో ఈ సంఘటన కలవరపర్చింది.',\n",
       " 'ఆ సినిమాకు వి. వి. వినాయక్ దర్శకత్వం వహించనున్నారు.',\n",
       " 'హైదరాబాద్కు మరో భారీ వర్షం సూచనలున్నాయని వాతావరణ శాఖ హెచ్చరించింది.',\n",
       " 'మనం అంటున్న మమ్ముట్టి ఫ్యామిలీ !']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telugu_cleaned_sentences=s\n",
    "telugu_cleaned_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fcf8db5-6f1f-4769-9f2c-49e273dde54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(telugu_cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986fd9db-c175-4156-a4b7-a1b8d025f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDataset:\n",
    "\n",
    "    def __init__(self, english_sentences, telugu_cleaned_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.telugu_sentences = telugu_cleaned_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.telugu_sentences[idx]\n",
    "\n",
    "    def to_tf_dataset(self, batch_size=32, shuffle=True):\n",
    "        # Create a TensorFlow dataset from the sentence pairs\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.english_sentences, self.telugu_sentences))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self.english_sentences))\n",
    "\n",
    "        # Batch the dataset\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        # Prefetch to improve performance\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f85e3843-125c-48a7-9ea6-64ec648c4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences,telugu_cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd6358b3-9b26-4065-bdbc-a80197af14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "tf_dataset = dataset.to_tf_dataset(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d000dc43-9895-4564-9e84-489609852a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3d34dec-9095-4229-8223-846514d1f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_dataset=tf_dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "774bcc9b-9920-44c2-b814-41aa54919aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'this is a must do.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an iterator\n",
    "iterator = iter(ex_dataset.as_numpy_iterator())\n",
    "\n",
    "# Fetch the next batch\n",
    "batch = next(iterator)\n",
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7fde4f1-6194-4d78-bba2-f88998f24bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=128\n",
    "batch_size=32\n",
    "num_heads=8\n",
    "english_max_length=telugu_max_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73300465-5da8-4f6c-be82-6ffa38aa840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, tel_batch,max_sequence_length):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = np.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = np.triu(look_ahead_mask,1)\n",
    "    encoder_padding_mask = np.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = np.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = np.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        #eng_sentence_length, tel_sentence_length = tf.shape(eng_batch[idx])[0], tf.shape(tel_batch[idx])[0]\n",
    "        eng_sentence_length, tel_sentence_length = len(eng_batch[idx]),len(tel_batch[idx])\n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "        tel_chars_to_padding_mask = np.arange(tel_sentence_length + 1, max_sequence_length)\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, tel_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, tel_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, tel_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = tf.convert_to_tensor(np.where(encoder_padding_mask, NEG_INFTY, 0))\n",
    "    decoder_self_attention_mask =  tf.convert_to_tensor(np.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0))\n",
    "    decoder_cross_attention_mask = tf.convert_to_tensor(np.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0))\n",
    "    encoder_self_attention_mask=tf.expand_dims(encoder_self_attention_mask,axis=1)\n",
    "    decoder_self_attention_mask=tf.expand_dims(decoder_self_attention_mask,axis=1)\n",
    "    decoder_cross_attention_mask=tf.expand_dims(decoder_cross_attention_mask,axis=1)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e55d0c5-3856-41c9-b878-ed5739d9a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks1(eng_batch, tel_batch, max_sequence_length=200):\n",
    "    # Get the batch size (number of sentences)\n",
    "    num_sentences = tf.shape(eng_batch)[0]\n",
    "\n",
    "    # Look-ahead mask for self-attention in the decoder\n",
    "    look_ahead_mask = tf.linalg.band_part(tf.ones((max_sequence_length, max_sequence_length)), -1, 0)  # Upper triangular matrix\n",
    "    look_ahead_mask = tf.cast(look_ahead_mask, dtype=tf.float32)\n",
    "\n",
    "    # Padding masks (initialize with False)\n",
    "    encoder_padding_mask = tf.fill([num_sentences, max_sequence_length], False)\n",
    "    decoder_padding_mask_self_attention = tf.fill([num_sentences, max_sequence_length], False)\n",
    "    decoder_padding_mask_cross_attention = tf.fill([num_sentences, max_sequence_length], False)\n",
    "\n",
    "    # Expand dimensions to match required shape\n",
    "    encoder_padding_mask = tf.expand_dims(encoder_padding_mask, axis=1)  # Shape: [num_sentences, 1, max_sequence_length]\n",
    "    decoder_padding_mask_self_attention = tf.expand_dims(decoder_padding_mask_self_attention, axis=1)\n",
    "    decoder_padding_mask_cross_attention = tf.expand_dims(decoder_padding_mask_cross_attention, axis=1)\n",
    "\n",
    "    # Broadcast to the required shapes\n",
    "    encoder_self_attention_mask = tf.broadcast_to(encoder_padding_mask, [num_sentences, max_sequence_length, max_sequence_length])\n",
    "    decoder_self_attention_mask = tf.broadcast_to(decoder_padding_mask_self_attention, [num_sentences, max_sequence_length, max_sequence_length])\n",
    "    decoder_cross_attention_mask = tf.broadcast_to(decoder_padding_mask_cross_attention, [num_sentences, max_sequence_length, max_sequence_length])\n",
    "\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af8cbeee-9289-4a4c-873e-887da7d48fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng,tel=batch\n",
    "encoder_self_attention_mask,decoder_self_attention_mask,decoder_cross_attention_mask=create_masks(eng,tel,telugu_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45bc6bfe-78a9-4daa-bf8c-edcadfa64beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in range(len(eng)):\n",
    "    p=len(eng[idx])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46210099-71ad-4f54-9195-34d63eadd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng,tel=batch\n",
    "encoder_self_attention_mask1,decoder_self_attention_mask1,decoder_cross_attention_mask1=create_masks1(eng,tel,telugu_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daa7a070-ed6e-4edf-8672-dcf8059a5a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7f21b3b-3eef-4f8b-8659-62a8164552a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 1, 200, 200]),\n",
       " TensorShape([32, 1, 200, 200]),\n",
       " TensorShape([32, 1, 200, 200]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask.shape,decoder_self_attention_mask.shape,decoder_cross_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b20af6c1-1157-4522-8265-fef2836faf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 200, 200]),\n",
       " TensorShape([32, 200, 200]),\n",
       " TensorShape([32, 200, 200]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask1.shape,decoder_self_attention_mask1.shape,decoder_cross_attention_mask1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41d4afad-cf06-4b57-960b-df49cbce34d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=bool, numpy=\n",
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5427d4e-6c8e-45a3-afe6-6fadc074d541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=float64, numpy=\n",
       "array([ 0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,  0.e+00,\n",
       "        0.e+00,  0.e+00,  0.e+00,  0.e+00, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09, -1.e+09,\n",
       "       -1.e+09, -1.e+09, -1.e+09, -1.e+09])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df1ff36-18ae-4a51-8e06-d95aa8719779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model:int,num_heads:int) -> None :\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.wq=Dense(d_model)\n",
    "        self.wk=Dense(d_model)\n",
    "        self.wv=Dense(d_model)\n",
    "        self.dense=Dense(d_model)\n",
    "        self.d_head=d_model//num_heads\n",
    "\n",
    "    \n",
    "    def scaled_dot_product_attention(self,q,k,v,mask):\n",
    "        a=tf.matmul(q,k,transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        attention_weights=a/tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.float32)\n",
    "            attention_weights +=mask\n",
    "        attention_scores=tf.nn.softmax(attention_weights)\n",
    "        outputs=tf.matmul(attention_scores,v)\n",
    "        return outputs,attention_scores\n",
    "\n",
    "    \n",
    "    def split_heads(self, x, batch_size,seq_len):\n",
    "        if tf.shape(x)[-1] % self.num_heads != 0:\n",
    "            raise ValueError(\"The last dimension of the input tensor must be divisible by num_heads.\")\n",
    "    \n",
    "        x = tf.reshape(x, (batch_size,seq_len, self.num_heads, self.d_head))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    \n",
    "    def call(self,x,mask):\n",
    "        batch_size=tf.shape(x)[0]\n",
    "        seq_len=tf.shape(x)[1]\n",
    "        \n",
    "        q=self.wq(x)\n",
    "        k=self.wk(x)\n",
    "        v=self.wv(x)\n",
    "        q=self.split_heads(q,batch_size,seq_len)\n",
    "        k=self.split_heads(k,batch_size,seq_len)\n",
    "        v=self.split_heads(v,batch_size,seq_len)\n",
    "        outputs,attention_weights=self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        attention = tf.transpose(outputs, perm=[0, 2, 1, 3])\n",
    "        attention = tf.reshape(attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(attention)\n",
    "        return output\n",
    "\n",
    "# Instantiate the MultiHeadAttention layer\n",
    "\n",
    "# Example number of attention heads\n",
    "#mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Call the layer\n",
    "#output = mha_layer(english_embedded_tokens,mask=english_padding_mask)\n",
    "#output\n",
    "\n",
    "# Now `output` will be the result of the multi-head attention operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b540e440-b8a0-4200-93e4-fffb88b99c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,dff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.dff=dff\n",
    "        self.layer1=tf.keras.layers.Dense(dff,activation=\"relu\")\n",
    "        self.layer2=tf.keras.layers.Dense(d_model)\n",
    "    def call(self,x):\n",
    "        out1=self.layer1(x)\n",
    "        out2=self.layer2(out1)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc841b1c-fbf1-43b4-8943-39cfd930d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.d_model=d_model\n",
    "        self.wq=tf.keras.layers.Dense(d_model)\n",
    "        self.wk=tf.keras.layers.Dense(d_model)\n",
    "        self.wv=tf.keras.layers.Dense(d_model)\n",
    "        self.dense=tf.keras.layers.Dense(d_model)\n",
    "        self.d_head=d_model//num_heads\n",
    "\n",
    "\n",
    "    def split_heads(self,y,batch_size,seq_len):\n",
    "        if tf.shape(y)[-1] % self.num_heads != 0:\n",
    "            raise ValueError(\"The last dimension of the input tensor must be divisible by num_heads.\")\n",
    "        y=tf.reshape(y,(batch_size,seq_len,self.num_heads,self.d_head))\n",
    "        y=tf.transpose(y,perm=[0,2,1,3])\n",
    "        return y\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask):\n",
    "        qk=tf.matmul(q,k,transpose_b=True)\n",
    "        dk=float(tf.shape(k)[-1])\n",
    "        attention_weights=qk/tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            attention_weights +=mask\n",
    "        attention_score=tf.nn.softmax(attention_weights)\n",
    "        output=tf.matmul(attention_score,v)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def call(self,x,y,mask):\n",
    "        q=self.wq(y)\n",
    "        k=self.wk(x)\n",
    "        v=self.wv(x)\n",
    "        batch_size=tf.shape(y)[0]\n",
    "        seq_len=tf.shape(y)[1]\n",
    "        q=self.split_heads(q,batch_size,seq_len)\n",
    "        k=self.split_heads(k,batch_size,seq_len)\n",
    "        v=self.split_heads(v,batch_size,seq_len)\n",
    "        attention=self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        output=tf.transpose(attention,perm=[0,2,1,3])\n",
    "        final_output=tf.reshape(output,(batch_size,-1,self.d_model))\n",
    "        return final_output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64cbbfda-124a-4763-b799-1a42ba4cd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceEmbedding(tf.keras.layers.Layer):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.d_model=d_model\n",
    "        #self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    def decoding(self,batch):\n",
    "       # batch=np.array(batch)\n",
    "        decoded_list=[]\n",
    "        #b=batch.numpy()\n",
    "        #batch=tf.make_ndarray(batch)\n",
    "        for i in (batch):\n",
    "            p=i.decode(\"utf-8\")\n",
    "            #p=p.tolist()\n",
    "            #p=np.array(p)\n",
    "            decoded_list.append(p)\n",
    "        return decoded_list\n",
    "  \n",
    "    def position_encoder(self):\n",
    "        # Create the positional encodings with shape (max_sequence_length, d_model)\n",
    "        pos = np.arange(self.max_sequence_length)[:, np.newaxis]\n",
    "        i = np.arange(self.d_model)[np.newaxis, :]\n",
    "        \n",
    "        # Positional encoding formula used in Transformer models\n",
    "        angle_rads = pos / np.power(10000, (2 * (i // 2)) / np.float32(self.d_model))\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Apply sin to even indices\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Apply cos to odd indices\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]  # Add batch dimension\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "        \n",
    "        OOV_TOKEN = self.language_to_index.get(\"<unk>\", self.vocab_size - 1)\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            #z=sentence[np.newaxis,...]\n",
    "            #sentence=z[0]\n",
    "            #if isinstance(sentence, str):\n",
    "                #sentence = list(sentence)\n",
    "            #sentence=sentence[np.newaxis,...]\n",
    "            #sentence=sentence[0]\n",
    "            sentence_word_indicies = [self.language_to_index.get(token, OOV_TOKEN) for token in list(sentence[:self.max_sequence_length])]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return tf.convert_to_tensor(sentence_word_indicies[:self.max_sequence_length])\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence in batch:\n",
    "           tokenized.append( tokenize(sentence, start_token, end_token) )\n",
    "        tokenized = tf.stack(tokenized)\n",
    "        return tokenized\n",
    "    \n",
    "    def call(self, x, start_token, end_token): # sentence\n",
    "        \"\"\"if isinstance(x, list):\n",
    "        # Convert list elements to NumPy array only if their shapes are consistent\n",
    "            x = [tensor.numpy() if hasattr(tensor, 'numpy') else tensor for tensor in x]\n",
    "\n",
    "    # If all tensors are consistent, you can convert them to a single array\n",
    "        try:\n",
    "            x = np.array(x)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting x to a NumPy array: {e}\")\n",
    "        # Handle error, possibly by checking or reshaping elements\"\"\"\n",
    "        print(\"shape of x=\",x.shape)\n",
    "        #print(type(x[0]))\n",
    "    # Continue with your decoding logic\n",
    "        x = self.decoding(batch=np.array(x))\n",
    "        #x=self.decoding(batch=np.array(x))\n",
    "        \n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder()\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c1a1ef2-7dda-4961-b581-e8615b09a6c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m k\u001b[38;5;241m=\u001b[39md\n\u001b[0;32m      3\u001b[0m aaa\u001b[38;5;241m=\u001b[39m[e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m k]\n\u001b[0;32m      4\u001b[0m z\u001b[38;5;241m=\u001b[39maaa[\u001b[38;5;241m0\u001b[39m][np\u001b[38;5;241m.\u001b[39mnewaxis,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "k=d\n",
    "\n",
    "aaa=[e for e in k]\n",
    "z=aaa[0][np.newaxis,...]\n",
    "z[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07953a3f-77d7-4d51-928b-01e223546557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(batch):\n",
    "    decoded_list=[]\n",
    "        #b=batch.numpy()\n",
    "        #batch=tf.make_ndarray(batch)\n",
    "    for i in (batch):\n",
    "        p=i.decode(\"utf-8\")\n",
    "            #p=p.tolist()\n",
    "        p=np.array(p)\n",
    "        decoded_list.append(p)\n",
    "    return decoded_list\n",
    "bat=decoding(batch[1])\n",
    "decoder = build_decoder(num_layers=12, d_model=d_model, num_heads=8, dff=512,max_sequence_length=200,language_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "output = decoder([encoder_out, batch[1], decoder_self_attention_mask, decoder_cross_attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c5c25-defa-46ae-89da-b5f2b80dbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "478788b4-4e68-4f7e-befe-7a618ba6d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,dff):\n",
    "        super().__init__()\n",
    "        self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.norm_and_add1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_and_add2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.feed_forward=Feedforward(dff=dff,d_model=d_model)\n",
    "    def call(self,x,mask):\n",
    "        attention_out=self.attention(x=x,mask=mask)\n",
    "        norm_and_add1_out=self.norm_and_add1(x+attention_out)\n",
    "        feed_forward_out=self.feed_forward(norm_and_add1_out)\n",
    "        norm_and_add2_out=self.norm_and_add2(norm_and_add1_out+feed_forward_out)\n",
    "        return norm_and_add2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8b7b64f-c168-4d22-9e27-219f70d1654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26072595-b55e-43fe-a579-b53f663bea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model,num_heads,dff,num_encoders,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding=SentenceEmbedding(max_sequence_length=max_sequence_length, d_model=d_model, language_to_index=language_to_index, START_TOKEN=START_TOKEN, END_TOKEN=END_TOKEN, PADDING_TOKEN=PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder([Encoder_layer(d_model=d_model,num_heads=num_heads,dff=dff) for _ in range(num_encoders)])\n",
    "\n",
    "    def call(self, x,mask,start_token,end_token):\n",
    "        x=self.sentence_embedding(x=x,start_token=start_token,end_token=end_token)\n",
    "        x = self.layers(x=x,mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fde188e3-bff6-4b58-80ec-4cfeb63e96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=Encoder(d_model=d_model,num_heads=num_heads,dff=512,num_encoders=12,max_sequence_length=english_max_length,language_to_index=english_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a07b84-4541-4498-8da9-2f7901f28911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the award in different categories were also given to the winners.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an iterator\n",
    "iterator = iter(ex_dataset.as_numpy_iterator())\n",
    "\n",
    "# Fetch the next batch\n",
    "batch = next(iterator)\n",
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ed00745-a40e-4bb3-987c-12546ef632a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n"
     ]
    }
   ],
   "source": [
    "encoder_out=[]\n",
    "for inputs, targets in ex_dataset.as_numpy_iterator():\n",
    "    out=s(inputs,mask=encoder_self_attention_mask,start_token=True,end_token=True)\n",
    "    encoder_out.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17996bc5-7e5e-4dc2-b833-b32615cbb817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.5564932 , -0.5669649 ,  0.7027901 , ..., -0.53199303,\n",
       "          -1.3212402 , -0.776253  ],\n",
       "         [-0.5504785 , -0.5705082 ,  0.70116484, ..., -0.53427404,\n",
       "          -1.3195268 , -0.7772775 ],\n",
       "         [-0.5492785 , -0.5775336 ,  0.70136094, ..., -0.53291005,\n",
       "          -1.3175031 , -0.7803993 ],\n",
       "         ...,\n",
       "         [-0.5963789 , -0.5895573 ,  0.7945759 , ..., -0.74501467,\n",
       "          -0.8226432 , -0.9577145 ],\n",
       "         [-0.597221  , -0.5872512 ,  0.8022083 , ..., -0.74653566,\n",
       "          -0.8225584 , -0.9655758 ],\n",
       "         [-0.593818  , -0.58298   ,  0.8040438 , ..., -0.74689496,\n",
       "          -0.81762546, -0.9732748 ]],\n",
       " \n",
       "        [[-1.0716151 , -0.2758856 ,  0.7731406 , ..., -0.2441194 ,\n",
       "          -0.50318444, -0.5990235 ],\n",
       "         [-1.0653529 , -0.28005248,  0.7748195 , ..., -0.2476363 ,\n",
       "          -0.4986616 , -0.60288435],\n",
       "         [-1.0666635 , -0.28257647,  0.77536714, ..., -0.24508333,\n",
       "          -0.4984641 , -0.60864574],\n",
       "         ...,\n",
       "         [-0.95376545, -0.418268  ,  0.74574107, ..., -0.29118237,\n",
       "          -0.48366913, -0.66796774],\n",
       "         [-0.953626  , -0.41489792,  0.7536282 , ..., -0.29210618,\n",
       "          -0.4839762 , -0.6762593 ],\n",
       "         [-0.95157635, -0.4086093 ,  0.75293446, ..., -0.2897909 ,\n",
       "          -0.47764733, -0.6873007 ]],\n",
       " \n",
       "        [[-1.1036644 , -0.29608777,  0.3539137 , ..., -0.22033647,\n",
       "          -0.9865732 , -0.61869943],\n",
       "         [-1.0980034 , -0.30049735,  0.35581362, ..., -0.21943542,\n",
       "          -0.9875603 , -0.6290449 ],\n",
       "         [-1.098436  , -0.30892813,  0.36000466, ..., -0.21410285,\n",
       "          -0.98741204, -0.6310431 ],\n",
       "         ...,\n",
       "         [-0.9463632 , -0.44642133,  0.65228087, ..., -0.4215969 ,\n",
       "          -0.65154004, -0.73296416],\n",
       "         [-0.946982  , -0.4444359 ,  0.660287  , ..., -0.42369854,\n",
       "          -0.6534946 , -0.7414886 ],\n",
       "         [-0.9444964 , -0.4419547 ,  0.6603018 , ..., -0.42320052,\n",
       "          -0.6461147 , -0.7519346 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.0963136 , -0.2831757 ,  0.35535613, ..., -0.22245017,\n",
       "          -1.0090137 , -0.6038814 ],\n",
       "         [-1.0876743 , -0.29044205,  0.35734963, ..., -0.2208549 ,\n",
       "          -1.0095688 , -0.6147335 ],\n",
       "         [-1.0912772 , -0.30087072,  0.3634855 , ..., -0.21891679,\n",
       "          -1.0094115 , -0.6194747 ],\n",
       "         ...,\n",
       "         [-0.926625  , -0.4515016 ,  0.6608705 , ..., -0.4291912 ,\n",
       "          -0.68744224, -0.7273532 ],\n",
       "         [-0.92657   , -0.4487603 ,  0.6688588 , ..., -0.43109015,\n",
       "          -0.6893635 , -0.73599356],\n",
       "         [-0.92449576, -0.44604528,  0.669178  , ..., -0.43052572,\n",
       "          -0.6832953 , -0.74667406]],\n",
       " \n",
       "        [[-0.6454124 , -0.55266553,  0.4279716 , ..., -0.58495647,\n",
       "          -1.3117238 , -0.81108385],\n",
       "         [-0.63658184, -0.5600198 ,  0.42756557, ..., -0.5867606 ,\n",
       "          -1.3126708 , -0.8165589 ],\n",
       "         [-0.6299888 , -0.5714087 ,  0.422724  , ..., -0.5840041 ,\n",
       "          -1.3129405 , -0.813739  ],\n",
       "         ...,\n",
       "         [-0.6381163 , -0.58815753,  0.7101538 , ..., -0.6911451 ,\n",
       "          -0.87402123, -0.9569501 ],\n",
       "         [-0.6389358 , -0.58500224,  0.71641564, ..., -0.6921726 ,\n",
       "          -0.87369347, -0.9649032 ],\n",
       "         [-0.63708645, -0.5797171 ,  0.71769357, ..., -0.6921948 ,\n",
       "          -0.8667831 , -0.9746592 ]],\n",
       " \n",
       "        [[-1.1142294 , -0.16002925,  0.7755542 , ..., -0.3508806 ,\n",
       "          -0.27412137, -0.6295667 ],\n",
       "         [-1.1117277 , -0.16580972,  0.77586204, ..., -0.3538963 ,\n",
       "          -0.27194566, -0.6377784 ],\n",
       "         [-1.1156627 , -0.17017971,  0.77920425, ..., -0.3509979 ,\n",
       "          -0.26812497, -0.64133006],\n",
       "         ...,\n",
       "         [-0.9439502 , -0.392971  ,  0.72745097, ..., -0.44148955,\n",
       "          -0.32154745, -0.7001404 ],\n",
       "         [-0.945312  , -0.38865873,  0.7332847 , ..., -0.44255903,\n",
       "          -0.3203026 , -0.70856196],\n",
       "         [-0.9431359 , -0.38002738,  0.7309662 , ..., -0.44067767,\n",
       "          -0.31355208, -0.71790135]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.5866157 , -0.5525143 ,  0.70000744, ..., -0.49493462,\n",
       "          -1.3337783 , -0.7703441 ],\n",
       "         [-0.58179665, -0.5568265 ,  0.6987454 , ..., -0.49723658,\n",
       "          -1.331469  , -0.772033  ],\n",
       "         [-0.57948536, -0.56452274,  0.69822353, ..., -0.49449956,\n",
       "          -1.3299285 , -0.7755725 ],\n",
       "         ...,\n",
       "         [-0.62534297, -0.57572025,  0.78345937, ..., -0.7082344 ,\n",
       "          -0.81456435, -0.94572085],\n",
       "         [-0.6262148 , -0.5740037 ,  0.79101306, ..., -0.7097448 ,\n",
       "          -0.8149357 , -0.9541949 ],\n",
       "         [-0.6229073 , -0.5701721 ,  0.79223526, ..., -0.71076685,\n",
       "          -0.8087758 , -0.962453  ]],\n",
       " \n",
       "        [[-1.0744787 , -0.27898353,  0.77826726, ..., -0.26323825,\n",
       "          -0.51106733, -0.6095979 ],\n",
       "         [-1.0681347 , -0.28388745,  0.77888167, ..., -0.26668283,\n",
       "          -0.50530535, -0.6156523 ],\n",
       "         [-1.066542  , -0.28799742,  0.77952117, ..., -0.26415208,\n",
       "          -0.5053626 , -0.6203433 ],\n",
       "         ...,\n",
       "         [-0.9565688 , -0.42344972,  0.74836737, ..., -0.30987775,\n",
       "          -0.48972553, -0.6779521 ],\n",
       "         [-0.95612174, -0.4202316 ,  0.7563185 , ..., -0.31078815,\n",
       "          -0.48975015, -0.6860772 ],\n",
       "         [-0.95369875, -0.4140351 ,  0.75551313, ..., -0.3086175 ,\n",
       "          -0.483304  , -0.6965898 ]],\n",
       " \n",
       "        [[-1.0820639 , -0.29101023,  0.39918813, ..., -0.23768768,\n",
       "          -1.0465809 , -0.6168238 ],\n",
       "         [-1.0753988 , -0.2973768 ,  0.40242705, ..., -0.2370702 ,\n",
       "          -1.0470564 , -0.62836313],\n",
       "         [-1.0772306 , -0.30899942,  0.40505797, ..., -0.23441617,\n",
       "          -1.0481813 , -0.62885   ],\n",
       "         ...,\n",
       "         [-0.92022794, -0.44941002,  0.684904  , ..., -0.44193476,\n",
       "          -0.70990115, -0.73695415],\n",
       "         [-0.92046714, -0.44672006,  0.6923772 , ..., -0.4436846 ,\n",
       "          -0.7120436 , -0.7452704 ],\n",
       "         [-0.9176997 , -0.44449446,  0.6924351 , ..., -0.4432382 ,\n",
       "          -0.70576406, -0.7558112 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.1151699 , -0.27326325,  0.344872  , ..., -0.18431245,\n",
       "          -0.9981333 , -0.5878479 ],\n",
       "         [-1.1085954 , -0.2804821 ,  0.34827134, ..., -0.18191433,\n",
       "          -0.9992156 , -0.59706914],\n",
       "         [-1.1101906 , -0.28786424,  0.35241288, ..., -0.17925419,\n",
       "          -1.0010451 , -0.6001679 ],\n",
       "         ...,\n",
       "         [-0.9385589 , -0.44062847,  0.6574066 , ..., -0.4073303 ,\n",
       "          -0.6778204 , -0.7141561 ],\n",
       "         [-0.93889254, -0.43809134,  0.66541564, ..., -0.40946472,\n",
       "          -0.67937356, -0.72202045],\n",
       "         [-0.93675834, -0.43555105,  0.66596955, ..., -0.40904197,\n",
       "          -0.67261803, -0.73233604]],\n",
       " \n",
       "        [[-0.66200435, -0.5438706 ,  0.41360998, ..., -0.5734119 ,\n",
       "          -1.2951593 , -0.8138643 ],\n",
       "         [-0.6504916 , -0.5483213 ,  0.41255218, ..., -0.57450116,\n",
       "          -1.297404  , -0.8185586 ],\n",
       "         [-0.648916  , -0.5608711 ,  0.40843153, ..., -0.5728477 ,\n",
       "          -1.2980266 , -0.818958  ],\n",
       "         ...,\n",
       "         [-0.6424503 , -0.58519363,  0.7060744 , ..., -0.6918927 ,\n",
       "          -0.8618091 , -0.95516694],\n",
       "         [-0.64402163, -0.5830415 ,  0.71310973, ..., -0.6926694 ,\n",
       "          -0.86141396, -0.9628229 ],\n",
       "         [-0.6427464 , -0.5778872 ,  0.7142331 , ..., -0.6931855 ,\n",
       "          -0.854031  , -0.9713798 ]],\n",
       " \n",
       "        [[-1.11488   , -0.1606243 ,  0.76729363, ..., -0.3525351 ,\n",
       "          -0.27754867, -0.628352  ],\n",
       "         [-1.1124992 , -0.1657942 ,  0.7672269 , ..., -0.3527527 ,\n",
       "          -0.27550083, -0.64121103],\n",
       "         [-1.116444  , -0.1725991 ,  0.77099574, ..., -0.35226142,\n",
       "          -0.26988044, -0.6406872 ],\n",
       "         ...,\n",
       "         [-0.94410723, -0.3953225 ,  0.71756005, ..., -0.4460932 ,\n",
       "          -0.3247249 , -0.6992749 ],\n",
       "         [-0.9456006 , -0.390841  ,  0.7233339 , ..., -0.44671166,\n",
       "          -0.32394528, -0.7076318 ],\n",
       "         [-0.94342035, -0.38204324,  0.7211661 , ..., -0.44460556,\n",
       "          -0.3171408 , -0.71704674]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.5729613 , -0.5550019 ,  0.7042439 , ..., -0.50745445,\n",
       "          -1.340574  , -0.77086926],\n",
       "         [-0.5667232 , -0.5601543 ,  0.70482457, ..., -0.5096249 ,\n",
       "          -1.3380775 , -0.77475154],\n",
       "         [-0.56656057, -0.56546825,  0.7015346 , ..., -0.5068848 ,\n",
       "          -1.3372638 , -0.77578485],\n",
       "         ...,\n",
       "         [-0.60477597, -0.5858737 ,  0.7949227 , ..., -0.72856784,\n",
       "          -0.82987696, -0.9535187 ],\n",
       "         [-0.60559756, -0.583377  ,  0.8027031 , ..., -0.7300835 ,\n",
       "          -0.8297913 , -0.96148014],\n",
       "         [-0.6022334 , -0.5784957 ,  0.80438435, ..., -0.7305188 ,\n",
       "          -0.82397777, -0.96943325]],\n",
       " \n",
       "        [[-1.0738926 , -0.28261757,  0.77548003, ..., -0.26844493,\n",
       "          -0.5269834 , -0.6142976 ],\n",
       "         [-1.0662366 , -0.2872608 ,  0.77681434, ..., -0.2709448 ,\n",
       "          -0.5209994 , -0.6217147 ],\n",
       "         [-1.0666413 , -0.2905241 ,  0.7761999 , ..., -0.26778007,\n",
       "          -0.5216739 , -0.624984  ],\n",
       "         ...,\n",
       "         [-0.95717025, -0.4270342 ,  0.7449168 , ..., -0.31357545,\n",
       "          -0.50645345, -0.68220705],\n",
       "         [-0.95628566, -0.42390278,  0.7524953 , ..., -0.31446248,\n",
       "          -0.50622714, -0.6907257 ],\n",
       "         [-0.9534703 , -0.4178499 ,  0.7512862 , ..., -0.31231567,\n",
       "          -0.49943596, -0.7013802 ]],\n",
       " \n",
       "        [[-1.0846065 , -0.29519922,  0.39243606, ..., -0.23633611,\n",
       "          -1.0399027 , -0.6204077 ],\n",
       "         [-1.0776411 , -0.3030645 ,  0.39485547, ..., -0.23693427,\n",
       "          -1.040965  , -0.6311804 ],\n",
       "         [-1.0783868 , -0.31231168,  0.3990058 , ..., -0.23357207,\n",
       "          -1.041502  , -0.63420767],\n",
       "         ...,\n",
       "         [-0.92163795, -0.45179525,  0.680697  , ..., -0.44249436,\n",
       "          -0.7053234 , -0.73901266],\n",
       "         [-0.9218228 , -0.44918296,  0.6884368 , ..., -0.44419703,\n",
       "          -0.70716774, -0.7472601 ],\n",
       "         [-0.9189216 , -0.44737202,  0.6886496 , ..., -0.4439164 ,\n",
       "          -0.7008352 , -0.757876  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.1086454 , -0.27162546,  0.34467083, ..., -0.19708861,\n",
       "          -1.0057219 , -0.5925371 ],\n",
       "         [-1.0998476 , -0.27846527,  0.3461043 , ..., -0.19442016,\n",
       "          -1.0068274 , -0.60296524],\n",
       "         [-1.1040218 , -0.28340414,  0.35132122, ..., -0.19089402,\n",
       "          -1.0066004 , -0.6058751 ],\n",
       "         ...,\n",
       "         [-0.93698055, -0.4381225 ,  0.65499085, ..., -0.41326603,\n",
       "          -0.68250793, -0.7167329 ],\n",
       "         [-0.9372965 , -0.43539536,  0.6627072 , ..., -0.41542545,\n",
       "          -0.68410826, -0.7247571 ],\n",
       "         [-0.9351757 , -0.43284246,  0.66333956, ..., -0.41494665,\n",
       "          -0.67733675, -0.73522043]],\n",
       " \n",
       "        [[-0.652085  , -0.5539138 ,  0.39845446, ..., -0.57609683,\n",
       "          -1.3091455 , -0.8160727 ],\n",
       "         [-0.64180464, -0.55835205,  0.39686382, ..., -0.57705855,\n",
       "          -1.3118187 , -0.8218053 ],\n",
       "         [-0.6370058 , -0.5728572 ,  0.39419925, ..., -0.5765649 ,\n",
       "          -1.311376  , -0.82021046],\n",
       "         ...,\n",
       "         [-0.6379863 , -0.5876891 ,  0.7020473 , ..., -0.69053996,\n",
       "          -0.8690919 , -0.95529956],\n",
       "         [-0.6394373 , -0.5853572 ,  0.7091211 , ..., -0.69147116,\n",
       "          -0.8689954 , -0.9633396 ],\n",
       "         [-0.63779736, -0.580183  ,  0.71042585, ..., -0.69189185,\n",
       "          -0.861884  , -0.97254914]],\n",
       " \n",
       "        [[-1.0886846 , -0.1846832 ,  0.7908901 , ..., -0.3901965 ,\n",
       "          -0.31872588, -0.6570971 ],\n",
       "         [-1.0864179 , -0.18873072,  0.79249406, ..., -0.39156643,\n",
       "          -0.31610084, -0.6675334 ],\n",
       "         [-1.0881407 , -0.19720088,  0.7952812 , ..., -0.38922486,\n",
       "          -0.3134062 , -0.67080307],\n",
       "         ...,\n",
       "         [-0.9236978 , -0.41613284,  0.7360109 , ..., -0.47380295,\n",
       "          -0.36279324, -0.72523767],\n",
       "         [-0.92527163, -0.41112754,  0.74216795, ..., -0.47532642,\n",
       "          -0.36145592, -0.73378474],\n",
       "         [-0.92235386, -0.40307236,  0.7408001 , ..., -0.47411853,\n",
       "          -0.3552125 , -0.74272907]]], dtype=float32)>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5dab8-2067-4ae6-9d7d-ffb24ccdb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.char.decode(batch[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40c5c8ad-de31-44a6-980b-f82d18543dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,dff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.dff=dff\n",
    "        self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.norm_and_add1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_and_add2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.encoder_decoder_attention=CrossMultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.feed_forward=Feedforward(dff=dff,d_model=d_model)\n",
    "        self.norm_and_add3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    def call(self,x,y,self_attention_mask,cross_attention_mask): #x==>encoder's final query vector(output)\n",
    "        attention_out=self.attention(y,mask=self_attention_mask)\n",
    "        norm_and_add1_out=self.norm_and_add1(attention_out+y)\n",
    "        encoder_decoder_attention_out=self.encoder_decoder_attention(x,y,mask=cross_attention_mask)\n",
    "        norm_and_add2_out=self.norm_and_add2(norm_and_add1_out+encoder_decoder_attention_out)\n",
    "        feed_forward_out=self.feed_forward(norm_and_add2_out)\n",
    "        norm_and_add3_out=self.norm_and_add3(norm_and_add2_out+feed_forward_out)\n",
    "        return norm_and_add3_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1309a855-7a89-425c-a8bb-cdd99198a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, x,y,self_attention_mask,cross_attention_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,y,self_attention_mask,cross_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b17fada-056a-4d0b-af23-fbe795f3c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
    "        super(DecoderWrapper, self).__init__()\n",
    "        self.sentence_embedding=SentenceEmbedding(max_sequence_length=max_sequence_length, d_model=d_model, language_to_index=language_to_index, START_TOKEN=START_TOKEN, END_TOKEN=END_TOKEN, PADDING_TOKEN=PADDING_TOKEN)\n",
    "        self.decoder_layer = Decoder_layer(d_model, num_heads, dff)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        y=inputs[1]\n",
    "        self_attention_mask=inputs[2]\n",
    "        cross_attention_mask=inputs[3]\n",
    "        \n",
    "        \n",
    "        y=self.sentence_embedding(x=y,start_token=True,end_token=True)\n",
    "        return self.decoder_layer(x=x, y=y, self_attention_mask=self_attention_mask, cross_attention_mask=cross_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7839fe30-038d-4f04-b852-90febde71aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(num_layers, d_model, num_heads, dff,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    \n",
    "    # Add 12 decoder layers\n",
    "    for _ in range(num_layers):\n",
    "        decoder.add(DecoderWrapper(d_model, num_heads, dff,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN))\n",
    "    \n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb13d169-43c9-4df3-89ab-76b7c8e1f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nchar\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1383: UserWarning: Layer 'decoder_layer' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling MultiHeadAttention.call().\n",
      "\n",
      "\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n",
      "\n",
      "Arguments received by MultiHeadAttention.call():\n",
      "  • x=tf.Tensor(shape=(32, 200, 128), dtype=float32)\n",
      "  • mask=tf.Tensor(shape=(32, 1, 200, 200), dtype=float32)''\n",
      "  warnings.warn(\n",
      "C:\\Users\\nchar\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'decoder_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nchar\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1383: UserWarning: Layer 'decoder_wrapper' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling MultiHeadAttention.call().\n",
      "\n",
      "\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n",
      "\n",
      "Arguments received by MultiHeadAttention.call():\n",
      "  • x=tf.Tensor(shape=(32, 200, 128), dtype=float32)\n",
      "  • mask=tf.Tensor(shape=(32, 1, 200, 200), dtype=float32)''\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nchar\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1383: UserWarning: Layer 'cross_multi_head_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Layer \"dense_77\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
      "array([[[-0.5564932 , -0.5669649 ,  0.7027901 , ..., -0.53199303,\n",
      "         -1.3212402 , -0.776253  ],\n",
      "        [-0.5504785 , -0.5705082 ,  0.70116484, ..., -0.53427404,\n",
      "         -1.3195268 , -0.7772775 ],\n",
      "        [-0.5492785 , -0.5775336 ,  0.70136094, ..., -0.53291005,\n",
      "         -1.3175031 , -0.7803993 ],\n",
      "        ...,\n",
      "        [-0.5963789 , -0.5895573 ,  0.7945759 , ..., -0.74501467,\n",
      "         -0.8226432 , -0.9577145 ],\n",
      "        [-0.597221  , -0.5872512 ,  0.8022083 , ..., -0.74653566,\n",
      "         -0.8225584 , -0.9655758 ],\n",
      "        [-0.593818  , -0.58298   ,  0.8040438 , ..., -0.74689496,\n",
      "         -0.81762546, -0.9732748 ]],\n",
      "\n",
      "       [[-1.0716151 , -0.2758856 ,  0.7731406 , ..., -0.2441194 ,\n",
      "         -0.50318444, -0.5990235 ],\n",
      "        [-1.0653529 , -0.28005248,  0.7748195 , ..., -0.2476363 ,\n",
      "         -0.4986616 , -0.60288435],\n",
      "        [-1.0666635 , -0.28257647,  0.77536714, ..., -0.24508333,\n",
      "         -0.4984641 , -0.60864574],\n",
      "        ...,\n",
      "        [-0.95376545, -0.418268  ,  0.74574107, ..., -0.29118237,\n",
      "         -0.48366913, -0.66796774],\n",
      "        [-0.953626  , -0.41489792,  0.7536282 , ..., -0.29210618,\n",
      "         -0.4839762 , -0.6762593 ],\n",
      "        [-0.95157635, -0.4086093 ,  0.75293446, ..., -0.2897909 ,\n",
      "         -0.47764733, -0.6873007 ]],\n",
      "\n",
      "       [[-1.1036644 , -0.29608777,  0.3539137 , ..., -0.22033647,\n",
      "         -0.9865732 , -0.61869943],\n",
      "        [-1.0980034 , -0.30049735,  0.35581362, ..., -0.21943542,\n",
      "         -0.9875603 , -0.6290449 ],\n",
      "        [-1.098436  , -0.30892813,  0.36000466, ..., -0.21410285,\n",
      "         -0.98741204, -0.6310431 ],\n",
      "        ...,\n",
      "        [-0.9463632 , -0.44642133,  0.65228087, ..., -0.4215969 ,\n",
      "         -0.65154004, -0.73296416],\n",
      "        [-0.946982  , -0.4444359 ,  0.660287  , ..., -0.42369854,\n",
      "         -0.6534946 , -0.7414886 ],\n",
      "        [-0.9444964 , -0.4419547 ,  0.6603018 , ..., -0.42320052,\n",
      "         -0.6461147 , -0.7519346 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.0963136 , -0.2831757 ,  0.35535613, ..., -0.22245017,\n",
      "         -1.0090137 , -0.6038814 ],\n",
      "        [-1.0876743 , -0.29044205,  0.35734963, ..., -0.2208549 ,\n",
      "         -1.0095688 , -0.6147335 ],\n",
      "        [-1.0912772 , -0.30087072,  0.3634855 , ..., -0.21891679,\n",
      "         -1.0094115 , -0.6194747 ],\n",
      "        ...,\n",
      "        [-0.926625  , -0.4515016 ,  0.6608705 , ..., -0.4291912 ,\n",
      "         -0.68744224, -0.7273532 ],\n",
      "        [-0.92657   , -0.4487603 ,  0.6688588 , ..., -0.43109015,\n",
      "         -0.6893635 , -0.73599356],\n",
      "        [-0.92449576, -0.44604528,  0.669178  , ..., -0.43052572,\n",
      "         -0.6832953 , -0.74667406]],\n",
      "\n",
      "       [[-0.6454124 , -0.55266553,  0.4279716 , ..., -0.58495647,\n",
      "         -1.3117238 , -0.81108385],\n",
      "        [-0.63658184, -0.5600198 ,  0.42756557, ..., -0.5867606 ,\n",
      "         -1.3126708 , -0.8165589 ],\n",
      "        [-0.6299888 , -0.5714087 ,  0.422724  , ..., -0.5840041 ,\n",
      "         -1.3129405 , -0.813739  ],\n",
      "        ...,\n",
      "        [-0.6381163 , -0.58815753,  0.7101538 , ..., -0.6911451 ,\n",
      "         -0.87402123, -0.9569501 ],\n",
      "        [-0.6389358 , -0.58500224,  0.71641564, ..., -0.6921726 ,\n",
      "         -0.87369347, -0.9649032 ],\n",
      "        [-0.63708645, -0.5797171 ,  0.71769357, ..., -0.6921948 ,\n",
      "         -0.8667831 , -0.9746592 ]],\n",
      "\n",
      "       [[-1.1142294 , -0.16002925,  0.7755542 , ..., -0.3508806 ,\n",
      "         -0.27412137, -0.6295667 ],\n",
      "        [-1.1117277 , -0.16580972,  0.77586204, ..., -0.3538963 ,\n",
      "         -0.27194566, -0.6377784 ],\n",
      "        [-1.1156627 , -0.17017971,  0.77920425, ..., -0.3509979 ,\n",
      "         -0.26812497, -0.64133006],\n",
      "        ...,\n",
      "        [-0.9439502 , -0.392971  ,  0.72745097, ..., -0.44148955,\n",
      "         -0.32154745, -0.7001404 ],\n",
      "        [-0.945312  , -0.38865873,  0.7332847 , ..., -0.44255903,\n",
      "         -0.3203026 , -0.70856196],\n",
      "        [-0.9431359 , -0.38002738,  0.7309662 , ..., -0.44067767,\n",
      "         -0.31355208, -0.71790135]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
      "array([[[-0.5866157 , -0.5525143 ,  0.70000744, ..., -0.49493462,\n",
      "         -1.3337783 , -0.7703441 ],\n",
      "        [-0.58179665, -0.5568265 ,  0.6987454 , ..., -0.49723658,\n",
      "         -1.331469  , -0.772033  ],\n",
      "        [-0.57948536, -0.56452274,  0.69822353, ..., -0.49449956,\n",
      "         -1.3299285 , -0.7755725 ],\n",
      "        ...,\n",
      "        [-0.62534297, -0.57572025,  0.78345937, ..., -0.7082344 ,\n",
      "         -0.81456435, -0.94572085],\n",
      "        [-0.6262148 , -0.5740037 ,  0.79101306, ..., -0.7097448 ,\n",
      "         -0.8149357 , -0.9541949 ],\n",
      "        [-0.6229073 , -0.5701721 ,  0.79223526, ..., -0.71076685,\n",
      "         -0.8087758 , -0.962453  ]],\n",
      "\n",
      "       [[-1.0744787 , -0.27898353,  0.77826726, ..., -0.26323825,\n",
      "         -0.51106733, -0.6095979 ],\n",
      "        [-1.0681347 , -0.28388745,  0.77888167, ..., -0.26668283,\n",
      "         -0.50530535, -0.6156523 ],\n",
      "        [-1.066542  , -0.28799742,  0.77952117, ..., -0.26415208,\n",
      "         -0.5053626 , -0.6203433 ],\n",
      "        ...,\n",
      "        [-0.9565688 , -0.42344972,  0.74836737, ..., -0.30987775,\n",
      "         -0.48972553, -0.6779521 ],\n",
      "        [-0.95612174, -0.4202316 ,  0.7563185 , ..., -0.31078815,\n",
      "         -0.48975015, -0.6860772 ],\n",
      "        [-0.95369875, -0.4140351 ,  0.75551313, ..., -0.3086175 ,\n",
      "         -0.483304  , -0.6965898 ]],\n",
      "\n",
      "       [[-1.0820639 , -0.29101023,  0.39918813, ..., -0.23768768,\n",
      "         -1.0465809 , -0.6168238 ],\n",
      "        [-1.0753988 , -0.2973768 ,  0.40242705, ..., -0.2370702 ,\n",
      "         -1.0470564 , -0.62836313],\n",
      "        [-1.0772306 , -0.30899942,  0.40505797, ..., -0.23441617,\n",
      "         -1.0481813 , -0.62885   ],\n",
      "        ...,\n",
      "        [-0.92022794, -0.44941002,  0.684904  , ..., -0.44193476,\n",
      "         -0.70990115, -0.73695415],\n",
      "        [-0.92046714, -0.44672006,  0.6923772 , ..., -0.4436846 ,\n",
      "         -0.7120436 , -0.7452704 ],\n",
      "        [-0.9176997 , -0.44449446,  0.6924351 , ..., -0.4432382 ,\n",
      "         -0.70576406, -0.7558112 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.1151699 , -0.27326325,  0.344872  , ..., -0.18431245,\n",
      "         -0.9981333 , -0.5878479 ],\n",
      "        [-1.1085954 , -0.2804821 ,  0.34827134, ..., -0.18191433,\n",
      "         -0.9992156 , -0.59706914],\n",
      "        [-1.1101906 , -0.28786424,  0.35241288, ..., -0.17925419,\n",
      "         -1.0010451 , -0.6001679 ],\n",
      "        ...,\n",
      "        [-0.9385589 , -0.44062847,  0.6574066 , ..., -0.4073303 ,\n",
      "         -0.6778204 , -0.7141561 ],\n",
      "        [-0.93889254, -0.43809134,  0.66541564, ..., -0.40946472,\n",
      "         -0.67937356, -0.72202045],\n",
      "        [-0.93675834, -0.43555105,  0.66596955, ..., -0.40904197,\n",
      "         -0.67261803, -0.73233604]],\n",
      "\n",
      "       [[-0.66200435, -0.5438706 ,  0.41360998, ..., -0.5734119 ,\n",
      "         -1.2951593 , -0.8138643 ],\n",
      "        [-0.6504916 , -0.5483213 ,  0.41255218, ..., -0.57450116,\n",
      "         -1.297404  , -0.8185586 ],\n",
      "        [-0.648916  , -0.5608711 ,  0.40843153, ..., -0.5728477 ,\n",
      "         -1.2980266 , -0.818958  ],\n",
      "        ...,\n",
      "        [-0.6424503 , -0.58519363,  0.7060744 , ..., -0.6918927 ,\n",
      "         -0.8618091 , -0.95516694],\n",
      "        [-0.64402163, -0.5830415 ,  0.71310973, ..., -0.6926694 ,\n",
      "         -0.86141396, -0.9628229 ],\n",
      "        [-0.6427464 , -0.5778872 ,  0.7142331 , ..., -0.6931855 ,\n",
      "         -0.854031  , -0.9713798 ]],\n",
      "\n",
      "       [[-1.11488   , -0.1606243 ,  0.76729363, ..., -0.3525351 ,\n",
      "         -0.27754867, -0.628352  ],\n",
      "        [-1.1124992 , -0.1657942 ,  0.7672269 , ..., -0.3527527 ,\n",
      "         -0.27550083, -0.64121103],\n",
      "        [-1.116444  , -0.1725991 ,  0.77099574, ..., -0.35226142,\n",
      "         -0.26988044, -0.6406872 ],\n",
      "        ...,\n",
      "        [-0.94410723, -0.3953225 ,  0.71756005, ..., -0.4460932 ,\n",
      "         -0.3247249 , -0.6992749 ],\n",
      "        [-0.9456006 , -0.390841  ,  0.7233339 , ..., -0.44671166,\n",
      "         -0.32394528, -0.7076318 ],\n",
      "        [-0.94342035, -0.38204324,  0.7211661 , ..., -0.44460556,\n",
      "         -0.3171408 , -0.71704674]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
      "array([[[-0.5729613 , -0.5550019 ,  0.7042439 , ..., -0.50745445,\n",
      "         -1.340574  , -0.77086926],\n",
      "        [-0.5667232 , -0.5601543 ,  0.70482457, ..., -0.5096249 ,\n",
      "         -1.3380775 , -0.77475154],\n",
      "        [-0.56656057, -0.56546825,  0.7015346 , ..., -0.5068848 ,\n",
      "         -1.3372638 , -0.77578485],\n",
      "        ...,\n",
      "        [-0.60477597, -0.5858737 ,  0.7949227 , ..., -0.72856784,\n",
      "         -0.82987696, -0.9535187 ],\n",
      "        [-0.60559756, -0.583377  ,  0.8027031 , ..., -0.7300835 ,\n",
      "         -0.8297913 , -0.96148014],\n",
      "        [-0.6022334 , -0.5784957 ,  0.80438435, ..., -0.7305188 ,\n",
      "         -0.82397777, -0.96943325]],\n",
      "\n",
      "       [[-1.0738926 , -0.28261757,  0.77548003, ..., -0.26844493,\n",
      "         -0.5269834 , -0.6142976 ],\n",
      "        [-1.0662366 , -0.2872608 ,  0.77681434, ..., -0.2709448 ,\n",
      "         -0.5209994 , -0.6217147 ],\n",
      "        [-1.0666413 , -0.2905241 ,  0.7761999 , ..., -0.26778007,\n",
      "         -0.5216739 , -0.624984  ],\n",
      "        ...,\n",
      "        [-0.95717025, -0.4270342 ,  0.7449168 , ..., -0.31357545,\n",
      "         -0.50645345, -0.68220705],\n",
      "        [-0.95628566, -0.42390278,  0.7524953 , ..., -0.31446248,\n",
      "         -0.50622714, -0.6907257 ],\n",
      "        [-0.9534703 , -0.4178499 ,  0.7512862 , ..., -0.31231567,\n",
      "         -0.49943596, -0.7013802 ]],\n",
      "\n",
      "       [[-1.0846065 , -0.29519922,  0.39243606, ..., -0.23633611,\n",
      "         -1.0399027 , -0.6204077 ],\n",
      "        [-1.0776411 , -0.3030645 ,  0.39485547, ..., -0.23693427,\n",
      "         -1.040965  , -0.6311804 ],\n",
      "        [-1.0783868 , -0.31231168,  0.3990058 , ..., -0.23357207,\n",
      "         -1.041502  , -0.63420767],\n",
      "        ...,\n",
      "        [-0.92163795, -0.45179525,  0.680697  , ..., -0.44249436,\n",
      "         -0.7053234 , -0.73901266],\n",
      "        [-0.9218228 , -0.44918296,  0.6884368 , ..., -0.44419703,\n",
      "         -0.70716774, -0.7472601 ],\n",
      "        [-0.9189216 , -0.44737202,  0.6886496 , ..., -0.4439164 ,\n",
      "         -0.7008352 , -0.757876  ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.1086454 , -0.27162546,  0.34467083, ..., -0.19708861,\n",
      "         -1.0057219 , -0.5925371 ],\n",
      "        [-1.0998476 , -0.27846527,  0.3461043 , ..., -0.19442016,\n",
      "         -1.0068274 , -0.60296524],\n",
      "        [-1.1040218 , -0.28340414,  0.35132122, ..., -0.19089402,\n",
      "         -1.0066004 , -0.6058751 ],\n",
      "        ...,\n",
      "        [-0.93698055, -0.4381225 ,  0.65499085, ..., -0.41326603,\n",
      "         -0.68250793, -0.7167329 ],\n",
      "        [-0.9372965 , -0.43539536,  0.6627072 , ..., -0.41542545,\n",
      "         -0.68410826, -0.7247571 ],\n",
      "        [-0.9351757 , -0.43284246,  0.66333956, ..., -0.41494665,\n",
      "         -0.67733675, -0.73522043]],\n",
      "\n",
      "       [[-0.652085  , -0.5539138 ,  0.39845446, ..., -0.57609683,\n",
      "         -1.3091455 , -0.8160727 ],\n",
      "        [-0.64180464, -0.55835205,  0.39686382, ..., -0.57705855,\n",
      "         -1.3118187 , -0.8218053 ],\n",
      "        [-0.6370058 , -0.5728572 ,  0.39419925, ..., -0.5765649 ,\n",
      "         -1.311376  , -0.82021046],\n",
      "        ...,\n",
      "        [-0.6379863 , -0.5876891 ,  0.7020473 , ..., -0.69053996,\n",
      "         -0.8690919 , -0.95529956],\n",
      "        [-0.6394373 , -0.5853572 ,  0.7091211 , ..., -0.69147116,\n",
      "         -0.8689954 , -0.9633396 ],\n",
      "        [-0.63779736, -0.580183  ,  0.71042585, ..., -0.69189185,\n",
      "         -0.861884  , -0.97254914]],\n",
      "\n",
      "       [[-1.0886846 , -0.1846832 ,  0.7908901 , ..., -0.3901965 ,\n",
      "         -0.31872588, -0.6570971 ],\n",
      "        [-1.0864179 , -0.18873072,  0.79249406, ..., -0.39156643,\n",
      "         -0.31610084, -0.6675334 ],\n",
      "        [-1.0881407 , -0.19720088,  0.7952812 , ..., -0.38922486,\n",
      "         -0.3134062 , -0.67080307],\n",
      "        ...,\n",
      "        [-0.9236978 , -0.41613284,  0.7360109 , ..., -0.47380295,\n",
      "         -0.36279324, -0.72523767],\n",
      "        [-0.92527163, -0.41112754,  0.74216795, ..., -0.47532642,\n",
      "         -0.36145592, -0.73378474],\n",
      "        [-0.92235386, -0.40307236,  0.7408001 , ..., -0.47411853,\n",
      "         -0.3552125 , -0.74272907]]], dtype=float32)>]''\n",
      "  warnings.warn(\n",
      "C:\\Users\\nchar\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'cross_multi_head_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling CrossMultiHeadAttention.call().\n\n\u001b[1mLayer \"dense_77\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5564932 , -0.5669649 ,  0.7027901 , ..., -0.53199303,\n         -1.3212402 , -0.776253  ],\n        [-0.5504785 , -0.5705082 ,  0.70116484, ..., -0.53427404,\n         -1.3195268 , -0.7772775 ],\n        [-0.5492785 , -0.5775336 ,  0.70136094, ..., -0.53291005,\n         -1.3175031 , -0.7803993 ],\n        ...,\n        [-0.5963789 , -0.5895573 ,  0.7945759 , ..., -0.74501467,\n         -0.8226432 , -0.9577145 ],\n        [-0.597221  , -0.5872512 ,  0.8022083 , ..., -0.74653566,\n         -0.8225584 , -0.9655758 ],\n        [-0.593818  , -0.58298   ,  0.8040438 , ..., -0.74689496,\n         -0.81762546, -0.9732748 ]],\n\n       [[-1.0716151 , -0.2758856 ,  0.7731406 , ..., -0.2441194 ,\n         -0.50318444, -0.5990235 ],\n        [-1.0653529 , -0.28005248,  0.7748195 , ..., -0.2476363 ,\n         -0.4986616 , -0.60288435],\n        [-1.0666635 , -0.28257647,  0.77536714, ..., -0.24508333,\n         -0.4984641 , -0.60864574],\n        ...,\n        [-0.95376545, -0.418268  ,  0.74574107, ..., -0.29118237,\n         -0.48366913, -0.66796774],\n        [-0.953626  , -0.41489792,  0.7536282 , ..., -0.29210618,\n         -0.4839762 , -0.6762593 ],\n        [-0.95157635, -0.4086093 ,  0.75293446, ..., -0.2897909 ,\n         -0.47764733, -0.6873007 ]],\n\n       [[-1.1036644 , -0.29608777,  0.3539137 , ..., -0.22033647,\n         -0.9865732 , -0.61869943],\n        [-1.0980034 , -0.30049735,  0.35581362, ..., -0.21943542,\n         -0.9875603 , -0.6290449 ],\n        [-1.098436  , -0.30892813,  0.36000466, ..., -0.21410285,\n         -0.98741204, -0.6310431 ],\n        ...,\n        [-0.9463632 , -0.44642133,  0.65228087, ..., -0.4215969 ,\n         -0.65154004, -0.73296416],\n        [-0.946982  , -0.4444359 ,  0.660287  , ..., -0.42369854,\n         -0.6534946 , -0.7414886 ],\n        [-0.9444964 , -0.4419547 ,  0.6603018 , ..., -0.42320052,\n         -0.6461147 , -0.7519346 ]],\n\n       ...,\n\n       [[-1.0963136 , -0.2831757 ,  0.35535613, ..., -0.22245017,\n         -1.0090137 , -0.6038814 ],\n        [-1.0876743 , -0.29044205,  0.35734963, ..., -0.2208549 ,\n         -1.0095688 , -0.6147335 ],\n        [-1.0912772 , -0.30087072,  0.3634855 , ..., -0.21891679,\n         -1.0094115 , -0.6194747 ],\n        ...,\n        [-0.926625  , -0.4515016 ,  0.6608705 , ..., -0.4291912 ,\n         -0.68744224, -0.7273532 ],\n        [-0.92657   , -0.4487603 ,  0.6688588 , ..., -0.43109015,\n         -0.6893635 , -0.73599356],\n        [-0.92449576, -0.44604528,  0.669178  , ..., -0.43052572,\n         -0.6832953 , -0.74667406]],\n\n       [[-0.6454124 , -0.55266553,  0.4279716 , ..., -0.58495647,\n         -1.3117238 , -0.81108385],\n        [-0.63658184, -0.5600198 ,  0.42756557, ..., -0.5867606 ,\n         -1.3126708 , -0.8165589 ],\n        [-0.6299888 , -0.5714087 ,  0.422724  , ..., -0.5840041 ,\n         -1.3129405 , -0.813739  ],\n        ...,\n        [-0.6381163 , -0.58815753,  0.7101538 , ..., -0.6911451 ,\n         -0.87402123, -0.9569501 ],\n        [-0.6389358 , -0.58500224,  0.71641564, ..., -0.6921726 ,\n         -0.87369347, -0.9649032 ],\n        [-0.63708645, -0.5797171 ,  0.71769357, ..., -0.6921948 ,\n         -0.8667831 , -0.9746592 ]],\n\n       [[-1.1142294 , -0.16002925,  0.7755542 , ..., -0.3508806 ,\n         -0.27412137, -0.6295667 ],\n        [-1.1117277 , -0.16580972,  0.77586204, ..., -0.3538963 ,\n         -0.27194566, -0.6377784 ],\n        [-1.1156627 , -0.17017971,  0.77920425, ..., -0.3509979 ,\n         -0.26812497, -0.64133006],\n        ...,\n        [-0.9439502 , -0.392971  ,  0.72745097, ..., -0.44148955,\n         -0.32154745, -0.7001404 ],\n        [-0.945312  , -0.38865873,  0.7332847 , ..., -0.44255903,\n         -0.3203026 , -0.70856196],\n        [-0.9431359 , -0.38002738,  0.7309662 , ..., -0.44067767,\n         -0.31355208, -0.71790135]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5866157 , -0.5525143 ,  0.70000744, ..., -0.49493462,\n         -1.3337783 , -0.7703441 ],\n        [-0.58179665, -0.5568265 ,  0.6987454 , ..., -0.49723658,\n         -1.331469  , -0.772033  ],\n        [-0.57948536, -0.56452274,  0.69822353, ..., -0.49449956,\n         -1.3299285 , -0.7755725 ],\n        ...,\n        [-0.62534297, -0.57572025,  0.78345937, ..., -0.7082344 ,\n         -0.81456435, -0.94572085],\n        [-0.6262148 , -0.5740037 ,  0.79101306, ..., -0.7097448 ,\n         -0.8149357 , -0.9541949 ],\n        [-0.6229073 , -0.5701721 ,  0.79223526, ..., -0.71076685,\n         -0.8087758 , -0.962453  ]],\n\n       [[-1.0744787 , -0.27898353,  0.77826726, ..., -0.26323825,\n         -0.51106733, -0.6095979 ],\n        [-1.0681347 , -0.28388745,  0.77888167, ..., -0.26668283,\n         -0.50530535, -0.6156523 ],\n        [-1.066542  , -0.28799742,  0.77952117, ..., -0.26415208,\n         -0.5053626 , -0.6203433 ],\n        ...,\n        [-0.9565688 , -0.42344972,  0.74836737, ..., -0.30987775,\n         -0.48972553, -0.6779521 ],\n        [-0.95612174, -0.4202316 ,  0.7563185 , ..., -0.31078815,\n         -0.48975015, -0.6860772 ],\n        [-0.95369875, -0.4140351 ,  0.75551313, ..., -0.3086175 ,\n         -0.483304  , -0.6965898 ]],\n\n       [[-1.0820639 , -0.29101023,  0.39918813, ..., -0.23768768,\n         -1.0465809 , -0.6168238 ],\n        [-1.0753988 , -0.2973768 ,  0.40242705, ..., -0.2370702 ,\n         -1.0470564 , -0.62836313],\n        [-1.0772306 , -0.30899942,  0.40505797, ..., -0.23441617,\n         -1.0481813 , -0.62885   ],\n        ...,\n        [-0.92022794, -0.44941002,  0.684904  , ..., -0.44193476,\n         -0.70990115, -0.73695415],\n        [-0.92046714, -0.44672006,  0.6923772 , ..., -0.4436846 ,\n         -0.7120436 , -0.7452704 ],\n        [-0.9176997 , -0.44449446,  0.6924351 , ..., -0.4432382 ,\n         -0.70576406, -0.7558112 ]],\n\n       ...,\n\n       [[-1.1151699 , -0.27326325,  0.344872  , ..., -0.18431245,\n         -0.9981333 , -0.5878479 ],\n        [-1.1085954 , -0.2804821 ,  0.34827134, ..., -0.18191433,\n         -0.9992156 , -0.59706914],\n        [-1.1101906 , -0.28786424,  0.35241288, ..., -0.17925419,\n         -1.0010451 , -0.6001679 ],\n        ...,\n        [-0.9385589 , -0.44062847,  0.6574066 , ..., -0.4073303 ,\n         -0.6778204 , -0.7141561 ],\n        [-0.93889254, -0.43809134,  0.66541564, ..., -0.40946472,\n         -0.67937356, -0.72202045],\n        [-0.93675834, -0.43555105,  0.66596955, ..., -0.40904197,\n         -0.67261803, -0.73233604]],\n\n       [[-0.66200435, -0.5438706 ,  0.41360998, ..., -0.5734119 ,\n         -1.2951593 , -0.8138643 ],\n        [-0.6504916 , -0.5483213 ,  0.41255218, ..., -0.57450116,\n         -1.297404  , -0.8185586 ],\n        [-0.648916  , -0.5608711 ,  0.40843153, ..., -0.5728477 ,\n         -1.2980266 , -0.818958  ],\n        ...,\n        [-0.6424503 , -0.58519363,  0.7060744 , ..., -0.6918927 ,\n         -0.8618091 , -0.95516694],\n        [-0.64402163, -0.5830415 ,  0.71310973, ..., -0.6926694 ,\n         -0.86141396, -0.9628229 ],\n        [-0.6427464 , -0.5778872 ,  0.7142331 , ..., -0.6931855 ,\n         -0.854031  , -0.9713798 ]],\n\n       [[-1.11488   , -0.1606243 ,  0.76729363, ..., -0.3525351 ,\n         -0.27754867, -0.628352  ],\n        [-1.1124992 , -0.1657942 ,  0.7672269 , ..., -0.3527527 ,\n         -0.27550083, -0.64121103],\n        [-1.116444  , -0.1725991 ,  0.77099574, ..., -0.35226142,\n         -0.26988044, -0.6406872 ],\n        ...,\n        [-0.94410723, -0.3953225 ,  0.71756005, ..., -0.4460932 ,\n         -0.3247249 , -0.6992749 ],\n        [-0.9456006 , -0.390841  ,  0.7233339 , ..., -0.44671166,\n         -0.32394528, -0.7076318 ],\n        [-0.94342035, -0.38204324,  0.7211661 , ..., -0.44460556,\n         -0.3171408 , -0.71704674]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5729613 , -0.5550019 ,  0.7042439 , ..., -0.50745445,\n         -1.340574  , -0.77086926],\n        [-0.5667232 , -0.5601543 ,  0.70482457, ..., -0.5096249 ,\n         -1.3380775 , -0.77475154],\n        [-0.56656057, -0.56546825,  0.7015346 , ..., -0.5068848 ,\n         -1.3372638 , -0.77578485],\n        ...,\n        [-0.60477597, -0.5858737 ,  0.7949227 , ..., -0.72856784,\n         -0.82987696, -0.9535187 ],\n        [-0.60559756, -0.583377  ,  0.8027031 , ..., -0.7300835 ,\n         -0.8297913 , -0.96148014],\n        [-0.6022334 , -0.5784957 ,  0.80438435, ..., -0.7305188 ,\n         -0.82397777, -0.96943325]],\n\n       [[-1.0738926 , -0.28261757,  0.77548003, ..., -0.26844493,\n         -0.5269834 , -0.6142976 ],\n        [-1.0662366 , -0.2872608 ,  0.77681434, ..., -0.2709448 ,\n         -0.5209994 , -0.6217147 ],\n        [-1.0666413 , -0.2905241 ,  0.7761999 , ..., -0.26778007,\n         -0.5216739 , -0.624984  ],\n        ...,\n        [-0.95717025, -0.4270342 ,  0.7449168 , ..., -0.31357545,\n         -0.50645345, -0.68220705],\n        [-0.95628566, -0.42390278,  0.7524953 , ..., -0.31446248,\n         -0.50622714, -0.6907257 ],\n        [-0.9534703 , -0.4178499 ,  0.7512862 , ..., -0.31231567,\n         -0.49943596, -0.7013802 ]],\n\n       [[-1.0846065 , -0.29519922,  0.39243606, ..., -0.23633611,\n         -1.0399027 , -0.6204077 ],\n        [-1.0776411 , -0.3030645 ,  0.39485547, ..., -0.23693427,\n         -1.040965  , -0.6311804 ],\n        [-1.0783868 , -0.31231168,  0.3990058 , ..., -0.23357207,\n         -1.041502  , -0.63420767],\n        ...,\n        [-0.92163795, -0.45179525,  0.680697  , ..., -0.44249436,\n         -0.7053234 , -0.73901266],\n        [-0.9218228 , -0.44918296,  0.6884368 , ..., -0.44419703,\n         -0.70716774, -0.7472601 ],\n        [-0.9189216 , -0.44737202,  0.6886496 , ..., -0.4439164 ,\n         -0.7008352 , -0.757876  ]],\n\n       ...,\n\n       [[-1.1086454 , -0.27162546,  0.34467083, ..., -0.19708861,\n         -1.0057219 , -0.5925371 ],\n        [-1.0998476 , -0.27846527,  0.3461043 , ..., -0.19442016,\n         -1.0068274 , -0.60296524],\n        [-1.1040218 , -0.28340414,  0.35132122, ..., -0.19089402,\n         -1.0066004 , -0.6058751 ],\n        ...,\n        [-0.93698055, -0.4381225 ,  0.65499085, ..., -0.41326603,\n         -0.68250793, -0.7167329 ],\n        [-0.9372965 , -0.43539536,  0.6627072 , ..., -0.41542545,\n         -0.68410826, -0.7247571 ],\n        [-0.9351757 , -0.43284246,  0.66333956, ..., -0.41494665,\n         -0.67733675, -0.73522043]],\n\n       [[-0.652085  , -0.5539138 ,  0.39845446, ..., -0.57609683,\n         -1.3091455 , -0.8160727 ],\n        [-0.64180464, -0.55835205,  0.39686382, ..., -0.57705855,\n         -1.3118187 , -0.8218053 ],\n        [-0.6370058 , -0.5728572 ,  0.39419925, ..., -0.5765649 ,\n         -1.311376  , -0.82021046],\n        ...,\n        [-0.6379863 , -0.5876891 ,  0.7020473 , ..., -0.69053996,\n         -0.8690919 , -0.95529956],\n        [-0.6394373 , -0.5853572 ,  0.7091211 , ..., -0.69147116,\n         -0.8689954 , -0.9633396 ],\n        [-0.63779736, -0.580183  ,  0.71042585, ..., -0.69189185,\n         -0.861884  , -0.97254914]],\n\n       [[-1.0886846 , -0.1846832 ,  0.7908901 , ..., -0.3901965 ,\n         -0.31872588, -0.6570971 ],\n        [-1.0864179 , -0.18873072,  0.79249406, ..., -0.39156643,\n         -0.31610084, -0.6675334 ],\n        [-1.0881407 , -0.19720088,  0.7952812 , ..., -0.38922486,\n         -0.3134062 , -0.67080307],\n        ...,\n        [-0.9236978 , -0.41613284,  0.7360109 , ..., -0.47380295,\n         -0.36279324, -0.72523767],\n        [-0.92527163, -0.41112754,  0.74216795, ..., -0.47532642,\n         -0.36145592, -0.73378474],\n        [-0.92235386, -0.40307236,  0.7408001 , ..., -0.47411853,\n         -0.3552125 , -0.74272907]]], dtype=float32)>]\u001b[0m\n\nArguments received by CrossMultiHeadAttention.call():\n  • x=['tf.Tensor(shape=(32, 200, 128), dtype=float32)', 'tf.Tensor(shape=(32, 200, 128), dtype=float32)', 'tf.Tensor(shape=(32, 200, 128), dtype=float32)']\n  • y=tf.Tensor(shape=(32, 200, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(32, 1, 200, 200), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m decoder \u001b[38;5;241m=\u001b[39m build_decoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, d_model\u001b[38;5;241m=\u001b[39md_model, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,max_sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,language_to_index\u001b[38;5;241m=\u001b[39mtelugu_to_index,START_TOKEN\u001b[38;5;241m=\u001b[39mSTART_TOKEN,END_TOKEN\u001b[38;5;241m=\u001b[39mEND_TOKEN,PADDING_TOKEN\u001b[38;5;241m=\u001b[39mPADDING_TOKEN)\n\u001b[1;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m decoder([encoder_out, batch[\u001b[38;5;241m1\u001b[39m], decoder_self_attention_mask, decoder_cross_attention_mask])\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m, in \u001b[0;36mDecoderWrapper.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     11\u001b[0m cross_attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     14\u001b[0m y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embedding(x\u001b[38;5;241m=\u001b[39my,start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layer(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, self_attention_mask\u001b[38;5;241m=\u001b[39mself_attention_mask, cross_attention_mask\u001b[38;5;241m=\u001b[39mcross_attention_mask)\n",
      "Cell \u001b[1;32mIn[41], line 16\u001b[0m, in \u001b[0;36mDecoder_layer.call\u001b[1;34m(self, x, y, self_attention_mask, cross_attention_mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(y,mask\u001b[38;5;241m=\u001b[39mself_attention_mask)\n\u001b[0;32m     15\u001b[0m norm_and_add1_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_and_add1(attention_out\u001b[38;5;241m+\u001b[39my)\n\u001b[1;32m---> 16\u001b[0m encoder_decoder_attention_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_decoder_attention(x,y,mask\u001b[38;5;241m=\u001b[39mcross_attention_mask)\n\u001b[0;32m     17\u001b[0m norm_and_add2_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_and_add2(norm_and_add1_out\u001b[38;5;241m+\u001b[39mencoder_decoder_attention_out)\n\u001b[0;32m     18\u001b[0m feed_forward_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(norm_and_add2_out)\n",
      "Cell \u001b[1;32mIn[32], line 34\u001b[0m, in \u001b[0;36mCrossMultiHeadAttention.call\u001b[1;34m(self, x, y, mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,y,mask):\n\u001b[0;32m     33\u001b[0m     q\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwq(y)\n\u001b[1;32m---> 34\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(x)\n\u001b[0;32m     35\u001b[0m     v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv(x)\n\u001b[0;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(y)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling CrossMultiHeadAttention.call().\n\n\u001b[1mLayer \"dense_77\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5564932 , -0.5669649 ,  0.7027901 , ..., -0.53199303,\n         -1.3212402 , -0.776253  ],\n        [-0.5504785 , -0.5705082 ,  0.70116484, ..., -0.53427404,\n         -1.3195268 , -0.7772775 ],\n        [-0.5492785 , -0.5775336 ,  0.70136094, ..., -0.53291005,\n         -1.3175031 , -0.7803993 ],\n        ...,\n        [-0.5963789 , -0.5895573 ,  0.7945759 , ..., -0.74501467,\n         -0.8226432 , -0.9577145 ],\n        [-0.597221  , -0.5872512 ,  0.8022083 , ..., -0.74653566,\n         -0.8225584 , -0.9655758 ],\n        [-0.593818  , -0.58298   ,  0.8040438 , ..., -0.74689496,\n         -0.81762546, -0.9732748 ]],\n\n       [[-1.0716151 , -0.2758856 ,  0.7731406 , ..., -0.2441194 ,\n         -0.50318444, -0.5990235 ],\n        [-1.0653529 , -0.28005248,  0.7748195 , ..., -0.2476363 ,\n         -0.4986616 , -0.60288435],\n        [-1.0666635 , -0.28257647,  0.77536714, ..., -0.24508333,\n         -0.4984641 , -0.60864574],\n        ...,\n        [-0.95376545, -0.418268  ,  0.74574107, ..., -0.29118237,\n         -0.48366913, -0.66796774],\n        [-0.953626  , -0.41489792,  0.7536282 , ..., -0.29210618,\n         -0.4839762 , -0.6762593 ],\n        [-0.95157635, -0.4086093 ,  0.75293446, ..., -0.2897909 ,\n         -0.47764733, -0.6873007 ]],\n\n       [[-1.1036644 , -0.29608777,  0.3539137 , ..., -0.22033647,\n         -0.9865732 , -0.61869943],\n        [-1.0980034 , -0.30049735,  0.35581362, ..., -0.21943542,\n         -0.9875603 , -0.6290449 ],\n        [-1.098436  , -0.30892813,  0.36000466, ..., -0.21410285,\n         -0.98741204, -0.6310431 ],\n        ...,\n        [-0.9463632 , -0.44642133,  0.65228087, ..., -0.4215969 ,\n         -0.65154004, -0.73296416],\n        [-0.946982  , -0.4444359 ,  0.660287  , ..., -0.42369854,\n         -0.6534946 , -0.7414886 ],\n        [-0.9444964 , -0.4419547 ,  0.6603018 , ..., -0.42320052,\n         -0.6461147 , -0.7519346 ]],\n\n       ...,\n\n       [[-1.0963136 , -0.2831757 ,  0.35535613, ..., -0.22245017,\n         -1.0090137 , -0.6038814 ],\n        [-1.0876743 , -0.29044205,  0.35734963, ..., -0.2208549 ,\n         -1.0095688 , -0.6147335 ],\n        [-1.0912772 , -0.30087072,  0.3634855 , ..., -0.21891679,\n         -1.0094115 , -0.6194747 ],\n        ...,\n        [-0.926625  , -0.4515016 ,  0.6608705 , ..., -0.4291912 ,\n         -0.68744224, -0.7273532 ],\n        [-0.92657   , -0.4487603 ,  0.6688588 , ..., -0.43109015,\n         -0.6893635 , -0.73599356],\n        [-0.92449576, -0.44604528,  0.669178  , ..., -0.43052572,\n         -0.6832953 , -0.74667406]],\n\n       [[-0.6454124 , -0.55266553,  0.4279716 , ..., -0.58495647,\n         -1.3117238 , -0.81108385],\n        [-0.63658184, -0.5600198 ,  0.42756557, ..., -0.5867606 ,\n         -1.3126708 , -0.8165589 ],\n        [-0.6299888 , -0.5714087 ,  0.422724  , ..., -0.5840041 ,\n         -1.3129405 , -0.813739  ],\n        ...,\n        [-0.6381163 , -0.58815753,  0.7101538 , ..., -0.6911451 ,\n         -0.87402123, -0.9569501 ],\n        [-0.6389358 , -0.58500224,  0.71641564, ..., -0.6921726 ,\n         -0.87369347, -0.9649032 ],\n        [-0.63708645, -0.5797171 ,  0.71769357, ..., -0.6921948 ,\n         -0.8667831 , -0.9746592 ]],\n\n       [[-1.1142294 , -0.16002925,  0.7755542 , ..., -0.3508806 ,\n         -0.27412137, -0.6295667 ],\n        [-1.1117277 , -0.16580972,  0.77586204, ..., -0.3538963 ,\n         -0.27194566, -0.6377784 ],\n        [-1.1156627 , -0.17017971,  0.77920425, ..., -0.3509979 ,\n         -0.26812497, -0.64133006],\n        ...,\n        [-0.9439502 , -0.392971  ,  0.72745097, ..., -0.44148955,\n         -0.32154745, -0.7001404 ],\n        [-0.945312  , -0.38865873,  0.7332847 , ..., -0.44255903,\n         -0.3203026 , -0.70856196],\n        [-0.9431359 , -0.38002738,  0.7309662 , ..., -0.44067767,\n         -0.31355208, -0.71790135]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5866157 , -0.5525143 ,  0.70000744, ..., -0.49493462,\n         -1.3337783 , -0.7703441 ],\n        [-0.58179665, -0.5568265 ,  0.6987454 , ..., -0.49723658,\n         -1.331469  , -0.772033  ],\n        [-0.57948536, -0.56452274,  0.69822353, ..., -0.49449956,\n         -1.3299285 , -0.7755725 ],\n        ...,\n        [-0.62534297, -0.57572025,  0.78345937, ..., -0.7082344 ,\n         -0.81456435, -0.94572085],\n        [-0.6262148 , -0.5740037 ,  0.79101306, ..., -0.7097448 ,\n         -0.8149357 , -0.9541949 ],\n        [-0.6229073 , -0.5701721 ,  0.79223526, ..., -0.71076685,\n         -0.8087758 , -0.962453  ]],\n\n       [[-1.0744787 , -0.27898353,  0.77826726, ..., -0.26323825,\n         -0.51106733, -0.6095979 ],\n        [-1.0681347 , -0.28388745,  0.77888167, ..., -0.26668283,\n         -0.50530535, -0.6156523 ],\n        [-1.066542  , -0.28799742,  0.77952117, ..., -0.26415208,\n         -0.5053626 , -0.6203433 ],\n        ...,\n        [-0.9565688 , -0.42344972,  0.74836737, ..., -0.30987775,\n         -0.48972553, -0.6779521 ],\n        [-0.95612174, -0.4202316 ,  0.7563185 , ..., -0.31078815,\n         -0.48975015, -0.6860772 ],\n        [-0.95369875, -0.4140351 ,  0.75551313, ..., -0.3086175 ,\n         -0.483304  , -0.6965898 ]],\n\n       [[-1.0820639 , -0.29101023,  0.39918813, ..., -0.23768768,\n         -1.0465809 , -0.6168238 ],\n        [-1.0753988 , -0.2973768 ,  0.40242705, ..., -0.2370702 ,\n         -1.0470564 , -0.62836313],\n        [-1.0772306 , -0.30899942,  0.40505797, ..., -0.23441617,\n         -1.0481813 , -0.62885   ],\n        ...,\n        [-0.92022794, -0.44941002,  0.684904  , ..., -0.44193476,\n         -0.70990115, -0.73695415],\n        [-0.92046714, -0.44672006,  0.6923772 , ..., -0.4436846 ,\n         -0.7120436 , -0.7452704 ],\n        [-0.9176997 , -0.44449446,  0.6924351 , ..., -0.4432382 ,\n         -0.70576406, -0.7558112 ]],\n\n       ...,\n\n       [[-1.1151699 , -0.27326325,  0.344872  , ..., -0.18431245,\n         -0.9981333 , -0.5878479 ],\n        [-1.1085954 , -0.2804821 ,  0.34827134, ..., -0.18191433,\n         -0.9992156 , -0.59706914],\n        [-1.1101906 , -0.28786424,  0.35241288, ..., -0.17925419,\n         -1.0010451 , -0.6001679 ],\n        ...,\n        [-0.9385589 , -0.44062847,  0.6574066 , ..., -0.4073303 ,\n         -0.6778204 , -0.7141561 ],\n        [-0.93889254, -0.43809134,  0.66541564, ..., -0.40946472,\n         -0.67937356, -0.72202045],\n        [-0.93675834, -0.43555105,  0.66596955, ..., -0.40904197,\n         -0.67261803, -0.73233604]],\n\n       [[-0.66200435, -0.5438706 ,  0.41360998, ..., -0.5734119 ,\n         -1.2951593 , -0.8138643 ],\n        [-0.6504916 , -0.5483213 ,  0.41255218, ..., -0.57450116,\n         -1.297404  , -0.8185586 ],\n        [-0.648916  , -0.5608711 ,  0.40843153, ..., -0.5728477 ,\n         -1.2980266 , -0.818958  ],\n        ...,\n        [-0.6424503 , -0.58519363,  0.7060744 , ..., -0.6918927 ,\n         -0.8618091 , -0.95516694],\n        [-0.64402163, -0.5830415 ,  0.71310973, ..., -0.6926694 ,\n         -0.86141396, -0.9628229 ],\n        [-0.6427464 , -0.5778872 ,  0.7142331 , ..., -0.6931855 ,\n         -0.854031  , -0.9713798 ]],\n\n       [[-1.11488   , -0.1606243 ,  0.76729363, ..., -0.3525351 ,\n         -0.27754867, -0.628352  ],\n        [-1.1124992 , -0.1657942 ,  0.7672269 , ..., -0.3527527 ,\n         -0.27550083, -0.64121103],\n        [-1.116444  , -0.1725991 ,  0.77099574, ..., -0.35226142,\n         -0.26988044, -0.6406872 ],\n        ...,\n        [-0.94410723, -0.3953225 ,  0.71756005, ..., -0.4460932 ,\n         -0.3247249 , -0.6992749 ],\n        [-0.9456006 , -0.390841  ,  0.7233339 , ..., -0.44671166,\n         -0.32394528, -0.7076318 ],\n        [-0.94342035, -0.38204324,  0.7211661 , ..., -0.44460556,\n         -0.3171408 , -0.71704674]]], dtype=float32)>, <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\narray([[[-0.5729613 , -0.5550019 ,  0.7042439 , ..., -0.50745445,\n         -1.340574  , -0.77086926],\n        [-0.5667232 , -0.5601543 ,  0.70482457, ..., -0.5096249 ,\n         -1.3380775 , -0.77475154],\n        [-0.56656057, -0.56546825,  0.7015346 , ..., -0.5068848 ,\n         -1.3372638 , -0.77578485],\n        ...,\n        [-0.60477597, -0.5858737 ,  0.7949227 , ..., -0.72856784,\n         -0.82987696, -0.9535187 ],\n        [-0.60559756, -0.583377  ,  0.8027031 , ..., -0.7300835 ,\n         -0.8297913 , -0.96148014],\n        [-0.6022334 , -0.5784957 ,  0.80438435, ..., -0.7305188 ,\n         -0.82397777, -0.96943325]],\n\n       [[-1.0738926 , -0.28261757,  0.77548003, ..., -0.26844493,\n         -0.5269834 , -0.6142976 ],\n        [-1.0662366 , -0.2872608 ,  0.77681434, ..., -0.2709448 ,\n         -0.5209994 , -0.6217147 ],\n        [-1.0666413 , -0.2905241 ,  0.7761999 , ..., -0.26778007,\n         -0.5216739 , -0.624984  ],\n        ...,\n        [-0.95717025, -0.4270342 ,  0.7449168 , ..., -0.31357545,\n         -0.50645345, -0.68220705],\n        [-0.95628566, -0.42390278,  0.7524953 , ..., -0.31446248,\n         -0.50622714, -0.6907257 ],\n        [-0.9534703 , -0.4178499 ,  0.7512862 , ..., -0.31231567,\n         -0.49943596, -0.7013802 ]],\n\n       [[-1.0846065 , -0.29519922,  0.39243606, ..., -0.23633611,\n         -1.0399027 , -0.6204077 ],\n        [-1.0776411 , -0.3030645 ,  0.39485547, ..., -0.23693427,\n         -1.040965  , -0.6311804 ],\n        [-1.0783868 , -0.31231168,  0.3990058 , ..., -0.23357207,\n         -1.041502  , -0.63420767],\n        ...,\n        [-0.92163795, -0.45179525,  0.680697  , ..., -0.44249436,\n         -0.7053234 , -0.73901266],\n        [-0.9218228 , -0.44918296,  0.6884368 , ..., -0.44419703,\n         -0.70716774, -0.7472601 ],\n        [-0.9189216 , -0.44737202,  0.6886496 , ..., -0.4439164 ,\n         -0.7008352 , -0.757876  ]],\n\n       ...,\n\n       [[-1.1086454 , -0.27162546,  0.34467083, ..., -0.19708861,\n         -1.0057219 , -0.5925371 ],\n        [-1.0998476 , -0.27846527,  0.3461043 , ..., -0.19442016,\n         -1.0068274 , -0.60296524],\n        [-1.1040218 , -0.28340414,  0.35132122, ..., -0.19089402,\n         -1.0066004 , -0.6058751 ],\n        ...,\n        [-0.93698055, -0.4381225 ,  0.65499085, ..., -0.41326603,\n         -0.68250793, -0.7167329 ],\n        [-0.9372965 , -0.43539536,  0.6627072 , ..., -0.41542545,\n         -0.68410826, -0.7247571 ],\n        [-0.9351757 , -0.43284246,  0.66333956, ..., -0.41494665,\n         -0.67733675, -0.73522043]],\n\n       [[-0.652085  , -0.5539138 ,  0.39845446, ..., -0.57609683,\n         -1.3091455 , -0.8160727 ],\n        [-0.64180464, -0.55835205,  0.39686382, ..., -0.57705855,\n         -1.3118187 , -0.8218053 ],\n        [-0.6370058 , -0.5728572 ,  0.39419925, ..., -0.5765649 ,\n         -1.311376  , -0.82021046],\n        ...,\n        [-0.6379863 , -0.5876891 ,  0.7020473 , ..., -0.69053996,\n         -0.8690919 , -0.95529956],\n        [-0.6394373 , -0.5853572 ,  0.7091211 , ..., -0.69147116,\n         -0.8689954 , -0.9633396 ],\n        [-0.63779736, -0.580183  ,  0.71042585, ..., -0.69189185,\n         -0.861884  , -0.97254914]],\n\n       [[-1.0886846 , -0.1846832 ,  0.7908901 , ..., -0.3901965 ,\n         -0.31872588, -0.6570971 ],\n        [-1.0864179 , -0.18873072,  0.79249406, ..., -0.39156643,\n         -0.31610084, -0.6675334 ],\n        [-1.0881407 , -0.19720088,  0.7952812 , ..., -0.38922486,\n         -0.3134062 , -0.67080307],\n        ...,\n        [-0.9236978 , -0.41613284,  0.7360109 , ..., -0.47380295,\n         -0.36279324, -0.72523767],\n        [-0.92527163, -0.41112754,  0.74216795, ..., -0.47532642,\n         -0.36145592, -0.73378474],\n        [-0.92235386, -0.40307236,  0.7408001 , ..., -0.47411853,\n         -0.3552125 , -0.74272907]]], dtype=float32)>]\u001b[0m\n\nArguments received by CrossMultiHeadAttention.call():\n  • x=['tf.Tensor(shape=(32, 200, 128), dtype=float32)', 'tf.Tensor(shape=(32, 200, 128), dtype=float32)', 'tf.Tensor(shape=(32, 200, 128), dtype=float32)']\n  • y=tf.Tensor(shape=(32, 200, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(32, 1, 200, 200), dtype=float32)"
     ]
    }
   ],
   "source": [
    "decoder = build_decoder(num_layers=12, d_model=d_model, num_heads=8, dff=512,max_sequence_length=200,language_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "output = decoder([encoder_out, batch[1], decoder_self_attention_mask, decoder_cross_attention_mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bed55-46cd-4ae1-941f-1bef64ea6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=[encoder_out, batch[1], decoder_self_attention_mask, decoder_cross_attention_mask]\n",
    "a,b,c,d=input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85517ec-85cc-4ab2-ba9e-b013e55c9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape,type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad78ec3-bdea-4932-911b-51d592db83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=np.char.decode(np.array(batch[1][0]),\"utf-8\")\n",
    "print(aa)\n",
    "print(type(aa))\n",
    "print(aa.shape)\n",
    "aa=aa[np.newaxis,...]\n",
    "f=[p for p in list(aa)]\n",
    "ss=[p for p in f[0]]\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5db0d59e-47a6-47b2-9e3d-8a0f3db888dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,dff,num_decoders,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.num_decoders=num_decoders\n",
    "        self.sentence_embedding=SentenceEmbedding(max_sequence_length=max_sequence_length, d_model=d_model, language_to_index=language_to_index, START_TOKEN=START_TOKEN, END_TOKEN=END_TOKEN, PADDING_TOKEN=PADDING_TOKEN)\n",
    "        self.layers=SequentialDecoder([Decoder_layer(d_model=d_model,num_heads=num_heads,dff=dff) for _ in range(num_decoders)])\n",
    "    def call(self,x,y,self_attention_mask,cross_attention_mask,start_token,end_token):\n",
    "        y=self.sentence_embedding(x=y,start_token=start_token,end_token=end_token)\n",
    "        y=self.layers(x,y,self_attention_mask,cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8396b6ff-88ea-4904-bd90-ac0cb314e951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Decoder name=decoder, built=False>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec=Decoder(d_model=d_model,num_heads=num_heads,dff=512,num_decoders=12,max_sequence_length=english_max_length,language_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "769aa880-7c3f-4d7c-8ea8-5e98b408b4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n"
     ]
    }
   ],
   "source": [
    "decoder_out=[]\n",
    "i=0\n",
    "for inputs, targets in ex_dataset.as_numpy_iterator():\n",
    "    q=dec(encoder_out[i],targets,self_attention_mask=decoder_self_attention_mask,cross_attention_mask=decoder_cross_attention_mask,start_token=True,end_token=True)\n",
    "    decoder_out.append(q)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "369f5a70-62ce-488f-be68-3c0a6f9d605b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.12675643, -1.236485  ,  0.83896685, ...,  0.9615514 ,\n",
       "          -1.0675697 ,  0.6910839 ],\n",
       "         [ 0.25561354, -1.4587431 ,  1.5860904 , ...,  0.85845864,\n",
       "          -1.1830355 ,  0.9274713 ],\n",
       "         [ 0.22521988, -2.118495  ,  1.7130054 , ...,  0.8661974 ,\n",
       "          -1.3735244 ,  1.215216  ],\n",
       "         ...,\n",
       "         [ 0.69056064, -1.6265372 ,  1.7750108 , ...,  0.3218359 ,\n",
       "          -0.32397062,  0.7509952 ],\n",
       "         [ 0.08341514, -1.9896104 ,  1.8339286 , ...,  0.41262177,\n",
       "          -0.23586677,  0.599564  ],\n",
       "         [-0.52436805, -1.6517448 ,  1.4073623 , ...,  0.50126094,\n",
       "          -0.19646999,  0.5848802 ]],\n",
       " \n",
       "        [[ 0.06443958, -1.2698898 ,  0.55010194, ...,  0.8248065 ,\n",
       "          -0.88962567,  0.19901723],\n",
       "         [ 0.42031652, -1.4723619 ,  1.2789083 , ...,  0.8127414 ,\n",
       "          -1.0535455 ,  0.48613787],\n",
       "         [ 0.43921334, -2.1460023 ,  1.4687192 , ...,  0.728765  ,\n",
       "          -1.2868421 ,  0.77122486],\n",
       "         ...,\n",
       "         [ 0.6255777 , -1.6706951 ,  1.6653863 , ...,  0.25820643,\n",
       "          -0.48955902,  0.9939783 ],\n",
       "         [ 0.01382509, -2.040935  ,  1.733629  , ...,  0.38472944,\n",
       "          -0.38340288,  0.8682158 ],\n",
       "         [-0.61827326, -1.6808817 ,  1.2779123 , ...,  0.46454138,\n",
       "          -0.34141177,  0.8407725 ]],\n",
       " \n",
       "        [[-0.12840894, -1.2413819 ,  0.79107827, ...,  0.8410908 ,\n",
       "          -0.9605268 ,  0.5401689 ],\n",
       "         [ 0.19905232, -1.4441477 ,  1.5456762 , ...,  0.80492705,\n",
       "          -1.1529224 ,  0.8210435 ],\n",
       "         [ 0.19247298, -2.1009004 ,  1.6581742 , ...,  0.81244147,\n",
       "          -1.2874701 ,  1.0983257 ],\n",
       "         ...,\n",
       "         [ 0.5522272 , -1.7537355 ,  1.7105359 , ...,  0.04985365,\n",
       "          -0.36302295,  0.93786305],\n",
       "         [-0.04876027, -2.0932708 ,  1.7742974 , ...,  0.15220988,\n",
       "          -0.27800316,  0.7921425 ],\n",
       "         [-0.66924155, -1.7493949 ,  1.3477514 , ...,  0.23845138,\n",
       "          -0.25529853,  0.7816539 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.10796876, -1.224859  ,  0.7917805 , ...,  0.8578147 ,\n",
       "          -0.94925576,  0.5219785 ],\n",
       "         [ 0.21978   , -1.4718386 ,  1.5057873 , ...,  0.86792797,\n",
       "          -1.1573427 ,  0.85019207],\n",
       "         [ 0.16184913, -2.130954  ,  1.6910288 , ...,  0.8669071 ,\n",
       "          -1.2720085 ,  1.0850351 ],\n",
       "         ...,\n",
       "         [ 0.5623211 , -1.7405632 ,  1.7224233 , ...,  0.06899127,\n",
       "          -0.34564638,  0.9216731 ],\n",
       "         [-0.04051068, -2.0801373 ,  1.7882587 , ...,  0.17103574,\n",
       "          -0.26274306,  0.77652943],\n",
       "         [-0.65779215, -1.7381369 ,  1.360795  , ...,  0.25466555,\n",
       "          -0.24306116,  0.76405764]],\n",
       " \n",
       "        [[-0.08677842, -1.2289408 ,  0.8227209 , ...,  0.9191594 ,\n",
       "          -1.0000352 ,  0.6573751 ],\n",
       "         [ 0.33548245, -1.4169514 ,  1.5041826 , ...,  0.85684013,\n",
       "          -1.1742376 ,  0.96260077],\n",
       "         [ 0.27339798, -2.056248  ,  1.7016202 , ...,  0.8820117 ,\n",
       "          -1.3338966 ,  1.2215469 ],\n",
       "         ...,\n",
       "         [ 0.67697376, -1.6447827 ,  1.8100905 , ...,  0.27247775,\n",
       "          -0.3050624 ,  0.7716318 ],\n",
       "         [ 0.06767689, -2.0066965 ,  1.8656837 , ...,  0.3630207 ,\n",
       "          -0.21427073,  0.6247204 ],\n",
       "         [-0.5388787 , -1.6684501 ,  1.4374146 , ...,  0.44856262,\n",
       "          -0.18241946,  0.6127484 ]],\n",
       " \n",
       "        [[-0.00640225, -1.2781683 ,  0.6402393 , ...,  0.82078433,\n",
       "          -0.9298987 ,  0.28084743],\n",
       "         [ 0.4180091 , -1.4724405 ,  1.3962722 , ...,  0.771566  ,\n",
       "          -1.1402915 ,  0.58120733],\n",
       "         [ 0.32802925, -2.185253  ,  1.5558511 , ...,  0.7661888 ,\n",
       "          -1.2589017 ,  0.8124064 ],\n",
       "         ...,\n",
       "         [ 0.58196676, -1.6460607 ,  1.6705766 , ...,  0.26204184,\n",
       "          -0.4501761 ,  0.99388075],\n",
       "         [-0.02650388, -2.0070674 ,  1.7369485 , ...,  0.3804271 ,\n",
       "          -0.34778315,  0.87156296],\n",
       "         [-0.6523139 , -1.6434543 ,  1.2885448 , ...,  0.46711642,\n",
       "          -0.3028075 ,  0.8442131 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.1309877 , -1.2524505 ,  0.8200937 , ...,  0.95153886,\n",
       "          -1.0738114 ,  0.6724131 ],\n",
       "         [ 0.22672424, -1.4821423 ,  1.5341859 , ...,  0.90395284,\n",
       "          -1.2655559 ,  0.9712987 ],\n",
       "         [ 0.1655938 , -2.1681042 ,  1.6913836 , ...,  0.93951404,\n",
       "          -1.3419302 ,  1.1837014 ],\n",
       "         ...,\n",
       "         [ 0.68386215, -1.6253628 ,  1.7915423 , ...,  0.31289485,\n",
       "          -0.32225364,  0.75590503],\n",
       "         [ 0.07697996, -1.9876941 ,  1.8475548 , ...,  0.4046897 ,\n",
       "          -0.2300308 ,  0.60439175],\n",
       "         [-0.52949923, -1.6495453 ,  1.4232473 , ...,  0.49476826,\n",
       "          -0.18960713,  0.59044814]],\n",
       " \n",
       "        [[ 0.05815716, -1.2685938 ,  0.5467967 , ...,  0.8245713 ,\n",
       "          -0.8986781 ,  0.22186378],\n",
       "         [ 0.4873878 , -1.5022748 ,  1.2900335 , ...,  0.74849236,\n",
       "          -1.1531012 ,  0.4741032 ],\n",
       "         [ 0.42329416, -2.1126564 ,  1.5222112 , ...,  0.7762159 ,\n",
       "          -1.3009987 ,  0.7505199 ],\n",
       "         ...,\n",
       "         [ 0.6222604 , -1.6721263 ,  1.6612054 , ...,  0.25443658,\n",
       "          -0.4969577 ,  1.0161455 ],\n",
       "         [ 0.01142273, -2.0444434 ,  1.7289898 , ...,  0.37986714,\n",
       "          -0.3869459 ,  0.8922617 ],\n",
       "         [-0.6214324 , -1.6840789 ,  1.2726253 , ...,  0.46119636,\n",
       "          -0.34537008,  0.8665645 ]],\n",
       " \n",
       "        [[-0.09919246, -1.2343583 ,  0.78730834, ...,  0.82931125,\n",
       "          -0.96289825,  0.50912404],\n",
       "         [ 0.28051254, -1.522365  ,  1.5328524 , ...,  0.82013154,\n",
       "          -1.1150335 ,  0.8004127 ],\n",
       "         [ 0.23700064, -2.0942051 ,  1.6192334 , ...,  0.7976688 ,\n",
       "          -1.3098947 ,  1.0455099 ],\n",
       "         ...,\n",
       "         [ 0.5669367 , -1.74395   ,  1.7075891 , ...,  0.04858475,\n",
       "          -0.35765764,  0.91102356],\n",
       "         [-0.03446697, -2.081903  ,  1.7729717 , ...,  0.15034588,\n",
       "          -0.2746295 ,  0.7665256 ],\n",
       "         [-0.654823  , -1.7411157 ,  1.344865  , ...,  0.2348312 ,\n",
       "          -0.25510257,  0.75517416]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.11113986, -1.219383  ,  0.7884298 , ...,  0.8555119 ,\n",
       "          -0.9531961 ,  0.51351076],\n",
       "         [ 0.29620233, -1.4332741 ,  1.473293  , ...,  0.8539101 ,\n",
       "          -1.1569713 ,  0.7987221 ],\n",
       "         [ 0.2970475 , -2.1712499 ,  1.6373453 , ...,  0.8160381 ,\n",
       "          -1.2472284 ,  1.0908428 ],\n",
       "         ...,\n",
       "         [ 0.5589277 , -1.742397  ,  1.712299  , ...,  0.06518735,\n",
       "          -0.34677693,  0.92042875],\n",
       "         [-0.04507581, -2.081725  ,  1.7790295 , ...,  0.16715896,\n",
       "          -0.26604298,  0.7750616 ],\n",
       "         [-0.6631856 , -1.7398424 ,  1.3516129 , ...,  0.25140226,\n",
       "          -0.24686272,  0.76186734]],\n",
       " \n",
       "        [[-0.06508838, -1.2296418 ,  0.80683166, ...,  0.9239652 ,\n",
       "          -1.0092837 ,  0.63423526],\n",
       "         [ 0.3624867 , -1.5048631 ,  1.4455934 , ...,  0.8891429 ,\n",
       "          -1.2009666 ,  0.9598047 ],\n",
       "         [ 0.31330544, -2.161772  ,  1.6204944 , ...,  0.87423337,\n",
       "          -1.3455999 ,  1.1064668 ],\n",
       "         ...,\n",
       "         [ 0.6929819 , -1.6582016 ,  1.7693691 , ...,  0.29135007,\n",
       "          -0.31165963,  0.7418989 ],\n",
       "         [ 0.08425674, -2.019739  ,  1.8305156 , ...,  0.38203162,\n",
       "          -0.22773004,  0.5939189 ],\n",
       "         [-0.52019715, -1.6821618 ,  1.400056  , ...,  0.46063477,\n",
       "          -0.20123473,  0.5799282 ]],\n",
       " \n",
       "        [[-0.00520124, -1.2729291 ,  0.6487287 , ...,  0.81809545,\n",
       "          -0.9294697 ,  0.27771407],\n",
       "         [ 0.37654355, -1.5039551 ,  1.3754188 , ...,  0.764973  ,\n",
       "          -1.165202  ,  0.60654753],\n",
       "         [ 0.37786785, -2.1644075 ,  1.574405  , ...,  0.6889921 ,\n",
       "          -1.3129414 ,  0.7964263 ],\n",
       "         ...,\n",
       "         [ 0.58712363, -1.6298109 ,  1.6949445 , ...,  0.2539609 ,\n",
       "          -0.44256628,  0.995262  ],\n",
       "         [-0.02130578, -1.992518  ,  1.7606424 , ...,  0.3728246 ,\n",
       "          -0.3378423 ,  0.87361187],\n",
       "         [-0.64759505, -1.6288928 ,  1.3110772 , ...,  0.46141663,\n",
       "          -0.2910233 ,  0.8471934 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 128), dtype=float32, numpy=\n",
       " array([[[-0.13179629, -1.244396  ,  0.82050073, ...,  0.9430527 ,\n",
       "          -1.0660366 ,  0.67246294],\n",
       "         [ 0.24849077, -1.4285605 ,  1.546567  , ...,  0.9034076 ,\n",
       "          -1.2655419 ,  0.96127546],\n",
       "         [ 0.21885669, -2.1271732 ,  1.6947491 , ...,  0.8742487 ,\n",
       "          -1.3854222 ,  1.2280945 ],\n",
       "         ...,\n",
       "         [ 0.68640316, -1.6253151 ,  1.7852998 , ...,  0.30960417,\n",
       "          -0.31998265,  0.7554946 ],\n",
       "         [ 0.07957721, -1.987455  ,  1.8427223 , ...,  0.4022646 ,\n",
       "          -0.22844271,  0.6035509 ],\n",
       "         [-0.5290321 , -1.649857  ,  1.4169129 , ...,  0.49256608,\n",
       "          -0.18853685,  0.5902383 ]],\n",
       " \n",
       "        [[ 0.05712339, -1.2649666 ,  0.55283123, ...,  0.8253555 ,\n",
       "          -0.8965941 ,  0.21345976],\n",
       "         [ 0.45202327, -1.5028296 ,  1.2855517 , ...,  0.77821004,\n",
       "          -1.1383404 ,  0.54675883],\n",
       "         [ 0.3901931 , -2.1754084 ,  1.4744221 , ...,  0.7692585 ,\n",
       "          -1.235974  ,  0.74552095],\n",
       "         ...,\n",
       "         [ 0.6234623 , -1.6667607 ,  1.6670873 , ...,  0.2568854 ,\n",
       "          -0.49482232,  1.0061345 ],\n",
       "         [ 0.01134869, -2.0388336 ,  1.7355356 , ...,  0.38147032,\n",
       "          -0.38585225,  0.8820625 ],\n",
       "         [-0.6213362 , -1.6791906 ,  1.2790534 , ...,  0.46262896,\n",
       "          -0.34379065,  0.85619396]],\n",
       " \n",
       "        [[-0.0931644 , -1.2307887 ,  0.7884817 , ...,  0.83339405,\n",
       "          -0.9614745 ,  0.5214275 ],\n",
       "         [ 0.31720093, -1.4524512 ,  1.4831599 , ...,  0.7517425 ,\n",
       "          -1.1181697 ,  0.8554798 ],\n",
       "         [ 0.298029  , -2.0921178 ,  1.6532567 , ...,  0.808162  ,\n",
       "          -1.268585  ,  1.0910488 ],\n",
       "         ...,\n",
       "         [ 0.5752235 , -1.7395843 ,  1.7107365 , ...,  0.05065306,\n",
       "          -0.3548909 ,  0.92072743],\n",
       "         [-0.02607199, -2.0781863 ,  1.7760327 , ...,  0.15276295,\n",
       "          -0.27053046,  0.7754252 ],\n",
       "         [-0.64629334, -1.7367477 ,  1.34843   , ...,  0.23748085,\n",
       "          -0.25017282,  0.76403445]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.10896578, -1.2072458 ,  0.7897432 , ...,  0.85211694,\n",
       "          -0.9650801 ,  0.5119404 ],\n",
       "         [ 0.29594386, -1.4208122 ,  1.4752908 , ...,  0.84957814,\n",
       "          -1.1719779 ,  0.797111  ],\n",
       "         [ 0.29472142, -2.1590362 ,  1.6369908 , ...,  0.81045747,\n",
       "          -1.2608819 ,  1.0892618 ],\n",
       "         ...,\n",
       "         [ 0.56364495, -1.7448623 ,  1.6955333 , ...,  0.07558678,\n",
       "          -0.36515462,  0.9117219 ],\n",
       "         [-0.04009643, -2.084367  ,  1.7633637 , ...,  0.17670438,\n",
       "          -0.28553393,  0.76663995],\n",
       "         [-0.65611756, -1.7415696 ,  1.3352581 , ...,  0.2602402 ,\n",
       "          -0.26621047,  0.75229204]],\n",
       " \n",
       "        [[-0.07429605, -1.2152941 ,  0.82533467, ...,  0.9187282 ,\n",
       "          -1.0079061 ,  0.6235749 ],\n",
       "         [ 0.31296727, -1.4164053 ,  1.4893947 , ...,  0.79891163,\n",
       "          -1.2132984 ,  0.8981505 ],\n",
       "         [ 0.30351332, -2.0909839 ,  1.642557  , ...,  0.8708121 ,\n",
       "          -1.2734681 ,  1.1446472 ],\n",
       "         ...,\n",
       "         [ 0.68196315, -1.6414076 ,  1.80793   , ...,  0.27887824,\n",
       "          -0.3089317 ,  0.7472795 ],\n",
       "         [ 0.0714625 , -2.0032434 ,  1.8645256 , ...,  0.36936894,\n",
       "          -0.22008789,  0.60010046],\n",
       "         [-0.53437203, -1.66507   ,  1.4336232 , ...,  0.4522678 ,\n",
       "          -0.18858746,  0.5881388 ]],\n",
       " \n",
       "        [[-0.03766853, -1.2613527 ,  0.64924103, ...,  0.80714333,\n",
       "          -0.9309781 ,  0.32077903],\n",
       "         [ 0.39417195, -1.5259308 ,  1.3549582 , ...,  0.7429205 ,\n",
       "          -1.1586365 ,  0.58220005],\n",
       "         [ 0.37574837, -2.2077916 ,  1.5739101 , ...,  0.7093909 ,\n",
       "          -1.2699294 ,  0.88213676],\n",
       "         ...,\n",
       "         [ 0.56393915, -1.6257163 ,  1.6941888 , ...,  0.22948214,\n",
       "          -0.452359  ,  1.0466881 ],\n",
       "         [-0.04259948, -1.9890125 ,  1.7588598 , ...,  0.3485137 ,\n",
       "          -0.34267357,  0.92495227],\n",
       "         [-0.6679474 , -1.6253954 ,  1.312393  , ...,  0.43881947,\n",
       "          -0.2958531 ,  0.90090036]]], dtype=float32)>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce355f53-4aaa-4090-bfa5-217e395b1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,d_model,num_heads,dff,num_layers,target_vocab_size,max_sequence_length,english_to_index,telugu_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.encoder=Encoder(d_model=d_model,num_heads=num_heads,dff=dff,num_encoders=num_layers,max_sequence_length=max_sequence_length,language_to_index=english_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "        self.decoder=Decoder(d_model=d_model,num_heads=num_heads,dff=dff,num_decoders=num_layers,max_sequence_length=max_sequence_length,language_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "        self.final_layer=tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self,input,output,encoder_padding_mask,decoder_self_attention_mask,decoder_cross_attention_mask,enc_start_token,enc_end_token,dec_start_token,dec_end_token):\n",
    "        input=self.encoder(x=input,mask=encoder_padding_mask,start_token=enc_start_token,end_token=enc_end_token)\n",
    "        decoder_out=self.decoder(x=input,y=output,self_attention_mask=decoder_self_attention_mask,cross_attention_mask=decoder_self_attention_mask,start_token=dec_start_token,end_token=dec_end_token)\n",
    "        out=self.final_layer(decoder_out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "041e1fa5-e8f9-452e-acf4-6ab57287a059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=False>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer1=Transformer(d_model=d_model,num_heads=8,dff=512,num_layers=8,target_vocab_size=len(telugu_vocabulary),max_sequence_length=200,english_to_index=english_to_index,telugu_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "transformer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e59124f9-4172-4b2f-89d4-caff6ea7c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n"
     ]
    }
   ],
   "source": [
    "transformer1_out=[]\n",
    "for inputs, targets in ex_dataset.as_numpy_iterator():\n",
    "    encoder_self_attention_mask,decoder_self_attention_mask,decoder_cross_attention_mask=create_masks(inputs,targets,max_sequence_length)\n",
    "    out=transformer1(input=inputs,output=targets,encoder_padding_mask=encoder_self_attention_mask,decoder_self_attention_mask=decoder_self_attention_mask,decoder_cross_attention_mask=decoder_cross_attention_mask,enc_start_token=True,enc_end_token=True,dec_start_token=True,dec_end_token=True)\n",
    "    transformer1_out.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d954071-519d-4e50-a4ad-2e49451401f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(32, 200, 200), dtype=float32, numpy=\n",
       " array([[[-0.9805976 ,  0.26144007, -0.44898546, ..., -0.16023135,\n",
       "          -0.28240028,  0.21092094],\n",
       "         [-0.96884197,  0.48624775, -0.7030204 , ...,  0.02565575,\n",
       "          -0.34626055,  0.3026498 ],\n",
       "         [-0.8836389 ,  0.62890166, -1.0194259 , ...,  0.22547042,\n",
       "          -0.41912046,  0.4871763 ],\n",
       "         ...,\n",
       "         [-1.1965665 ,  0.9623729 , -0.35888073, ...,  1.0347722 ,\n",
       "          -0.99591994, -0.52045894],\n",
       "         [-1.1215929 ,  0.9340674 , -0.36325344, ...,  0.83151424,\n",
       "          -0.8709799 , -0.46235687],\n",
       "         [-1.0077833 ,  0.75008017, -0.34515855, ...,  0.5804842 ,\n",
       "          -0.6513667 , -0.45095974]],\n",
       " \n",
       "        [[-0.9834087 ,  0.2401881 , -0.4326916 , ..., -0.13490653,\n",
       "          -0.27415076,  0.15732469],\n",
       "         [-0.9563782 ,  0.500438  , -0.6833268 , ...,  0.05365911,\n",
       "          -0.43095502,  0.26959205],\n",
       "         [-0.9102559 ,  0.596536  , -0.94664   , ...,  0.2711814 ,\n",
       "          -0.48502964,  0.42561686],\n",
       "         ...,\n",
       "         [-1.1771113 ,  0.9685636 , -0.35302892, ...,  1.0850868 ,\n",
       "          -0.9018831 , -0.54181457],\n",
       "         [-1.0942434 ,  0.9332654 , -0.32614407, ...,  0.901333  ,\n",
       "          -0.80233467, -0.4991922 ],\n",
       "         [-0.9643097 ,  0.7657818 , -0.2796052 , ...,  0.6480088 ,\n",
       "          -0.5965544 , -0.4926545 ]],\n",
       " \n",
       "        [[-0.9828743 ,  0.25699085, -0.43954742, ..., -0.14787942,\n",
       "          -0.27478313,  0.18796416],\n",
       "         [-0.98233205,  0.5106777 , -0.6680105 , ...,  0.07387172,\n",
       "          -0.38529035,  0.30895853],\n",
       "         [-0.8688538 ,  0.57927215, -0.9142564 , ...,  0.24144354,\n",
       "          -0.42584926,  0.48897845],\n",
       "         ...,\n",
       "         [-1.1875502 ,  0.9773706 , -0.34648886, ...,  1.084524  ,\n",
       "          -0.9167573 , -0.5342628 ],\n",
       "         [-1.099203  ,  0.94528294, -0.32172117, ...,  0.9009811 ,\n",
       "          -0.8126558 , -0.48911172],\n",
       "         [-0.96913695,  0.77749413, -0.27303797, ...,  0.6472182 ,\n",
       "          -0.60312706, -0.47852948]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.97999275,  0.26444492, -0.44531152, ..., -0.1543656 ,\n",
       "          -0.27703395,  0.19035843],\n",
       "         [-0.9842276 ,  0.456119  , -0.636962  , ...,  0.0595515 ,\n",
       "          -0.3625749 ,  0.26141924],\n",
       "         [-0.8748564 ,  0.54961467, -0.9578339 , ...,  0.27299622,\n",
       "          -0.41766238,  0.4538979 ],\n",
       "         ...,\n",
       "         [-1.1657985 ,  0.921895  , -0.41906172, ...,  1.0364077 ,\n",
       "          -0.9633695 , -0.49869418],\n",
       "         [-1.0795918 ,  0.8907093 , -0.42039484, ...,  0.8440856 ,\n",
       "          -0.8588258 , -0.44878215],\n",
       "         [-0.9486519 ,  0.72521514, -0.40086162, ...,  0.5921649 ,\n",
       "          -0.6405809 , -0.43967   ]],\n",
       " \n",
       "        [[-0.98759395,  0.2368404 , -0.43860188, ..., -0.13365953,\n",
       "          -0.27299133,  0.14411235],\n",
       "         [-0.9355744 ,  0.4357898 , -0.6659325 , ...,  0.02912709,\n",
       "          -0.33446652,  0.25988644],\n",
       "         [-0.91265553,  0.57034624, -0.9392211 , ...,  0.24830186,\n",
       "          -0.39911297,  0.43310282],\n",
       "         ...,\n",
       "         [-1.1933995 ,  0.9757336 , -0.36361745, ...,  1.0881776 ,\n",
       "          -0.91807204, -0.56220937],\n",
       "         [-1.1074675 ,  0.9403456 , -0.33853552, ...,  0.9092989 ,\n",
       "          -0.81391925, -0.5184747 ],\n",
       "         [-0.9801963 ,  0.774348  , -0.29025298, ...,  0.65843654,\n",
       "          -0.6078625 , -0.5104323 ]],\n",
       " \n",
       "        [[-0.98294866,  0.25668225, -0.4393974 , ..., -0.14776228,\n",
       "          -0.2746177 ,  0.1876362 ],\n",
       "         [-0.95474344,  0.49622947, -0.65977055, ...,  0.07743473,\n",
       "          -0.3880376 ,  0.31335405],\n",
       "         [-0.8429521 ,  0.5799863 , -0.9227189 , ...,  0.2673583 ,\n",
       "          -0.4456151 ,  0.4675278 ],\n",
       "         ...,\n",
       "         [-1.1716633 ,  0.9118044 , -0.39752555, ...,  1.0416721 ,\n",
       "          -0.9547115 , -0.49778685],\n",
       "         [-1.088193  ,  0.8784699 , -0.4030954 , ...,  0.85053885,\n",
       "          -0.85706943, -0.44639164],\n",
       "         [-0.95617104,  0.7124273 , -0.37812936, ...,  0.59977084,\n",
       "          -0.6397737 , -0.4469918 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 200), dtype=float32, numpy=\n",
       " array([[[-0.9783002 ,  0.2635266 , -0.45469174, ..., -0.16039303,\n",
       "          -0.28412277,  0.21897003],\n",
       "         [-0.9819809 ,  0.5090381 , -0.6995439 , ...,  0.02335483,\n",
       "          -0.41327897,  0.3370574 ],\n",
       "         [-0.83960956,  0.59418195, -0.94734454, ...,  0.2089884 ,\n",
       "          -0.41303873,  0.49310824],\n",
       "         ...,\n",
       "         [-1.1901196 ,  0.9402254 , -0.37043118, ...,  1.0370214 ,\n",
       "          -0.99761194, -0.50137997],\n",
       "         [-1.1080418 ,  0.91155857, -0.37629205, ...,  0.8339506 ,\n",
       "          -0.8738663 , -0.44472283],\n",
       "         [-0.9902019 ,  0.7311603 , -0.35983267, ...,  0.5827915 ,\n",
       "          -0.6540667 , -0.43139404]],\n",
       " \n",
       "        [[-0.98708695,  0.25920856, -0.44075856, ..., -0.15117486,\n",
       "          -0.2774586 ,  0.1897505 ],\n",
       "         [-0.9609996 ,  0.47672817, -0.6556596 , ...,  0.05918179,\n",
       "          -0.36862966,  0.28714097],\n",
       "         [-0.9070823 ,  0.63807404, -0.9704413 , ...,  0.24270636,\n",
       "          -0.43594632,  0.47678405],\n",
       "         ...,\n",
       "         [-1.1782871 ,  0.9119068 , -0.40445408, ...,  1.0336593 ,\n",
       "          -0.9506715 , -0.5059896 ],\n",
       "         [-1.0943422 ,  0.88029534, -0.41075826, ...,  0.8419951 ,\n",
       "          -0.85276246, -0.45522156],\n",
       "         [-0.96297425,  0.71735686, -0.38681373, ...,  0.5910411 ,\n",
       "          -0.6360679 , -0.45513868]],\n",
       " \n",
       "        [[-0.98834324,  0.25781396, -0.44027582, ..., -0.15165785,\n",
       "          -0.2770985 ,  0.18810351],\n",
       "         [-0.96236557,  0.4753621 , -0.65534216, ...,  0.05870743,\n",
       "          -0.3680575 ,  0.28526977],\n",
       "         [-0.9083253 ,  0.63675666, -0.9703938 , ...,  0.24200532,\n",
       "          -0.4351787 ,  0.47534552],\n",
       "         ...,\n",
       "         [-1.1793315 ,  0.9082208 , -0.40517464, ...,  1.0307958 ,\n",
       "          -0.95112395, -0.50545675],\n",
       "         [-1.0950869 ,  0.8767088 , -0.41168034, ...,  0.8394104 ,\n",
       "          -0.8538163 , -0.45509315],\n",
       "         [-0.96346587,  0.7139329 , -0.38801372, ...,  0.59075665,\n",
       "          -0.6370875 , -0.45480603]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.985757  ,  0.2578848 , -0.44012544, ..., -0.15073383,\n",
       "          -0.27671826,  0.18985026],\n",
       "         [-0.95860714,  0.49667266, -0.660884  , ...,  0.0739233 ,\n",
       "          -0.38955787,  0.31467608],\n",
       "         [-0.846349  ,  0.5808062 , -0.92551327, ...,  0.26268524,\n",
       "          -0.4465615 ,  0.46844584],\n",
       "         ...,\n",
       "         [-1.1965381 ,  0.9842927 , -0.35874587, ...,  1.0773178 ,\n",
       "          -0.9195715 , -0.54078466],\n",
       "         [-1.1066371 ,  0.9533221 , -0.33343828, ...,  0.89218825,\n",
       "          -0.8122747 , -0.49488434],\n",
       "         [-0.96704155,  0.71584207, -0.39021114, ...,  0.59245324,\n",
       "          -0.6337252 , -0.45392555]],\n",
       " \n",
       "        [[-0.9854205 ,  0.23714745, -0.4396028 , ..., -0.13165379,\n",
       "          -0.2742858 ,  0.14603469],\n",
       "         [-0.9339514 ,  0.4368071 , -0.6673425 , ...,  0.03169101,\n",
       "          -0.3352366 ,  0.262072  ],\n",
       "         [-0.91129476,  0.5713722 , -0.93972063, ...,  0.2508133 ,\n",
       "          -0.40037423,  0.43495032],\n",
       "         ...,\n",
       "         [-1.1637532 ,  0.9539692 , -0.370906  , ...,  1.0746665 ,\n",
       "          -0.8855818 , -0.54165274],\n",
       "         [-1.0840279 ,  0.9173166 , -0.34592262, ...,  0.894403  ,\n",
       "          -0.78759855, -0.49983227],\n",
       "         [-0.9552001 ,  0.7508471 , -0.30215406, ...,  0.6435414 ,\n",
       "          -0.58165133, -0.4947803 ]],\n",
       " \n",
       "        [[-0.9796579 ,  0.26236463, -0.45012164, ..., -0.15999703,\n",
       "          -0.28227645,  0.21184227],\n",
       "         [-0.95837533,  0.4907907 , -0.6517333 , ...,  0.03508249,\n",
       "          -0.3662575 ,  0.2718148 ],\n",
       "         [-0.8925272 ,  0.61062527, -0.9645003 , ...,  0.25451577,\n",
       "          -0.41172916,  0.4628654 ],\n",
       "         ...,\n",
       "         [-1.1767876 ,  0.9152225 , -0.40646315, ...,  1.0319386 ,\n",
       "          -0.96691465, -0.48159593],\n",
       "         [-1.0890334 ,  0.884084  , -0.40881243, ...,  0.83411545,\n",
       "          -0.8583654 , -0.43087465],\n",
       "         [-0.9611491 ,  0.71495605, -0.3947144 , ...,  0.5796243 ,\n",
       "          -0.6386491 , -0.41662034]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32, 200, 200), dtype=float32, numpy=\n",
       " array([[[-0.98193294,  0.26106748, -0.44588292, ..., -0.15995197,\n",
       "          -0.28143352,  0.20608999],\n",
       "         [-0.9540102 ,  0.47029454, -0.740348  , ...,  0.0357945 ,\n",
       "          -0.35380858,  0.35316375],\n",
       "         [-0.8819612 ,  0.60938734, -1.0001671 , ...,  0.2596662 ,\n",
       "          -0.41831082,  0.488322  ],\n",
       "         ...,\n",
       "         [-1.1790284 ,  0.93611914, -0.37754667, ...,  1.0365316 ,\n",
       "          -0.9918236 , -0.50032973],\n",
       "         [-1.097821  ,  0.9073109 , -0.38408068, ...,  0.8332828 ,\n",
       "          -0.874276  , -0.44430485],\n",
       "         [-0.97979826,  0.7291259 , -0.3663447 , ...,  0.58410865,\n",
       "          -0.65403765, -0.43120372]],\n",
       " \n",
       "        [[-0.9790973 ,  0.2613424 , -0.44408128, ..., -0.15443887,\n",
       "          -0.27600154,  0.18736881],\n",
       "         [-0.95495594,  0.4774557 , -0.6588322 , ...,  0.05648501,\n",
       "          -0.36771423,  0.28330696],\n",
       "         [-0.90127325,  0.639701  , -0.9712856 , ...,  0.24102712,\n",
       "          -0.4363761 ,  0.47319   ],\n",
       "         ...,\n",
       "         [-1.1750224 ,  0.9295226 , -0.3910034 , ...,  1.0399901 ,\n",
       "          -0.97935176, -0.5011721 ],\n",
       "         [-1.0893216 ,  0.8996741 , -0.39710072, ...,  0.83757865,\n",
       "          -0.8675935 , -0.44816253],\n",
       "         [-0.9658958 ,  0.7268691 , -0.37774625, ...,  0.58898735,\n",
       "          -0.6485094 , -0.4353019 ]],\n",
       " \n",
       "        [[-0.982733  ,  0.2641576 , -0.4432441 , ..., -0.15410808,\n",
       "          -0.27739725,  0.19188094],\n",
       "         [-0.965037  ,  0.48240376, -0.6898132 , ...,  0.04588384,\n",
       "          -0.42977196,  0.32689834],\n",
       "         [-0.868837  ,  0.5896483 , -0.9215664 , ...,  0.24735084,\n",
       "          -0.44436783,  0.4571409 ],\n",
       "         ...,\n",
       "         [-1.1745521 ,  0.92586625, -0.42774475, ...,  1.0204601 ,\n",
       "          -0.9604421 , -0.49587163],\n",
       "         [-1.0870507 ,  0.8948352 , -0.42904538, ...,  0.82863677,\n",
       "          -0.85586333, -0.44725418],\n",
       "         [-0.9566831 ,  0.73025966, -0.4073327 , ...,  0.5788751 ,\n",
       "          -0.63862175, -0.4411397 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.97820425,  0.2589852 , -0.4445883 , ..., -0.15554087,\n",
       "          -0.27611795,  0.19071522],\n",
       "         [-0.9800847 ,  0.5040799 , -0.69069695, ...,  0.0252014 ,\n",
       "          -0.40729067,  0.3075533 ],\n",
       "         [-0.8396037 ,  0.5854752 , -0.9412851 , ...,  0.21064557,\n",
       "          -0.40323767,  0.46980378],\n",
       "         ...,\n",
       "         [-1.1705719 ,  0.9141912 , -0.4105204 , ...,  1.0318195 ,\n",
       "          -0.96550316, -0.49363786],\n",
       "         [-1.0833982 ,  0.88273656, -0.41280067, ...,  0.83780026,\n",
       "          -0.86052155, -0.4439517 ],\n",
       "         [-0.9532427 ,  0.715333  , -0.3950392 , ...,  0.5847099 ,\n",
       "          -0.6424634 , -0.43418312]],\n",
       " \n",
       "        [[-0.9713083 ,  0.2700702 , -0.4679569 , ..., -0.15421423,\n",
       "          -0.28495353,  0.22573042],\n",
       "         [-0.95213836,  0.52974427, -0.71451133, ...,  0.03925333,\n",
       "          -0.44320685,  0.33069518],\n",
       "         [-0.89151603,  0.6005788 , -1.0061489 , ...,  0.26336533,\n",
       "          -0.45548218,  0.53768986],\n",
       "         ...,\n",
       "         [-1.1807705 ,  0.9698618 , -0.37470764, ...,  1.0395875 ,\n",
       "          -0.99342436, -0.49934554],\n",
       "         [-1.1000293 ,  0.9425239 , -0.37831375, ...,  0.8328377 ,\n",
       "          -0.86412853, -0.43968877],\n",
       "         [-0.98315734,  0.7612701 , -0.36060125, ...,  0.57882464,\n",
       "          -0.64585733, -0.42524925]],\n",
       " \n",
       "        [[-0.98813593,  0.23722449, -0.43908384, ..., -0.13331789,\n",
       "          -0.2734141 ,  0.14437637],\n",
       "         [-0.98285204,  0.47816536, -0.6567777 , ...,  0.0650732 ,\n",
       "          -0.41840315,  0.23886053],\n",
       "         [-0.90046966,  0.62628174, -0.9644837 , ...,  0.27382103,\n",
       "          -0.43962342,  0.41085008],\n",
       "         ...,\n",
       "         [-1.181772  ,  0.96271336, -0.35462445, ...,  1.0788548 ,\n",
       "          -0.9014786 , -0.5546728 ],\n",
       "         [-1.0985986 ,  0.92714393, -0.32863945, ...,  0.89758575,\n",
       "          -0.80383366, -0.51283807],\n",
       "         [-0.9695136 ,  0.7614125 , -0.28316128, ...,  0.64734185,\n",
       "          -0.60184646, -0.5084059 ]]], dtype=float32)>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2636b35-96dd-4e71-9189-70d98f533058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(telugu_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed4ff548-6626-4b0a-a4cd-e08a01a30df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(71, 128), dtype=float32, path=transformer/encoder_2/sentence_embedding_2/embedding_2/embeddings>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_264/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_264/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_265/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_265/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_266/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_266/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_267/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/multi_head_attention_36/dense_267/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/layer_normalization_84/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/layer_normalization_84/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/layer_normalization_85/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/layer_normalization_85/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/feedforward_36/dense_268/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/feedforward_36/dense_268/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/feedforward_36/dense_269/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_24/feedforward_36/dense_269/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_270/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_270/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_271/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_271/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_272/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_272/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_273/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/multi_head_attention_37/dense_273/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/layer_normalization_86/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/layer_normalization_86/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/layer_normalization_87/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/layer_normalization_87/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/feedforward_37/dense_274/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/feedforward_37/dense_274/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/feedforward_37/dense_275/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_25/feedforward_37/dense_275/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_276/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_276/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_277/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_277/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_278/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_278/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_279/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/multi_head_attention_38/dense_279/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/layer_normalization_88/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/layer_normalization_88/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/layer_normalization_89/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/layer_normalization_89/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/feedforward_38/dense_280/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/feedforward_38/dense_280/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/feedforward_38/dense_281/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_26/feedforward_38/dense_281/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_282/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_282/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_283/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_283/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_284/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_284/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_285/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/multi_head_attention_39/dense_285/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/layer_normalization_90/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/layer_normalization_90/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/layer_normalization_91/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/layer_normalization_91/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/feedforward_39/dense_286/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/feedforward_39/dense_286/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/feedforward_39/dense_287/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_27/feedforward_39/dense_287/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_288/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_288/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_289/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_289/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_290/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_290/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_291/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/multi_head_attention_40/dense_291/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/layer_normalization_92/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/layer_normalization_92/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/layer_normalization_93/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/layer_normalization_93/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/feedforward_40/dense_292/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/feedforward_40/dense_292/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/feedforward_40/dense_293/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_28/feedforward_40/dense_293/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_294/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_294/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_295/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_295/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_296/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_296/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_297/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/multi_head_attention_41/dense_297/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/layer_normalization_94/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/layer_normalization_94/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/layer_normalization_95/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/layer_normalization_95/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/feedforward_41/dense_298/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/feedforward_41/dense_298/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/feedforward_41/dense_299/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_29/feedforward_41/dense_299/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_300/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_300/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_301/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_301/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_302/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_302/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_303/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/multi_head_attention_42/dense_303/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/layer_normalization_96/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/layer_normalization_96/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/layer_normalization_97/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/layer_normalization_97/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/feedforward_42/dense_304/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/feedforward_42/dense_304/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/feedforward_42/dense_305/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_30/feedforward_42/dense_305/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_306/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_306/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_307/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_307/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_308/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_308/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_309/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/multi_head_attention_43/dense_309/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/layer_normalization_98/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/layer_normalization_98/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/layer_normalization_99/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/layer_normalization_99/beta>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/feedforward_43/dense_310/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/feedforward_43/dense_310/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/feedforward_43/dense_311/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/encoder_2/sequential_encoder_2/encoder_layer_31/feedforward_43/dense_311/bias>,\n",
       " <KerasVariable shape=(200, 128), dtype=float32, path=transformer/decoder_1/sentence_embedding_3/embedding_3/embeddings>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_312/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_312/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_313/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_313/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_314/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_314/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_315/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/multi_head_attention_44/dense_315/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_100/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_100/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_101/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_101/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_316/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_316/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_317/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_317/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_318/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/cross_multi_head_attention_12/dense_318/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/feedforward_44/dense_320/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/feedforward_44/dense_320/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/feedforward_44/dense_321/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/feedforward_44/dense_321/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_102/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_12/layer_normalization_102/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_322/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_322/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_323/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_323/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_324/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_324/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_325/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/multi_head_attention_45/dense_325/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_103/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_103/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_104/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_104/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_326/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_326/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_327/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_327/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_328/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/cross_multi_head_attention_13/dense_328/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/feedforward_45/dense_330/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/feedforward_45/dense_330/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/feedforward_45/dense_331/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/feedforward_45/dense_331/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_105/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_13/layer_normalization_105/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_332/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_332/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_333/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_333/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_334/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_334/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_335/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/multi_head_attention_46/dense_335/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_106/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_106/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_107/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_107/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_336/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_336/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_337/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_337/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_338/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/cross_multi_head_attention_14/dense_338/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/feedforward_46/dense_340/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/feedforward_46/dense_340/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/feedforward_46/dense_341/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/feedforward_46/dense_341/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_108/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_14/layer_normalization_108/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_342/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_342/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_343/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_343/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_344/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_344/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_345/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/multi_head_attention_47/dense_345/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_109/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_109/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_110/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_110/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_346/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_346/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_347/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_347/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_348/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/cross_multi_head_attention_15/dense_348/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/feedforward_47/dense_350/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/feedforward_47/dense_350/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/feedforward_47/dense_351/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/feedforward_47/dense_351/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_111/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_15/layer_normalization_111/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_352/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_352/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_353/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_353/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_354/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_354/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_355/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/multi_head_attention_48/dense_355/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_112/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_112/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_113/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_113/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_356/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_356/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_357/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_357/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_358/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/cross_multi_head_attention_16/dense_358/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/feedforward_48/dense_360/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/feedforward_48/dense_360/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/feedforward_48/dense_361/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/feedforward_48/dense_361/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_114/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_16/layer_normalization_114/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_362/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_362/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_363/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_363/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_364/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_364/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_365/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/multi_head_attention_49/dense_365/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_115/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_115/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_116/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_116/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_366/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_366/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_367/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_367/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_368/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/cross_multi_head_attention_17/dense_368/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/feedforward_49/dense_370/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/feedforward_49/dense_370/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/feedforward_49/dense_371/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/feedforward_49/dense_371/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_117/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_17/layer_normalization_117/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_372/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_372/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_373/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_373/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_374/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_374/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_375/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/multi_head_attention_50/dense_375/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_118/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_118/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_119/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_119/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_376/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_376/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_377/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_377/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_378/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/cross_multi_head_attention_18/dense_378/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/feedforward_50/dense_380/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/feedforward_50/dense_380/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/feedforward_50/dense_381/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/feedforward_50/dense_381/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_120/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_18/layer_normalization_120/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_382/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_382/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_383/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_383/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_384/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_384/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_385/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/multi_head_attention_51/dense_385/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_121/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_121/beta>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_122/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_122/beta>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_386/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_386/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_387/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_387/bias>,\n",
       " <KerasVariable shape=(128, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_388/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/cross_multi_head_attention_19/dense_388/bias>,\n",
       " <KerasVariable shape=(128, 512), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/feedforward_51/dense_390/kernel>,\n",
       " <KerasVariable shape=(512,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/feedforward_51/dense_390/bias>,\n",
       " <KerasVariable shape=(512, 128), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/feedforward_51/dense_391/kernel>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/feedforward_51/dense_391/bias>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_123/gamma>,\n",
       " <KerasVariable shape=(128,), dtype=float32, path=transformer/decoder_1/sequential_decoder_1/decoder_layer_19/layer_normalization_123/beta>,\n",
       " <KerasVariable shape=(128, 200), dtype=float32, path=transformer/dense_392/kernel>,\n",
       " <KerasVariable shape=(200,), dtype=float32, path=transformer/dense_392/bias>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer1.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bcab9aa-4a5f-4f70-9320-fe917021dfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformer1.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3bf1344-8305-46d3-9841-47b6743d503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(out):\n",
    "    probabilities = tf.nn.softmax(out, axis=-1)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Step 2: Get the most likely character ID for each position\n",
    "    predicted_ids = tf.argmax(probabilities, axis=-1)  # Shape: (batch_size, sequence_length)\n",
    "\n",
    "# Step 3: Convert predicted character IDs to their respective characters\n",
    "    predicted_chars = np.vectorize(index_to_telugu.get)(predicted_ids.numpy())  # Maps the ID to the corresponding character\n",
    "\n",
    "# Step 4: Combine characters into full strings for each sequence\n",
    "    predicted_sentences = [''.join(sentence) for sentence in predicted_chars]\n",
    "\n",
    "# Output the predicted sentences\n",
    "#for i, sentence in enumerate(predicted_sentences):\n",
    " #   print(f\"Predicted sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "208cb86f-b0c4-4cfd-bb80-389ad1e2ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real: (batch_size, seq_len)\n",
    "    # pred: (batch_size, seq_len, vocab_size)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # Masking padding tokens\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # Apply the mask to the loss\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  # Normalize loss by number of non-masked tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "909bdaea-a5d0-400e-92f1-6a263c5958bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b35564e-c10f-4a92-a9a2-7716f4a26ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # Add extra dimensions to fit attention shape\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # Mask out future tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bb5bbf1-e8e8-4e9c-9cd5-92b8cc4c60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(transformer, inputs, targets, encoder_self_attention_mask,decoder_self_attention_mask , ncoder_cross_attention_mask):\n",
    "    # inputs: input sequence to the encoder\n",
    "    # targets: output sequence to the decoder (shifted by 1 for teacher forcing)\n",
    "    \n",
    "    #target_input = targets[:, :-1]  # Remove the last token\n",
    "    #target_real = targets[:, 1:]    # Shifted by one to the right\n",
    "   \n",
    "    \n",
    "    #with tf.compat.v1.Session() as sess:\n",
    "       # p=sess.run(targets)\n",
    "    with tf.GradientTape() as tape:\n",
    "        def get_numpy_array(tensor):\n",
    "            return tensor.numpy()\n",
    "       \n",
    "         # Forward pass through the transformer\n",
    "        predictions = transformer(input=inputs,output=targets,encoder_padding_mask=encoder_self_attention_mask,decoder_self_attention_mask=decoder_self_attention_mask,decoder_cross_attention_mask=decoder_cross_attention_mask,enc_start_token=True,enc_end_token=True,dec_start_token=True,dec_end_token=True)\n",
    "        t= transformer.decoder.sentence_embedding.decoding(np.array(targets))\n",
    "        targets= transformer.decoder.sentence_embedding.batch_tokenize(t, start_token=False, end_token=True)\n",
    "       \n",
    "        # Calculate the loss\n",
    "        loss = loss_function(targets, predictions)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "\n",
    "    # Update weights using the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747e065-e5e8-496e-8fa4-f2dd5de93958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "     # Create the masks\n",
    "        #enc_padding_mask = create_padding_mask(inputs)\n",
    "        #dec_padding_mask = create_padding_mask(targets)\n",
    "        #look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "        #combined_mask = tf.maximum(dec_padding_mask, look_ahead_mask)\n",
    "        \n",
    "    for inputs, targets in ex_dataset.as_numpy_iterator():\n",
    "       \n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask=create_masks(np.array(inputs),np.array(targets),max_sequence_length=200)\n",
    "        # Run a single training step\n",
    "        loss = train_step(transformer1, inputs, targets, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / num_batches}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0c4a329d-7fd0-4b35-a757-7acdc674b399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "(32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "(32,)\n",
      "shape of x= (32,)\n",
      "shape of x= (32,)\n",
      "(32,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs, targets in ex_dataset.as_numpy_iterator():\n",
    "    encoder_self_attention_mask,decoder_self_attention_mask,decoder_cross_attention_mask=create_masks(inputs,targets,max_sequence_length)\n",
    "    predictions = transformer1(input=inputs,output=targets,encoder_padding_mask=encoder_self_attention_mask,decoder_self_attention_mask=decoder_self_attention_mask,decoder_cross_attention_mask=decoder_cross_attention_mask,enc_start_token=True,enc_end_token=True,dec_start_token=True,dec_end_token=True)\n",
    "    print(inputs.shape)  \n",
    "\n",
    "    \n",
    "#inputs=np.array(inputs)\n",
    "#for idx in range(len(inputs)):\n",
    "s#=len(inputs[idx])\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0570834-16b4-449e-98d0-35dd14e08831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ఈ దాడిలోనే దాదాపు 300 మంది ముష్కరులు ప్రాణాలు వదిలారు.\n",
      "ఈ సమస్య ప్రత్యేక శ్రద్ధ అవసరం.\n",
      "రింత రెచ్చిపోయారు.\n",
      "మనం తనకోసం చేసేవాటిని యెహోవా ఎలా చూస్తాడు?\n",
      "గతంలో చోటు చేసుకున్నకొన్ని సంఘటనలు .\n",
      "న్యూఢిల్లీ: గుజరాత్ కేడర్ ఐపీఎస్ ఆఫీసర్ రాకేశ్ ఆస్థానాను బోర్డర్ సెక్యూరిటీ ఫోర్స్ (బీఎస్ఎఫ్) డైరెక్టర్ జనరల్గా కేంద్రం నియమించింది.\n",
      "వెంకట్ రామ్జీ ఈ చిత్రం ద్వారా దర్శకుడిగా పరిచయమవుతున్నారు.\n",
      "ఇరు పక్షాల మధ్య తీవ్రస్థాయిలో వాదోపవాదాలు సాగాయి.\n",
      "కొత్త చలాన్లు వస్తూనే ఉన్నాయి.\n",
      "ఈ ఘటనలన్నింటిపై పోలీసులు క్షుణ్ణంగా విచారణ జరపనున్నారు.\n",
      "రైతుల నోట్లో మట్టికొడుతున్న ప్రభుత్వం\n",
      "సడెన్గా ఎందుకిలా?\n",
      "ఇక ప్రపంచ జనాభా మొత్తంగా అడ్డూ అదుపూ లేకుండా పెరిగిపోతోంది.\n",
      "చిరుత సంచరించిన దృశ్యాలు సీసీ కెమెరాల్లో రియార్డయ్యాయి.\n",
      "పార్లమెంట్ స్థానం\n",
      " -, పెన్పాయింట్  లేదా పెన్ విండోస్ను అమలు చేస్తున్న 3125 మోడల్ పెన్ కంప్యూటర్ను విడుదల చేసింది.\n",
      "దీంతో రెపో రేటు 6 శాతానికి దిగివచ్చింది.\n",
      "ఈ అద్భుతమైన రాత్రి ఆనందించండి!\n",
      "జార్ఖండ్కు మైనింగ్ ఎంత ప్రాధాన్యత కలిగినదో వివరిస్తూ శ్రీ జోషి, జార్ఖండ్కు మైనింగ్ జీవన రేఖ వంటిందని, జార్ఖండ్ అభివృద్ధిలో ఇది కీలక పాత్ర వహిస్తుందని చెప్పారు.\n",
      "రూ. 30 కోట్ల విలువైన ఆస్తి పత్రాలు స్వాధీనం\n",
      "వెంటనే అతన్ని ఆసుపత్రికి తీసుకొని వెళ్లారు.\n",
      "‘పుస్తకాలు చాలా తక్కువగానే చదువుతాను.\n",
      "మన శరీరంలోని ప్రతి అవయవం తన విధులను సక్రమంగా నిర్వర్తించడానికి థైరాయిడ్ గ్రంథి స్రవించే హార్మోన్ చాలా అవసరం.\n",
      "చిన్నారి ప్రైవేట్పార్ట్స్ వద్ద గాయాలున్నట్టుగా గుర్తించారు.\n",
      "మెచ్చుకుంటూ అన్నాడు.\n",
      "తలకు బలమైన గాయమవడంతో బాలుడు అక్కడికక్కడే ప్రాణాలు కోల్పోయాడు.\n",
      "అక్కడ నుండి తూర్పు, పడమర వైపు వ్యాపించింది.\n",
      "దేశం కోసం గాంధీ తన జీవితాన్నే త్యాగం చేశారని కొనియాడారు.\n",
      "లేని పక్షంలో తాము ఈ అంశంపై ఎన్నికల కమిషన్కు ఫిర్యాదు చేస్తామని తెలిపారు.\n",
      "అందరినీ మెప్పిస్తుంది !\n",
      "డెవలప్మెంట్ కన్సల్టెంట్స్ లిమిటెడ్\n",
      "మహిళల క్రికెట్ జట్టు కోచ్ గా 'డబ్ల్యూవీ రమన్'\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=0\n",
    "for i in targets:\n",
    "    print(i.decode(\"utf-8\"))\n",
    "    j=j+1\n",
    "print(j)\n",
    "type(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4060a5-51df-4721-a779-ca78423ce06e",
   "metadata": {},
   "outputs": [],
   "source": [
    " for batch, (inputs, targets) in enumerate(dataset):\n",
    "    continue\n",
    "    #target_input = targets[:, :-1]  # Remove the last token\n",
    "    #target_real = targets[:, 1:] \n",
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc20a6-a97a-4e97-aed8-271964a4b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, PADDING_TOKEN))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def train_step(eng_batch, tel_batch, transformer, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, tel_batch,max_sequence_length=200)\n",
    "\n",
    "        tel_predictions = transformer(\n",
    "            eng_batch,\n",
    "            tel_batch,\n",
    "            encoder_self_attention_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            decoder_cross_attention_mask,\n",
    "            enc_start_token=False,\n",
    "            enc_end_token=False,\n",
    "            dec_start_token=True,\n",
    "            dec_end_token=True\n",
    "        )\n",
    "        k= transformer.decoder.sentence_embedding.decoding(np.array(tel_batch))\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(k, start_token=False, end_token=True)\n",
    "        #loss = loss_function(labels, tel_predictions)\n",
    "    \n",
    "    #gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    #optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return tel_predictions,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebc229-a964-4e27-b909-022b82b78ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "tel_predictions=[]\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "for eng_batch, tel_batch in ex_dataset.as_numpy_iterator():\n",
    "    a,b= train_step(eng_batch,tel_batch,transformer1,optimizer)\n",
    "    labels.append(b)\n",
    "    tel_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcc76b-9313-4ce1-bc11-515cbc920ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[2].shape,tel_predictions[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130d9c8-c6fa-4ebb-a0bc-383510903b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    \"\"\"predictions = transformer1(eng_batch,\n",
    "            tel_batch,\n",
    "            encoder_self_attention_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            decoder_cross_attention_mask,\n",
    "            enc_start_token=False,\n",
    "            enc_end_token=False,\n",
    "            dec_start_token=True,\n",
    "            dec_end_token=True)  # Model's forward pass\"\"\"\n",
    "    loss = loss_function(labels[0],tel_predictions[0] )  # Loss calculation\n",
    "#gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
    "#len(gradients)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f6c42-cdfc-4396-8ff5-6c5997f3fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in transformer1.trainable_variables:\n",
    "    print(f\"Layer {layer.name} is trainable: {layer.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa268-cbe8-475d-b830-e909efd46843",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "with tf.GradientTape() as tape:\n",
    "    #output = transformer1(input_data)\n",
    "    loss = loss_function(labels[2],tel_predictions[2])\n",
    "\n",
    "gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
    "\n",
    "# Debug the gradients\n",
    "for grad, var in zip(gradients, transformer1.trainable_variables):\n",
    "    if grad is None:\n",
    "        print(f\"No gradient for {var.name}\")\n",
    "    else:\n",
    "        print(f\"Gradient for {var.name}: {grad.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622332cc-4c02-45fe-8c01-0ce55447265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "with tf.GradientTape() as tape:\n",
    "    #output = transformer1(input_data)  # shape [32, 200, 200]\n",
    "    loss = loss_function(labels[2],tel_predictions[2])  # true_labels shape [32, 200]\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
    "#optimizer.apply_gradients(zip(gradients, transformer1.trainable_variables))\n",
    "for grad, var in zip(gradients, transformer1.trainable_variables):\n",
    "    if grad is None:\n",
    "        print(f\"Warning: No gradient computed for variable {var.name}\")\n",
    "    else:\n",
    "        print(f\"Gradient computed for {var.name}: {grad.shape}\")\n",
    "\n",
    "# Filter out None gradients and apply the rest\n",
    "gradients_vars = [(grad, var) for grad, var in zip(gradients, transformer1.trainable_variables) if grad is not None]\n",
    "\n",
    "if gradients_vars:\n",
    "    optimizer.apply_gradients(gradients_vars)\n",
    "else:\n",
    "    print(\"No valid gradients to apply.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612090e9-cdee-4525-b713-564bbefd354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "for i,j in zip(tel_predictions,labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_function(j, i)\n",
    "        gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer1.trainable_variables))\"\"\"\n",
    "with tf.GradientTape() as tape:\n",
    "    gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
    "    print(gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee2587-1f64-4726-94c5-74e65e8c14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    #tape.watch(transformer1.trainable_variables)  # Optional, but helps with debugging\n",
    "    #predictions = transformer1(inputs)\n",
    "    loss = loss_function(labels[2],tel_predictions[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f864a31-f2f0-48f6-b8c7-1c9588293d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7a092-124a-4433-8eb2-47019998efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in transformer1.trainable_variables:\n",
    "    print(var.name, var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef1842-f932-4484-b154-1cd2909005a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd0f0e-3101-4836-8146-f8e83a73b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer2=Transformer(d_model=d_model,num_heads=8,dff=512,num_layers=8,target_vocab_size=len(telugu_vocabulary),max_sequence_length=200,english_to_index=english_to_index,telugu_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676d2e4-c704-4857-b7bf-81879db8b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec6d71-3fb4-427e-a146-fe4b679ca904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6065bac-42a3-4ce7-af56-6c941a31aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19913cdf-6887-4e36-b4f6-a4a3db4262f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer2.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3130b79-cca6-4f4e-b8ad-7836026cf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer2.fit([input=eng,output=tel,encoder_padding_mask=encoder_self_attention_mask,decoder_self_attention_mask=decoder_self_attention_mask,decoder_cross_attention_mask=decoder_cross_attention_mask,enc_start_token=True,enc_end_token=True,dec_start_token=True,dec_end_token=True]\n",
    "\n",
    "                epochs=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744ca4-c79f-4468-8d37-5de0f97065f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636810e3-38f3-415b-8d01-a08b94f53fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac902b-3b5a-4f5d-885a-7fbaf3f74f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# Optimizer and loss function setup\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# Custom loss function that ignores padding tokens\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, PADDING_TOKEN))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "# Training step for a single batch\n",
    "@tf.function\n",
    "def train_step(eng_batch, tel_batch, transformer, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, tel_batch,max_sequence_length=200)\n",
    "\n",
    "        tel_predictions = transformer(\n",
    "            eng_batch,\n",
    "            tel_batch,\n",
    "            encoder_self_attention_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            decoder_cross_attention_mask,\n",
    "            enc_start_token=False,\n",
    "            enc_end_token=False,\n",
    "            dec_start_token=True,\n",
    "            dec_end_token=True\n",
    "        )\n",
    "\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(tel_batch, start_token=False, end_token=True)\n",
    "        loss = loss_function(labels, tel_predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Custom train function to loop over the dataset\n",
    "def train_model(transformer, train_dataset, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        total_loss = 0\n",
    "        batch_num=0\n",
    "\n",
    "        for eng_batch, tel_batch in train_dataset.as_numpy_iterator():\n",
    "            loss = train_step(eng_batch, tel_batch, transformer, optimizer)\n",
    "            total_loss += loss\n",
    "\n",
    "            if batch_num % 100 == 0:\n",
    "                print(f'Batch {batch_num}: Loss {loss.numpy()}')\n",
    "                # Additional logging for debugging purposes\n",
    "                predicted_sentence = evaluate_sentence(transformer, eng_batch[0])\n",
    "                print(f'Predicted translation: {predicted_sentence}')\n",
    "            batch_num=batch_num+1\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss.numpy()}')\n",
    "\n",
    "# Evaluation step (inference) for translating a sentence\n",
    "def evaluate_sentence(transformer, eng_sentence):\n",
    "    tel_sentence = ['']\n",
    "    for word_counter in range(max_sequence_length):\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_sentence, tel_sentence,max_sequence_length=200)\n",
    "        \n",
    "        predictions = transformer(\n",
    "            eng_sentence,\n",
    "            tel_sentence,\n",
    "            encoder_self_attention_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            decoder_cross_attention_mask,\n",
    "            enc_start_token=False,\n",
    "            enc_end_token=False,\n",
    "            dec_start_token=True,\n",
    "            dec_end_token=False\n",
    "        )\n",
    "\n",
    "        next_token_prob_distribution = predictions[0, -1, :]\n",
    "        next_token_index = tf.argmax(next_token_prob_distribution, axis=-1).numpy()\n",
    "\n",
    "        next_token = index_to_telugu[next_token_index]\n",
    "        tel_sentence.append(next_token)\n",
    "\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "\n",
    "    return ' '.join(kn_sentence)\n",
    "\n",
    "# Model training\n",
    "num_epochs = 10\n",
    "train_model(transformer2,ex_dataset, num_epochs)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f84fe1-8e6d-4ef2-9ee0-ebc836417fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for eng_batch, tel_batch in ex_dataset.as_numpy_iterator():\n",
    "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, tel_batch,max_sequence_length=200)\n",
    "encoder_self_attention_mask\"\"\"                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebaff8-fb40-4658-ab1a-d5c460421bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer3=Transformer(d_model=d_model,num_heads=8,dff=512,num_layers=8,target_vocab_size=len(telugu_vocabulary),max_sequence_length=200,english_to_index=english_to_index,telugu_to_index=telugu_to_index,START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN)\n",
    "transformer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582b715-590c-4477-8065-a94a5d4cdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define loss function (sparse categorical crossentropy for token prediction)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# Define the optimizer (Adam is commonly used for transformers)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Function to calculate loss\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, PADDING_TOKEN))  # Ignore padding tokens\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # Apply the mask to zero out the loss for padding tokens\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "# Backpropagation function for a single batch\n",
    "@tf.function\n",
    "def train_step(eng_batch, tel_batch, transformer, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Create masks for the batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(\n",
    "            eng_batch, tel_batch, max_sequence_length=200\n",
    "        )\n",
    "        \n",
    "        # Forward pass: Get predictions from transformer\n",
    "        tel_predictions = transformer(\n",
    "            eng_batch,\n",
    "            tel_batch,\n",
    "            encoder_self_attention_mask,\n",
    "            decoder_self_attention_mask,\n",
    "            decoder_cross_attention_mask,\n",
    "            enc_start_token=False,\n",
    "            enc_end_token=False,\n",
    "            dec_start_token=True,\n",
    "            dec_end_token=True\n",
    "        )\n",
    "        \n",
    "        # Labels: Target Telugu sentence (shifted by one token)\n",
    "        labels = tel_batch\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(labels, tel_predictions)\n",
    "    \n",
    "    # Backward pass: Calculate gradients\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    \n",
    "    # Update the model's weights\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "def train_model(transformer, dataset, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over each batch in the dataset\n",
    "        for (batch_num, (eng_batch, tel_batch)) in enumerate(dataset):\n",
    "            # Perform backpropagation on each batch\n",
    "            batch_loss = train_step(eng_batch, tel_batch, transformer, optimizer)\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            if batch_num % 100 == 0:\n",
    "                print(f'Batch {batch_num} Loss {batch_loss:.4f}')\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} Loss: {total_loss/len(dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31822d94-94b5-4dc0-b219-80ba28fad19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(transformer3,ex_dataset,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce2d01-f613-40c3-bbad-94695d57aca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba188b6-0a38-4128-9de2-93fb25cdb72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
